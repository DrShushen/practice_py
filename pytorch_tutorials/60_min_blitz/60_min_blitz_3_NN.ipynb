{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks can be constructed using the `torch.nn` package.\n",
    "\n",
    "Now that you had a glimpse of `autograd`, `nn` depends on autograd to define models and differentiate them. An `nn.Module` contains layers, and a method f`orward(input)` that returns the output.\n",
    "\n",
    "For example, look at this network that classifies digit images:\n",
    "\n",
    "![img](./content/60_min_blitz_3_NN_img1.png)\n",
    "\n",
    "(convnet)\n",
    "\n",
    "It is a simple feed-forward network. It takes the input, feeds it through several layers one after the other, and then finally gives the output.\n",
    "\n",
    "A typical training procedure for a neural network is as follows:\n",
    "\n",
    "* Define the neural network that has some learnable parameters (or weights)\n",
    "* Iterate over a dataset of inputs\n",
    "* Process input through the network\n",
    "* Compute the loss (how far is the output from being correct)\n",
    "* Propagate gradients back into the network’s parameters\n",
    "* Update the weights of the network, typically using a simple update rule: `weight = weight - learning_rate * gradient`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let’s define this network: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn  # NN module, nn by convention\n",
    "import torch.nn.functional as F  # Functions, F by convention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(Net, self).__init__()  # Call the parent consructor to initialise stuff.\n",
    "        \n",
    "        # 1 input image channel, 6 output channels, 3x3 square convolution kernel:\n",
    "        # >> Conv2d(in_channels, out_channels, kernel_size[, ...])\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)  # Note now it becomes 6 \"channels\" input.\n",
    "        \n",
    "        # An affine operation: y = Wx + b\n",
    "        # Linear(in_features, out_features, bias=True)\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):  # x is the *input*.\n",
    "        \n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))  # Note the nonlinearity added here and elsewhere below - ReLU.\n",
    "        \n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        \n",
    "        # Flatten.\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        \n",
    "        # Fully connected layers at the end.\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    # backward() will be auto-generated with `autograd`.\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # All dimensions except the **batch dimension**.\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note: inherit your \"net\" from `nn.Module`.\n",
    "* `.Conv2D()`: https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d\n",
    "* `.Linear()`: https://pytorch.org/docs/stable/nn.html#torch.nn.Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "net = Net()\n",
    "display(net)  # Note that the connections between layers aren't explicitly defined, that is based on .forward()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You just have to define the `forward` function, and the `backward` function (where gradients are computed) is automatically defined for you using `autograd`. You can use any of the `Tensor` operations in the `forward` function.\n",
    "\n",
    "**The learnable parameters of a model are returned by `net.parameters()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(params)=10\n",
      "\n",
      "[Parameter containing:\n",
      "tensor([[[[-0.0199,  0.0639, -0.1107],\n",
      "          [ 0.0668,  0.2990,  0.0810],\n",
      "          [ 0.0042, -0.2457,  0.1999]]],\n",
      "\n",
      "\n",
      "        [[[-0.0983,  0.3058,  0.0727],\n",
      "          [-0.0775,  0.1638, -0.1003],\n",
      "          [ 0.3184, -0.0278, -0.2887]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0862,  0.2799,  0.1622],\n",
      "          [ 0.2039, -0.0608, -0.1061],\n",
      "          [ 0.2023, -0.2501, -0.0702]]],\n",
      "\n",
      "\n",
      "        [[[-0.1480,  0.1583,  0.0267],\n",
      "          [ 0.2382,  0.2834, -0.0075],\n",
      "          [-0.1894, -0.0008,  0.0747]]],\n",
      "\n",
      "\n",
      "        [[[-0.2462, -0.1901, -0.1427],\n",
      "          [-0.3212,  0.3289,  0.0872],\n",
      "          [ 0.1247, -0.3025,  0.1002]]],\n",
      "\n",
      "\n",
      "        [[[-0.2468, -0.2135,  0.1983],\n",
      "          [-0.2266, -0.1930, -0.2596],\n",
      "          [-0.0404, -0.1577,  0.1070]]]], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2425,  0.3082, -0.2659, -0.1044,  0.2444, -0.2036],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[[[-0.1117,  0.1356,  0.0394],\n",
      "          [-0.0577, -0.0572,  0.0802],\n",
      "          [ 0.1128\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "print(\"len(params)={} \\n\".format(len(params)))\n",
    "print(str(params)[:1000] + \"\\n...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter 0 \n",
      "\n",
      "params[0].size()=torch.Size([6, 1, 3, 3]) \n",
      "\n",
      "Parameter containing:\n",
      "tensor([[[[-0.0199,  0.0639, -0.1107],\n",
      "          [ 0.0668,  0.2990,  0.0810],\n",
      "          [ 0.0042, -0.2457,  0.1999]]],\n",
      "\n",
      "\n",
      "        [[[-0.0983,  0.3058,  0.0727],\n",
      "          [-0.0775,  0.1638, -0.1003],\n",
      "          [ 0.3184, -0.0278, -0.2887]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0862,  0.2799,  0.1622],\n",
      "          [ 0.2039, -0.0608, -0.1061],\n",
      "          [ 0.2023, -0.2501, -0.0702]]],\n",
      "\n",
      "\n",
      "        [[[-0.1480,  0.1583,  0.0267],\n",
      "          [ 0.2382,  0.2834, -0.0075],\n",
      "          [-0.1894, -0.0008,  0.0747]]],\n",
      "\n",
      "\n",
      "        [[[-0.2462, -0.1901, -0.1427],\n",
      "          [-0.3212,  0.3289,  0.0872],\n",
      "          [ 0.1247, -0.3025,  0.1002]]],\n",
      "\n",
      "\n",
      "        [[[-0.2468, -0.2135,  0.1983],\n",
      "          [-0.2266, -0.1930, -0.2596],\n",
      "          [-0.0404, -0.1577,  0.1070]]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# conv1's .weight\n",
    "print(\"parameter 0 \\n\")\n",
    "print(\"params[0].size()={} \\n\".format(params[0].size()))\n",
    "print(params[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some notes on parameters\n",
    "* In the above Conv2d (conv1 layer here) case, 6 (output channels) x 1 (input channel?) x (3x3) (convolution kernels)\n",
    "* **The params don't appear to be labelled by name, they just come as a list.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment with parameter ordering ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net2(nn.Module):\n",
    "    ### Same as Net but define layers in a different order.\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(Net2, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        \n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        \n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        \n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net2(\n",
       "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "net2 = Net2()\n",
    "display(net2)\n",
    "# Notice the print order is the same as the order the layers were defined in __init__()!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "params2 = list(net2.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(params2)=10 \n",
      "\n",
      "[Parameter containing:\n",
      "tensor([[ 0.0256,  0.0407,  0.0194,  ..., -0.0282,  0.0013, -0.0035],\n",
      "        [-0.0382,  0.0208,  0.0180,  ..., -0.0363,  0.0002,  0.0352],\n",
      "        [-0.0114, -0.0196, -0.0164,  ...,  0.0044, -0.0216,  0.0358],\n",
      "        ...,\n",
      "        [-0.0018, -0.0058,  0.0022,  ...,  0.0186, -0.0078,  0.0288],\n",
      "        [-0.0286,  0.0335, -0.0334,  ...,  0.0131, -0.0256,  0.0020],\n",
      "        [-0.0223,  0.0106,  0.0058,  ..., -0.0407,  0.0331,  0.0269]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0233,  0.0385,  0.0167,  0.0215, -0.0334,  0.0249,  0.0191, -0.0142,\n",
      "         0.0210,  0.0352,  0.0107,  0.0005,  0.0191, -0.0159, -0.0095, -0.0192,\n",
      "         0.0398, -0.0364,  0.0052,  0.0312,  0.0179,  0.0152, -0.0183, -0.0065,\n",
      "        -0.0167, -0.0281, -0.0137, -0.0393,  0.0400,  0.0339, -0.0293,  0.0133,\n",
      "        -0.0112,  0.0320, -0.0175, -0.0255, -0.0270, -0.0311, -0.0077, -0.0095,\n",
      "        -0.0275,  0.0049, -0.0360,  0.0069, -0.0077,  0.0253,  0.0074,  0.0411,\n",
      "        -0.02\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "print(\"len(params2)={} \\n\".format(len(params2)))\n",
    "print(str(params2)[:1000] + \"\\n...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter 0 \n",
      "\n",
      "params2[0].size()=torch.Size([120, 576]) \n",
      "\n",
      "Parameter containing:\n",
      "tensor([[ 0.0256,  0.0407,  0.0194,  ..., -0.0282,  0.0013, -0.0035],\n",
      "        [-0.0382,  0.0208,  0.0180,  ..., -0.0363,  0.0002,  0.0352],\n",
      "        [-0.0114, -0.0196, -0.0164,  ...,  0.0044, -0.0216,  0.0358],\n",
      "        ...,\n",
      "        [-0.0018, -0.0058,  0.0022,  ...,  0.0186, -0.0078,  0.0288],\n",
      "        [-0.0286,  0.0335, -0.0334,  ...,  0.0131, -0.0256,  0.0020],\n",
      "        [-0.0223,  0.0106,  0.0058,  ..., -0.0407,  0.0331,  0.0269]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 0th parameter weight.\n",
    "print(\"parameter 0 \\n\")\n",
    "print(\"params2[0].size()={} \\n\".format(params2[0].size()))\n",
    "print(params2[0])\n",
    "\n",
    "# Note: now it's not conv1's but fc1's parameters in position 0!\n",
    "# So .parameters() will return the weights in the layer definitin order!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note on `parameters`\n",
    "* **`.parameters()` will return the weights in the _layer definitin order_.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment with parameter ordering \\[END\\] ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's try a random 32x32 input.\n",
    "Note: expected input size of this net (LeNet) is 32x32. To use this net on MNIST dataset, please resize the images from the dataset to 32x32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0206,  0.3281, -1.5938,  ...,  1.8398,  0.7286, -1.2506],\n",
       "          [ 0.0960, -0.8373,  0.3181,  ..., -1.8976,  0.4062, -1.4908],\n",
       "          [ 1.0482, -0.4722,  0.0423,  ..., -0.2576, -0.0701,  1.7547],\n",
       "          ...,\n",
       "          [ 1.3112, -1.4368,  1.3746,  ..., -2.4759, -0.6301,  1.3009],\n",
       "          [ 0.9194, -0.4053, -0.4947,  ..., -0.4405,  0.1686,  0.1829],\n",
       "          [-0.6960,  0.3544, -0.7624,  ...,  2.3394, -1.2457, -0.2961]]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_input = torch.randn(1, 1, 32, 32)\n",
    "display(_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0826,  0.0899, -0.0101, -0.0280, -0.0894, -0.0471, -0.0544, -0.0436,\n",
       "          0.0614, -0.0471]], grad_fn=<AddmmBackward>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out = net(_input)\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0826,  0.0899, -0.0101, -0.0280, -0.0894, -0.0471, -0.0544, -0.0436,\n",
       "          0.0614, -0.0471]], grad_fn=<AddmmBackward>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out = net.forward(_input)\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `net(_input)` <=> `net.forward(_input)`.\n",
    "* Output is 1x10 as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zero the gradient buffers of all parameters and backprops with random gradients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()  # Sets gradients of all **model parameters** to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Gradients': torch.randn(1, 10)\n",
    "gradients = torch.randn(1, 10)\n",
    "out.backward(gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter 0 (conv1's .weight) \n",
      "\n",
      "params[0].size()=torch.Size([6, 1, 3, 3]) \n",
      "\n",
      "Parameter containing:\n",
      "tensor([[[[-0.0199,  0.0639, -0.1107],\n",
      "          [ 0.0668,  0.2990,  0.0810],\n",
      "          [ 0.0042, -0.2457,  0.1999]]],\n",
      "\n",
      "\n",
      "        [[[-0.0983,  0.3058,  0.0727],\n",
      "          [-0.0775,  0.1638, -0.1003],\n",
      "          [ 0.3184, -0.0278, -0.2887]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0862,  0.2799,  0.1622],\n",
      "          [ 0.2039, -0.0608, -0.1061],\n",
      "          [ 0.2023, -0.2501, -0.0702]]],\n",
      "\n",
      "\n",
      "        [[[-0.1480,  0.1583,  0.0267],\n",
      "          [ 0.2382,  0.2834, -0.0075],\n",
      "          [-0.1894, -0.0008,  0.0747]]],\n",
      "\n",
      "\n",
      "        [[[-0.2462, -0.1901, -0.1427],\n",
      "          [-0.3212,  0.3289,  0.0872],\n",
      "          [ 0.1247, -0.3025,  0.1002]]],\n",
      "\n",
      "\n",
      "        [[[-0.2468, -0.2135,  0.1983],\n",
      "          [-0.2266, -0.1930, -0.2596],\n",
      "          [-0.0404, -0.1577,  0.1070]]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# conv1's .weight - now have been updated!\n",
    "print(\"parameter 0 (conv1's .weight) \\n\")\n",
    "print(\"params[0].size()={} \\n\".format(params[0].size()))\n",
    "print(params[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "`torch.nn` only supports mini-batches. The entire `torch.nn` package only supports inputs that are a **mini-batch of samples, and not a single sample**.\n",
    "\n",
    "For example, `nn.Conv2d` will take in a 4D Tensor of `nSamples x nChannels x Height x Width`.\n",
    "\n",
    "**If you have a single sample, just use `input.unsqueeze(0)` to add a fake batch dimension.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before proceeding further, let’s recap all the classes you’ve seen so far.\n",
    "\n",
    "**Recap:**\n",
    "* `torch.Tensor` - A multi-dimensional array with support for `autograd` operations like `backward()`. Also holds the **gradient w.r.t. the tensor**.\n",
    "* `nn.Module` - Neural network module. Convenient way of encapsulating parameters, with helpers for moving them to GPU, exporting, loading, etc.\n",
    "* `nn.Parameter` - A kind of `Tensor`, that is automatically registered as a parameter when assigned as an attribute to a `Module`.\n",
    "* `autograd.Function` - Implements forward and backward definitions of an `autograd` operation. Every `Tensor` operation creates at least a single `Function` node that connects to functions that created a `Tensor` and encodes its history: `MulBackward0` etc.\n",
    "\n",
    "**At this point, we covered:**\n",
    "* Defining a neural network\n",
    "* Processing inputs and calling backward\n",
    "\n",
    "**Still Left:**\n",
    "* Computing the loss\n",
    "* Updating the weights of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A loss function takes the (output, target) pair of inputs, and computes a value that estimates how far away the output is from the target.\n",
    "\n",
    "There are several different loss functions under the `nn` package . A simple loss is: `nn.MSELoss` which computes the mean-squared error between the input and the target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'target'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1507,  0.2656, -1.4367, -0.0110, -1.6221, -0.0103, -1.6291, -0.0072,\n",
       "         -0.9954,  0.1068]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'output'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0826,  0.0899, -0.0101, -0.0280, -0.0894, -0.0471, -0.0544, -0.0436,\n",
       "          0.0614, -0.0471]], grad_fn=<AddmmBackward>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = net(_input)\n",
    "target = torch.randn(10)  # A dummy target, for example.\n",
    "target = target.view(1, -1)  # Make it the same shape as output (a 1x (1x10), because batching(?)).\n",
    "display(\"target\", target, \"output\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()  # Type of loss function. Meand Squared Error here.\n",
    "loss = criterion(output, target)  # Loss function **calculation** itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8093, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, if you follow `loss` in the backward direction, using its `.grad_fn` attribute, you will see a graph of computations that looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
    "\n",
    "-> view -> linear -> relu -> linear -> relu -> linear\n",
    "\n",
    "-> MSELoss -> loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, when we call `loss.backward()`, the whole graph is differentiated w.r.t. the loss, and all Tensors in the graph that has `requires_grad=True` will have their `.grad` Tensor accumulated with the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For illustration, let us follow a few steps backward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MseLossBackward at 0x7f83da1a4390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<AddmmBackward at 0x7f83da1ab1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<AccumulateGrad at 0x7f83da1ab390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(loss.grad_fn)  # .MSELoss()\n",
    "display(loss.grad_fn.next_functions[0][0])  # .Linear()\n",
    "display(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # .Linear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note the `.grad_fn.next_functions[0][0]` call - `.next_functions[i][j]` seems to allow traversing the Function graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprop "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To backpropagate the error all we have to do is to `loss.backward()` (see `autograd` tutorial).\n",
    "\n",
    "**You need to clear the existing gradients though, else gradients will be accumulated to existing gradients!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we shall call `loss.backward()`, and have a look at conv1’s bias gradients before and after the backward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad after backward\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.0037,  0.0011,  0.0029,  0.0037, -0.0169,  0.0063])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "net.zero_grad()  # Zeroes the gradient buffers of all parameters.\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "display(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()  # Crucial line.\n",
    "print('conv1.bias.grad after backward')\n",
    "display(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we have seen how to use loss functions.\n",
    "\n",
    "**Read Later:**\n",
    "* The neural network package contains various modules and loss functions that form the building blocks of deep neural networks. \n",
    "* A full list with documentation is [here](https://pytorch.org/docs/nn).\n",
    "\n",
    "**The only thing left to learn is:**\n",
    "* Updating the weights of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Side note - getting parameters of a layer. ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list(net.conv1.parameters())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[[[-0.0199,  0.0639, -0.1107],\n",
       "           [ 0.0668,  0.2990,  0.0810],\n",
       "           [ 0.0042, -0.2457,  0.1999]]],\n",
       " \n",
       " \n",
       "         [[[-0.0983,  0.3058,  0.0727],\n",
       "           [-0.0775,  0.1638, -0.1003],\n",
       "           [ 0.3184, -0.0278, -0.2887]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0862,  0.2799,  0.1622],\n",
       "           [ 0.2039, -0.0608, -0.1061],\n",
       "           [ 0.2023, -0.2501, -0.0702]]],\n",
       " \n",
       " \n",
       "         [[[-0.1480,  0.1583,  0.0267],\n",
       "           [ 0.2382,  0.2834, -0.0075],\n",
       "           [-0.1894, -0.0008,  0.0747]]],\n",
       " \n",
       " \n",
       "         [[[-0.2462, -0.1901, -0.1427],\n",
       "           [-0.3212,  0.3289,  0.0872],\n",
       "           [ 0.1247, -0.3025,  0.1002]]],\n",
       " \n",
       " \n",
       "         [[[-0.2468, -0.2135,  0.1983],\n",
       "           [-0.2266, -0.1930, -0.2596],\n",
       "           [-0.0404, -0.1577,  0.1070]]]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.2425,  0.3082, -0.2659, -0.1044,  0.2444, -0.2036],\n",
       "        requires_grad=True)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---\n",
      "\n",
      "net.conv1.weight\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[-0.0199,  0.0639, -0.1107],\n",
       "          [ 0.0668,  0.2990,  0.0810],\n",
       "          [ 0.0042, -0.2457,  0.1999]]],\n",
       "\n",
       "\n",
       "        [[[-0.0983,  0.3058,  0.0727],\n",
       "          [-0.0775,  0.1638, -0.1003],\n",
       "          [ 0.3184, -0.0278, -0.2887]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0862,  0.2799,  0.1622],\n",
       "          [ 0.2039, -0.0608, -0.1061],\n",
       "          [ 0.2023, -0.2501, -0.0702]]],\n",
       "\n",
       "\n",
       "        [[[-0.1480,  0.1583,  0.0267],\n",
       "          [ 0.2382,  0.2834, -0.0075],\n",
       "          [-0.1894, -0.0008,  0.0747]]],\n",
       "\n",
       "\n",
       "        [[[-0.2462, -0.1901, -0.1427],\n",
       "          [-0.3212,  0.3289,  0.0872],\n",
       "          [ 0.1247, -0.3025,  0.1002]]],\n",
       "\n",
       "\n",
       "        [[[-0.2468, -0.2135,  0.1983],\n",
       "          [-0.2266, -0.1930, -0.2596],\n",
       "          [-0.0404, -0.1577,  0.1070]]]], requires_grad=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---\n",
      "\n",
      "net.conv1.bias\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.2425,  0.3082, -0.2659, -0.1044,  0.2444, -0.2036],\n",
       "       requires_grad=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"list(net.conv1.parameters())\")\n",
    "display(list(net.conv1.parameters()))\n",
    "\n",
    "print(\"\\n---\\n\")\n",
    "\n",
    "print(\"net.conv1.weight\")\n",
    "display(net.conv1.weight)\n",
    "\n",
    "print(\"\\n---\\n\")\n",
    "\n",
    "print(\"net.conv1.bias\")\n",
    "display(net.conv1.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You can obtain parameters from a specific layer using `<module>.<layer>.parameters()`\n",
    "* At least on convolutional layers, there exist `.weight` and `.bias` parameters, which is clearer than getting all paramers at once with `.parameters()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Side note - getting parameters of a layer. \\[END\\] ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the weights "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest update rule used in practice is the Stochastic Gradient Descent (SGD):\n",
    "\n",
    "`weight = weight - learning_rate * gradient`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can implement this using simple python code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net.conv1.bias before weight update\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.2425,  0.3082, -0.2659, -0.1044,  0.2444, -0.2036],\n",
       "       requires_grad=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net.conv1.bias after weight update\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.2425,  0.3082, -0.2659, -0.1044,  0.2446, -0.2037],\n",
       "       requires_grad=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "print(\"net.conv1.bias before weight update\")\n",
    "display(net.conv1.bias)\n",
    "\n",
    "# Iterate through *all parameters of the net*:\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)  # This is SGD.\n",
    "    # ^ `.sub_()` is presumably just an inplace substitution of the parameters.\n",
    "\n",
    "print(\"---\")\n",
    "print(\"net.conv1.bias after weight update\")\n",
    "display(net.conv1.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(net.conv1.bias) = \n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "type(net.conv1.bias.data) = \n",
      "<class 'torch.Tensor'>\n",
      "\n",
      "---\n",
      "\n",
      "type(net.conv1.bias.grad) = \n",
      "<class 'torch.Tensor'>\n",
      "type(net.conv1.bias.grad.data) = \n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# Investigate `.data` and `grad.data`\n",
    "\n",
    "print(\"type(net.conv1.bias) = \\n{}\".format(type(net.conv1.bias)))  # torch.nn.parameter.Parameter\n",
    "print(\"type(net.conv1.bias.data) = \\n{}\".format(type(net.conv1.bias.data)))  # torch.Tensor (so .data just gets the underlying Tensor)\n",
    "\n",
    "print(\"\\n---\\n\")\n",
    "\n",
    "print(\"type(net.conv1.bias.grad) = \\n{}\".format(type(net.conv1.bias.grad)))  # Interestingly, this is **already** a torch.Tensor\n",
    "print(\"type(net.conv1.bias.grad.data) = \\n{}\".format(type(net.conv1.bias.grad.data)))  # Calling `.data` on torch.Tensor just returns the torch.Tensor itself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### However, as you use neural networks, you want to use various different update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc.\n",
    "##### To enable this, we built a small package: `torch.optim` that implements all these methods. \n",
    "##### Using it is very simple: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your optimizer.\n",
    "optimiser = optim.SGD(net.parameters(), lr=0.01)  # Seems first argument in the constructor is the net weights/params, others are optimiser hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net.conv1.bias before weight update\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.2426,  0.3082, -0.2660, -0.1045,  0.2450, -0.2037],\n",
       "       requires_grad=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "net.conv1.bias after weight update\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.2427,  0.3082, -0.2660, -0.1045,  0.2452, -0.2037],\n",
       "       requires_grad=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# In your TRAINING LOOP:\n",
    "# (i.e. this is a single step)\n",
    "\n",
    "print(\"net.conv1.bias before weight update\")\n",
    "display(net.conv1.bias)\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "optimiser.zero_grad()  # Don't forget!\n",
    "\n",
    "output = net(_input)  # Calculate the output (forward pass).\n",
    "\n",
    "loss = criterion(output, target)  # Calculate the loss value.\n",
    "\n",
    "loss.backward()  # Backprop the gradient from the loss.\n",
    "\n",
    "optimiser.step()  # HERE, the optimier's predefined step is called.\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "print(\"---\")\n",
    "print(\"net.conv1.bias after weight update\")\n",
    "display(net.conv1.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "Observe how gradient buffers had to be manually set to zero using `optimizer.zero_grad()`. This is because gradients are accumulated as explained in Backprop section."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37_pytorch_tutorials] *",
   "language": "python",
   "name": "conda-env-py37_pytorch_tutorials-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
