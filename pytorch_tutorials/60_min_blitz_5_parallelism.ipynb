{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: Data Parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will learn how to use multiple GPUs using `DataParallel`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s very easy to use GPUs with PyTorch. You can put the model on a GPU:\n",
    "```\n",
    "device = torch.device(\"cuda:0\")\n",
    "model.to(device)\n",
    "```\n",
    "\n",
    "Then, you can copy all your tensors to the GPU:\n",
    "```\n",
    "mytensor = my_tensor.to(device)\n",
    "```\n",
    "\n",
    "Please note that just calling `my_tensor.to(device)` returns a new copy of `my_tensor` on GPU instead of rewriting `my_tensor`. \n",
    "You need to assign it to a new tensor and use that tensor on the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s natural to execute your forward, backward propagations on multiple GPUs. \n",
    "However, Pytorch will only use one GPU by default. \n",
    "\n",
    "**You can easily run your operations on multiple GPUs by making your model run parallelly using `DataParallel`:**\n",
    "\n",
    "```\n",
    "model = nn.DataParallel(model)\n",
    "```\n",
    "\n",
    "That’s the core behind this tutorial. We will explore it in more detail below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import PyTorch modules and define parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters and DataLoaders\n",
    "\n",
    "input_size = 5\n",
    "output_size = 2\n",
    "\n",
    "batch_size = 30\n",
    "data_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Device "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "display(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy DataSet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make a dummy (random) dataset. You just need to implement the getitem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, size, length):\n",
    "        self.len = length\n",
    "        self.data = torch.randn(length, size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_loader = DataLoader(dataset=RandomDataset(input_size, data_size), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the demo, our model just gets an input, performs a linear operation, and gives an output. \n",
    "However, you can use `DataParallel` on any model (CNN, RNN, Capsule Net etc.)\n",
    "\n",
    "We’ve placed a print statement inside the model to monitor the size of input and output tensors. \n",
    "Please pay attention to what is printed at batch rank 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    # Our model\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, _input):\n",
    "        output = self.fc(_input)\n",
    "        print(\"\\tIn Model: input size\", _input.size(), \"output size\", output.size())\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model and DataParallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the core part of the tutorial.\n",
    "\n",
    "First, we need to make a model instance and check if we have multiple GPUs.\n",
    "\n",
    "If we have multiple GPUs, we can wrap our model using `nn.DataParallel`. \n",
    "\n",
    "Then we can put our model on GPUs by `model.to(device)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_size, output_size)  # Create instance of Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1:  # Useful: torch.cuda.device_count()\n",
    "    \n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    \n",
    "    # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "    \n",
    "    model = nn.DataParallel(model)  # Key line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turns out don't have more than one GPU here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we can see the sizes of input and output tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tIn Model: input size torch.Size([30, 5]) output size torch.Size([30, 2])\n",
      "Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
      "\tIn Model: input size torch.Size([30, 5]) output size torch.Size([30, 2])\n",
      "Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
      "\tIn Model: input size torch.Size([30, 5]) output size torch.Size([30, 2])\n",
      "Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
      "\tIn Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
      "Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])\n"
     ]
    }
   ],
   "source": [
    "for data in rand_loader:\n",
    "    _input = data.to(device)\n",
    "    output = model(_input)\n",
    "    print(\"Outside: input size\", _input.size(), \"output_size\", output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Nothing surprising because only one GPU on this machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **In theory would get:**\n",
    "\n",
    "**2 GPUs**\n",
    "```\n",
    "    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n",
    "    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n",
    "Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
    "    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n",
    "    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n",
    "Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
    "    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n",
    "    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n",
    "Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
    "    In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])\n",
    "    In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])\n",
    "Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])\n",
    "```\n",
    "\n",
    "**8 GPUs**\n",
    "```\n",
    "    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
    "    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
    "    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
    "    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
    "    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
    "    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
    "Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
    "    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
    "    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
    "    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
    "    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
    "    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
    "Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice the automatic breaking-down and stitching-up that `DataParallel` does for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataParallel splits your data automatically and **sends job orders to multiple models on several GPUs**.\n",
    "\n",
    "After each model finishes their job, **DataParallel collects and merges the results** before returning it to you.\n",
    "\n",
    "For more information, please check out https://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practice_py_pt",
   "language": "python",
   "name": "practice_py_pt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
