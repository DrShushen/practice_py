{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URLs:\n",
    "GitHub: [https://github.com/gabrielloye/LSTM_Sentiment-Analysis/blob/master/main.ipynb](https://github.com/gabrielloye/LSTM_Sentiment-Analysis/blob/master/main.ipynb)\n",
    "\n",
    "Article: [https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/](https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/)\n",
    "\n",
    "Data: [https://www.kaggle.com/bittlingmayer/amazonreviews](https://www.kaggle.com/bittlingmayer/amazonreviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script settings.\n",
    "SMALLER_SAMPLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ess/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bz2\n",
    "from collections import Counter\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = bz2.BZ2File('./data/train.ft.txt.bz2')\n",
    "test_file = bz2.BZ2File('./data/test.ft.txt.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = train_file.readlines()\n",
    "test_file = test_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training reivews: 3600000\n",
      "Number of test reviews: 400000\n"
     ]
    }
   ],
   "source": [
    "# Load files.\n",
    "\n",
    "print(\"Number of training reivews: \" + str(len(train_file)))\n",
    "print(\"Number of test reviews: \" + str(len(test_file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset, parse from bytes.\n",
    "\n",
    "if SMALLER_SAMPLE:\n",
    "    num_train = 9600\n",
    "    num_test = 2400\n",
    "else:\n",
    "    num_train = 800000  # We're training on the first 800,000 reviews in the dataset\n",
    "    num_test = 200000  # Using 200,000 reviews from test set\n",
    "\n",
    "train_file = [x.decode('utf-8') for x in train_file[:num_train]]\n",
    "test_file = [x.decode('utf-8') for x in test_file[:num_test]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__2 Stuning even for the non-gamer: This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_file[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Cleaning. --\n",
    "\n",
    "\n",
    "# Extracting labels from sentences:\n",
    "\n",
    "train_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in train_file]\n",
    "train_sentences = [x.split(' ', 1)[1][:-1].lower() for x in train_file]\n",
    "\n",
    "    \n",
    "test_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in test_file]\n",
    "test_sentences = [x.split(' ', 1)[1][:-1].lower() for x in test_file]\n",
    "\n",
    "\n",
    "# Some simple cleaning of data:\n",
    "\n",
    "for i in range(len(train_sentences)):\n",
    "    train_sentences[i] = re.sub('\\d','0',train_sentences[i])\n",
    "\n",
    "for i in range(len(test_sentences)):\n",
    "    test_sentences[i] = re.sub('\\d','0',test_sentences[i])\n",
    "\n",
    "\n",
    "# Modify URLs to <url>:\n",
    "\n",
    "for i in range(len(train_sentences)):\n",
    "    if 'www.' in train_sentences[i] or 'http:' in train_sentences[i] or 'https:' in train_sentences[i] or '.com' in train_sentences[i]:\n",
    "        train_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", train_sentences[i])\n",
    "        \n",
    "for i in range(len(test_sentences)):\n",
    "    if 'www.' in test_sentences[i] or 'http:' in test_sentences[i] or 'https:' in test_sentences[i] or '.com' in test_sentences[i]:\n",
    "        test_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", test_sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "['stuning even for the non-gamer: this sound track was beautiful! it paints the senery in your mind so well i would recomend it even to people who hate vid. game music! i have played the game chrono cross but out of all of the games i have ever played it has the best music! it backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. it would impress anyone who cares to listen! ^_^']\n",
      "[1]\n",
      "['great cd: my lovely pat has one of the great voices of her generation. i have listened to this cd for years and i still love it. when i\\'m in a good mood it makes me feel better. a bad mood just evaporates like sugar in the rain. this cd just oozes life. vocals are jusat stuunning and lyrics just kill. one of life\\'s hidden gems. this is a desert isle cd in my book. why she never made it big is just beyond me. everytime i play this, no matter black, white, young, old, male, female everybody says one thing \"who was that singing ?\"']\n"
     ]
    }
   ],
   "source": [
    "# Show, post clean and label extract.\n",
    "\n",
    "print(train_labels[:1])\n",
    "print(train_sentences[:1])\n",
    "\n",
    "print(test_labels[:1])\n",
    "print(test_sentences[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove file objects.\n",
    "del train_file, test_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% done\n",
      "12.5% done\n",
      "25.0% done\n",
      "37.5% done\n",
      "50.0% done\n",
      "62.5% done\n",
      "75.0% done\n",
      "87.5% done\n",
      "100% done\n"
     ]
    }
   ],
   "source": [
    "# Extract vocabulary from training set.\n",
    "\n",
    "words = Counter()  # Dictionary that will map a word to the number of times it appeared in all the training sentences\n",
    "\n",
    "for i, sentence in enumerate(train_sentences):\n",
    "    # The sentences will be stored as a list of words/tokens\n",
    "    train_sentences[i] = []\n",
    "    for word in nltk.word_tokenize(sentence):  # Tokenizing the words\n",
    "        words.update([word.lower()])  # Converting all the words to lower case\n",
    "        train_sentences[i].append(word)\n",
    "    if i % 100000 == 0:\n",
    "        print(str((i*100)/num_train) + \"% done\")\n",
    "\n",
    "print(\"100% done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "words counter dict:\n",
      "\n",
      "[('stuning', 9), ('even', 113265), ('for', 669566), ('the', 3223249), ('non-gamer', 4)]\n",
      "\n",
      "words_ sorted list:\n",
      "\n",
      "['_PAD', '_UNK', '.', 'the', ',']\n",
      "\n",
      "word2idx:\n",
      "\n",
      "[('_PAD', 0), ('_UNK', 1), ('.', 2), ('the', 3), (',', 4)]\n",
      "\n",
      "idx2word:\n",
      "\n",
      "[(0, '_PAD'), (1, '_UNK'), (2, '.'), (3, 'the'), (4, ',')]\n"
     ]
    }
   ],
   "source": [
    "# -- Construct vocabulary. --\n",
    "\n",
    "# Removing the words that only appear once\n",
    "words = {k:v for k,v in words.items() if v>1}\n",
    "print(\"\\nwords counter dict:\\n\")\n",
    "print(list(words.items())[:5])\n",
    "\n",
    "# Sorting the words according to the number of appearances, with the most common word being first\n",
    "words_ = sorted(words, key=words.get, reverse=True)\n",
    "# Adding padding and unknown to our vocabulary so that they will be assigned an index\n",
    "words_ = ['_PAD','_UNK'] + words_\n",
    "print(\"\\nwords_ sorted list:\\n\")\n",
    "print(words_[:5])\n",
    "\n",
    "# Dictionaries to store the word to index mappings and vice versa.\n",
    "# Note: '_PAD' --> 0, '_UNK' --> 1.\n",
    "word2idx = {o:i for i,o in enumerate(words_)}\n",
    "idx2word = {i:o for i,o in enumerate(words_)}\n",
    "\n",
    "print(\"\\nword2idx:\\n\")\n",
    "print(list(word2idx.items())[:5])\n",
    "print(\"\\nidx2word:\\n\")\n",
    "print(list(idx2word.items())[:5])\n",
    "\n",
    "if SMALLER_SAMPLE:\n",
    "    word2idx_fname = \"./data/word2idx_small.json\"\n",
    "    idx2word_fname = \"./data/idx2word_small.json\"\n",
    "else:\n",
    "    word2idx_fname = \"./data/word2idx.json\"\n",
    "    idx2word_fname = \"./data/idx2word.json\"\n",
    "\n",
    "with open(word2idx_fname, 'w') as f:\n",
    "    json.dump(word2idx, f)\n",
    "with open(idx2word_fname, 'w') as f:\n",
    "    json.dump(idx2word, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert words to indices.\n",
    "\n",
    "for i, sentence in enumerate(train_sentences):\n",
    "    # Looking up the mapping dictionary and assigning the index to the respective words\n",
    "    train_sentences[i] = [word2idx[word] if word in word2idx else 1 for word in sentence]  # Note: line with correction!\n",
    "\n",
    "for i, sentence in enumerate(test_sentences):\n",
    "    # For test sentences, we have to tokenize the sentences as well\n",
    "    test_sentences[i] = [word2idx[word.lower()] if word.lower() in word2idx else 0 for word in nltk.word_tokenize(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function that either shortens sentences or pads sentences with 0 to a fixed length\n",
    "\n",
    "def pad_input(sentences, seq_len):\n",
    "    features = np.zeros((len(sentences), seq_len),dtype=int)\n",
    "    for ii, review in enumerate(sentences):\n",
    "        if len(review) != 0:\n",
    "            features[ii, -len(review):] = np.array(review)[:seq_len]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply padding.\n",
    "\n",
    "seq_len = 200  # The length that the sentences will be padded/shortened to\n",
    "\n",
    "train_sentences = pad_input(train_sentences, seq_len)\n",
    "test_sentences = pad_input(test_sentences, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting our labels into numpy arrays.\n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_sentences[0]:\n",
      "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0    40    99    13    28  1445  4274    58    31    10     3\n",
      "    40  1778    10    85  1727     2     5    27   904     8    11    99\n",
      "    16   152     6     5   140    89     9     2    68     5   122    14\n",
      "     7    42  1845     9   210    59   243   109     2     7   134  1845\n",
      "    47 29399    38  2640    14     3  2378     2    11    99    47 18877\n",
      "   160     2   932    30     0     0     6   557    47  1282     2    31\n",
      "    10   160    21  2334  4156     2    11    12     7  3564 15134    99\n",
      "    14    28    24     2   182   102   130   147     9   239    12    47\n",
      "   821    59     2  2582     5   262    11     4    72   598   441     4\n",
      "   576     4   413     4   153     4  1686     4  1251  1814   519    31\n",
      "   179    33    80    18    17   825    62    32]\n",
      "test_labels[0]:\n",
      "1\n",
      "\n",
      " train_sentences: (800000, 200)\n",
      " train_labels: (800000,)\n",
      " test_sentences: (200000, 200)\n",
      " test_labels: (200000,)\n"
     ]
    }
   ],
   "source": [
    "# Preview padded:\n",
    "print(\"test_sentences[0]:\")\n",
    "print(test_sentences[0])\n",
    "print(\"test_labels[0]:\")\n",
    "print(test_labels[0])\n",
    "\n",
    "# Shapes:\n",
    "print(\"\\n train_sentences: {}\\n train_labels: {}\\n test_sentences: {}\\n test_labels: {}\".format(train_sentences.shape, train_labels.shape, test_sentences.shape, test_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the test set into a validation and test sets.\n",
    "\n",
    "split_frac = 0.5\n",
    "\n",
    "split_id = int(split_frac * len(test_sentences))\n",
    "\n",
    "val_sentences, test_sentences = test_sentences[:split_id], test_sentences[split_id:]\n",
    "val_labels, test_labels = test_labels[:split_id], test_labels[split_id:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as numpy array files for reuse.\n",
    "\n",
    "if SMALLER_SAMPLE:\n",
    "    filename = \"./data/processed_small.npz\"\n",
    "else:\n",
    "    filename = \"./data/processed.npz\"\n",
    "\n",
    "np.savez(filename, \n",
    "         train_sentences=train_sentences, train_labels=train_labels, \n",
    "         test_sentences=test_sentences, test_labels=test_labels,\n",
    "         val_sentences=val_sentences, val_labels=val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sense check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking equality of matrix train_sentences: True\n",
      "Checking equality of matrix train_labels: True\n",
      "Checking equality of matrix test_sentences: True\n",
      "Checking equality of matrix test_labels: True\n",
      "Checking equality of matrix val_sentences: True\n",
      "Checking equality of matrix val_labels: True\n"
     ]
    }
   ],
   "source": [
    "if not SMALLER_SAMPLE:\n",
    "    filename = \"./data/processed.npz\"\n",
    "    filename_check = \"./repo_files/processed.npz\"\n",
    "\n",
    "    npzfile = np.load(filename)\n",
    "    npzfile_check = np.load(filename_check)\n",
    "    \n",
    "    for name in npzfile.files:\n",
    "        a = npzfile[name]\n",
    "        b = npzfile_check[name]\n",
    "        print(\"Checking equality of matrix {}: {}\".format(name, np.all(a == b)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37_pytorch_tutorials_gabrielloye] *",
   "language": "python",
   "name": "conda-env-py37_pytorch_tutorials_gabrielloye-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
