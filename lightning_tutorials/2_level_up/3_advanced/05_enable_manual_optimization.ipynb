{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following [this](https://pytorch-lightning.readthedocs.io/en/stable/model/build_model_advanced.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customize training loop\n",
    "\n",
    "<img src=\"./assets/custom_loop.png\" width=1000/>\n",
    "\n",
    "Inject custom code anywhere in the Training loop using \n",
    "\n",
    "> any of the 20+ methods ([Hooks](https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html#lightning-hooks)) \n",
    "\n",
    "**available in the `LightningModule`**.\n",
    "\n",
    "```python\n",
    "class LitModel(pl.LightningModule):\n",
    "    def backward(self, loss, optimizer, optimizer_idx):\n",
    "        loss.backward()\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Optimization\n",
    "\n",
    "For advanced research topics like reinforcement learning, sparse coding, or GAN research, it may be desirable to manually manage the optimization process.\n",
    "\n",
    "This is only recommended for experts who need ultimate flexibility.\n",
    "\n",
    "Lightning will handle **only accelerator, precision and strategy logic**.\n",
    "\n",
    "**⚠️ The users are left with `optimizer.zero_grad()`, gradient accumulation, model toggling, etc..**\n",
    "\n",
    "To manually optimize, do the following:\n",
    "* Set `self.automatic_optimization=False` in your `LightningModule`’s `__init__`.\n",
    "* Use the following functions and call them manually:\n",
    "    * `self.optimizers()` to access your optimizers (one or multiple)\n",
    "    * `optimizer.zero_grad()` to clear the gradients *from the previous training step*\n",
    "    * [❗] `self.manual_backward(loss)` instead of `loss.backward()`\n",
    "    * `optimizer.step()` to update your model parameters\n",
    "\n",
    "\n",
    "**Here is a minimal example of manual optimization.**\n",
    "\n",
    "```python\n",
    "from pytorch_lightning import LightningModule\n",
    "\n",
    "\n",
    "class MyModel(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Important: This property activates manual optimization.\n",
    "        self.automatic_optimization = False  # NOTE.\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        opt = self.optimizers()\n",
    "        opt.zero_grad()\n",
    "        loss = self.compute_loss(batch)\n",
    "        self.manual_backward(loss)\n",
    "        opt.step()\n",
    "```\n",
    "\n",
    "**NOTE**\n",
    "Be careful where you call `optimizer.zero_grad()`, or your model won’t converge.\n",
    "\n",
    "It is good practice to call `optimizer.zero_grad()` before `self.manual_backward(loss)`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access your Own Optimizer\n",
    "\n",
    "> The provided optimizer is a `LightningOptimizer` object wrapping your own optimizer configured in your `configure_optimizers()`.\n",
    "\n",
    "You can access your own optimizer with `optimizer.optimizer`.\n",
    "\n",
    "⚠️ However, if you use your own optimizer to perform a step, Lightning won’t be able to support accelerators, precision and profiling for you!\n",
    "\n",
    "```python\n",
    "class Model(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.automatic_optimization = False\n",
    "        ...\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        optimizer = self.optimizers()\n",
    "\n",
    "        # `optimizer` is a `LightningOptimizer` wrapping the optimizer.\n",
    "        # To access it, do the following.\n",
    "        # However, it won't work on TPU, AMP, etc...\n",
    "        optimizer = optimizer.optimizer\n",
    "        ...\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Accumulation\n",
    "\n",
    "You can accumulate gradients over batches *similarly to `accumulate_grad_batches`* argument in `Trainer` for automatic optimization.\n",
    "\n",
    "To perform gradient accumulation with one optimizer after every `N` steps, you can do as such.\n",
    "\n",
    "```python\n",
    "def __init__(self):\n",
    "    super().__init__()\n",
    "    self.automatic_optimization = False\n",
    "\n",
    "\n",
    "def training_step(self, batch, batch_idx):\n",
    "    opt = self.optimizers()\n",
    "\n",
    "    loss = self.compute_loss(batch)\n",
    "    self.manual_backward(loss)\n",
    "\n",
    "    # accumulate gradients of N batches\n",
    "    if (batch_idx + 1) % N == 0:  # NOTE.\n",
    "        opt.step()\n",
    "        opt.zero_grad()  # NOTE.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Multiple Optimizers (like GANs)\n",
    "\n",
    "Here is an example training a simple GAN with multiple optimizers using manual optimization.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from pytorch_lightning import LightningModule\n",
    "\n",
    "\n",
    "class SimpleGAN(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.G = Generator()  # NOTE - we've defined this somewhere...\n",
    "        self.D = Discriminator()  # NOTE - we've defined this somewhere...\n",
    "\n",
    "        # Important: This property activates manual optimization.\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "    def sample_z(self, n) -> Tensor:\n",
    "        sample = self._Z.sample((n,))\n",
    "        return sample\n",
    "\n",
    "    def sample_G(self, n) -> Tensor:\n",
    "        z = self.sample_z(n)\n",
    "        return self.G(z)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Implementation follows the PyTorch tutorial:\n",
    "        # https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n",
    "        g_opt, d_opt = self.optimizers()  # NOTE - 2 optimizers returned.\n",
    "\n",
    "        X, _ = batch\n",
    "        batch_size = X.shape[0]\n",
    "\n",
    "        real_label = torch.ones((batch_size, 1), device=self.device)\n",
    "        fake_label = torch.zeros((batch_size, 1), device=self.device)\n",
    "\n",
    "        g_X = self.sample_G(batch_size)\n",
    "\n",
    "        ##########################\n",
    "        # Optimize Discriminator #\n",
    "        ##########################\n",
    "        d_x = self.D(X)\n",
    "        errD_real = self.criterion(d_x, real_label)\n",
    "\n",
    "        d_z = self.D(g_X.detach())\n",
    "        errD_fake = self.criterion(d_z, fake_label)\n",
    "\n",
    "        errD = errD_real + errD_fake\n",
    "\n",
    "        d_opt.zero_grad()\n",
    "        self.manual_backward(errD)\n",
    "        d_opt.step()\n",
    "\n",
    "        ######################\n",
    "        # Optimize Generator #\n",
    "        ######################\n",
    "        d_z = self.D(g_X)\n",
    "        errG = self.criterion(d_z, real_label)\n",
    "\n",
    "        g_opt.zero_grad()\n",
    "        self.manual_backward(errG)\n",
    "        g_opt.step()\n",
    "\n",
    "        self.log_dict({\"g_loss\": errG, \"d_loss\": errD}, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):  # NOTE.\n",
    "        g_opt = torch.optim.Adam(self.G.parameters(), lr=1e-5)\n",
    "        d_opt = torch.optim.Adam(self.D.parameters(), lr=1e-5)\n",
    "        return g_opt, d_opt  # NOTE: Returning 2 optimizers.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Scheduling\n",
    "\n",
    "Every **optimizer** you use can be paired with any [**Learning Rate Scheduler**](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate).\n",
    "\n",
    "Please see the documentation of `configure_optimizers()` for all the available options.\n",
    "\n",
    "> You can call `lr_scheduler.step()` at arbitrary intervals. Use `self.lr_schedulers()` in your `LightningModule` to access any learning rate schedulers defined in your `configure_optimizers()`.\n",
    "\n",
    "**⚠️ Warning**\n",
    "\n",
    "* `lr_scheduler.step()` can be called at arbitrary intervals by the user in case of **manual optimization**, or by Lightning if `\"interval\"` is defined in `configure_optimizers()` in case of automatic optimization.\n",
    "* Note that the `lr_scheduler_config` keys, such as `\"frequency\"` and `\"interval\"`, **will be ignored even if they are provided in your `configure_optimizers()` during *manual optimization***.\n",
    "\n",
    "Here is an example calling `lr_scheduler.step()` every step.\n",
    "\n",
    "```python\n",
    "# step every batch\n",
    "def __init__(self):\n",
    "    super().__init__()\n",
    "    self.automatic_optimization = False\n",
    "\n",
    "\n",
    "def training_step(self, batch, batch_idx):\n",
    "    # do forward, backward, and optimization\n",
    "    ...\n",
    "\n",
    "    # single scheduler\n",
    "    sch = self.lr_schedulers()\n",
    "    sch.step()\n",
    "\n",
    "    # multiple schedulers\n",
    "    sch1, sch2 = self.lr_schedulers()\n",
    "    sch1.step()\n",
    "    sch2.step()\n",
    "```\n",
    "\n",
    "If you want to call `lr_scheduler.step()` every `N` steps/epochs, do the following.\n",
    "\n",
    "```python\n",
    "def __init__(self):\n",
    "    super().__init__()\n",
    "    self.automatic_optimization = False\n",
    "\n",
    "\n",
    "def training_step(self, batch, batch_idx):\n",
    "    # do forward, backward, and optimization\n",
    "    ...\n",
    "\n",
    "    sch = self.lr_schedulers()\n",
    "\n",
    "    # step every N batches\n",
    "    if (batch_idx + 1) % N == 0:  # NOTE.\n",
    "        sch.step()\n",
    "\n",
    "    # step every N epochs\n",
    "    if self.trainer.is_last_batch and (self.trainer.current_epoch + 1) % N == 0:  # NOTE.\n",
    "        sch.step()\n",
    "```\n",
    "\n",
    "If you want to call schedulers that require a *metric value* after each epoch, consider doing the following:\n",
    "```python\n",
    "def __init__(self):\n",
    "    super().__init__()\n",
    "    self.automatic_optimization = False\n",
    "\n",
    "\n",
    "# NOTE!\n",
    "def training_epoch_end(self, outputs):\n",
    "    sch = self.lr_schedulers()\n",
    "\n",
    "    # If the selected scheduler is a ReduceLROnPlateau scheduler.\n",
    "    if isinstance(sch, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "        sch.step(self.trainer.callback_metrics[\"loss\"])\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Closure for LBFGS-like Optimizers\n",
    "\n",
    "See: https://pytorch-lightning.readthedocs.io/en/stable/model/build_model_advanced.html#use-closure-for-lbfgs-like-optimizers"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
