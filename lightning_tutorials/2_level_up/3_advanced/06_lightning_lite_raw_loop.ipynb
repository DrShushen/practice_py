{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following [this](https://pytorch-lightning.readthedocs.io/en/stable/model/build_model_expert.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lightning Lite\n",
    "\n",
    "> [`LightningLite`](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.lite.LightningLite.html#pytorch_lightning.lite.LightningLite)\n",
    "> enables **pure PyTorch users** to `scale their existing code on any kind of device` while retaining **full control over their own loops and optimization logic**.\n",
    "\n",
    "See gif:\n",
    "\n",
    "<img src=\"./assets/lightning_lite.gif\"/>\n",
    "\n",
    "LightningLite is the right tool for you if you match one of the two following descriptions:\n",
    "* I want to quickly scale my existing code to multiple devices with minimal code changes.\n",
    "* I would like to convert my existing code to the Lightning API, but a full path to Lightning transition might be too complex. I am looking for a stepping stone to ensure reproducibility during the transition.\n",
    "\n",
    "> ⚠️ Currently in Beta!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn by example\n",
    "\n",
    "#### My Existing PyTorch Code\n",
    "\n",
    "The `train` function contains a standard training loop used to train `MyModel` on `MyDataset` for `num_epochs` epochs.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    ...\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    ...\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    model = MyModel(...).to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), ...)\n",
    "\n",
    "    dataloader = DataLoader(MyDataset(...), ...)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(args.num_epochs):\n",
    "        for batch in dataloader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = model(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "train(args)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to LightningLite\n",
    "\n",
    "Here are five easy steps to let LightningLite scale your PyTorch models.\n",
    "1. Create the `LightningLite` object at the beginning of your training code.\n",
    "1. Remove all `.to` and `.cuda` calls since `LightningLite` will take care of it.\n",
    "1. Do:\n",
    "    * **apply `setup()` over each model and optimizers pair** and \n",
    "    * **`setup_dataloaders()` on all your dataloaders** and \n",
    "    * **replace `loss.backward()` by `lite.backward(loss)`**.\n",
    "1. Run the script from the terminal using `lightning run model path/to/train.py` or use the `launch()` method in a notebook.\n",
    "\n",
    "Code:\n",
    "```python\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from lightning.lite import LightningLite\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    ...\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    ...\n",
    "\n",
    "\n",
    "def train(args):\n",
    "\n",
    "    lite = LightningLite()  # NOTE.\n",
    "\n",
    "    model = MyModel(...)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), ...)\n",
    "    model, optimizer = lite.setup(model, optimizer)    # NOTE.  # Scale your model / optimizers\n",
    "\n",
    "    dataloader = DataLoader(MyDataset(...), ...)\n",
    "    dataloader = lite.setup_dataloaders(dataloader)    # NOTE.  # Scale your dataloaders\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(args.num_epochs):\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = model(batch)\n",
    "            lite.backward(loss)    # NOTE.  # instead of loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "train(args)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That’s all you need to do to your code. You can now train on any kind of device and scale your training.\n",
    "\n",
    "Check out [this](https://github.com/Lightning-AI/lightning/blob/master/examples/lite/image_classifier_2_lite.py) full MNIST training example with LightningLite.\n",
    "\n",
    "Here is how to train on eight GPUs with [`torch.bfloat16`](https://pytorch.org/docs/1.10.0/generated/torch.Tensor.bfloat16.html) precision:\n",
    "```sh\n",
    "lightning run model ./path/to/train.py --strategy=ddp --devices=8 --accelerator=cuda --precision=\"bf16\"\n",
    "```\n",
    "\n",
    "Here is how to use [DeepSpeed Zero3](https://www.deepspeed.ai/news/2021/03/07/zero3-offload.html) with eight GPUs and mixed precision:\n",
    "```sh\n",
    "lightning run model ./path/to/train.py --strategy=deepspeed --devices=8 --accelerator=cuda --precision=16\n",
    "```\n",
    "\n",
    "`LightningLite` can also figure it out automatically for you!\n",
    "```sh\n",
    "lightning run model ./path/to/train.py --devices=auto --accelerator=auto --precision=16\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also easily use distributed collectives if required.\n",
    "\n",
    "```python\n",
    "lite = LightningLite()\n",
    "\n",
    "# Transfer and concatenate tensors across processes\n",
    "lite.all_gather(...)\n",
    "\n",
    "# Transfer an object from one process to all the others\n",
    "lite.broadcast(..., src=...)\n",
    "\n",
    "# The total number of processes running across all devices and nodes.\n",
    "lite.world_size\n",
    "\n",
    "# The global index of the current process across all devices and nodes.\n",
    "lite.global_rank\n",
    "\n",
    "# The index of the current process among the processes running on the local node.\n",
    "lite.local_rank\n",
    "\n",
    "# The index of the current node.\n",
    "lite.node_rank\n",
    "\n",
    "# Whether this global rank is rank zero.\n",
    "if lite.is_global_zero:\n",
    "    # do something on rank 0\n",
    "    ...\n",
    "\n",
    "# Wait for all processes to enter this call.\n",
    "lite.barrier()\n",
    "```\n",
    "\n",
    "The code stays agnostic, whether you are running on CPU, on two GPUS or on multiple machines with many GPUs.\n",
    "\n",
    "If you require custom data or model device placement, you can deactivate `LightningLite`’s automatic placement by doing `lite.setup_dataloaders(..., move_to_device=False)` for the data and `lite.setup(..., move_to_device=False)` for the model. Furthermore, you can access the current device from `lite.device` or rely on `to_device()` utility to move an object to the current device."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed Training Pitfalls\n",
    "\n",
    "See: https://pytorch-lightning.readthedocs.io/en/stable/model/build_model_expert.html#distributed-training-pitfalls"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lightning Lite Flags\n",
    "\n",
    "See: https://pytorch-lightning.readthedocs.io/en/stable/model/build_model_expert.html#lightning-lite-flags\n",
    "\n",
    "Key things like:\n",
    "* `accelerator`, `devices`\n",
    "* `strategy`\n",
    "* `precision`\n",
    "* `save`, `load`\n",
    "* ..."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
