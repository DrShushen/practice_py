{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following [this](https://pytorch-lightning.readthedocs.io/en/stable/extensions/loops.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [❗] Loops\n",
    "\n",
    "Loops let advanced users **swap out the default gradient descent optimization loop** at the core of Lightning with a **different optimization paradigm**.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Lightning Trainer is built on top of the standard gradient descent optimization loop which works for 90%+ of machine learning use cases:\n",
    "\n",
    "```python\n",
    "for i, batch in enumerate(dataloader):\n",
    "    x, y = batch\n",
    "    y_hat = model(x)\n",
    "    loss = loss_function(y_hat, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, some new research use cases such as \n",
    "* meta-learning,\n",
    "* active learning,\n",
    "* recommendation systems, etc., \n",
    "\n",
    "require a different loop structure.\n",
    "\n",
    "For example here is a simple loop that guides the weight updates with a loss from a special validation split:\n",
    "\n",
    "```python\n",
    "for i, batch in enumerate(train_dataloader):\n",
    "    x, y = batch\n",
    "    y_hat = model(x)\n",
    "    loss = loss_function(y_hat, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Added this whole mechanic:\n",
    "    val_loss = 0\n",
    "    for i, val_batch in enumerate(val_dataloader):\n",
    "        x, y = val_batch\n",
    "        y_hat = model(x)\n",
    "        val_loss += loss_function(y_hat, y)\n",
    "\n",
    "    scale_gradients(model, 1 / val_loss)  # NOTE.\n",
    "    optimizer.step()  # Finally, step.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Lightning Loops, you can customize to non-standard gradient descent optimizations to get the same loop above:\n",
    "\n",
    "```python\n",
    "trainer = Trainer()\n",
    "trainer.fit_loop.epoch_loop = MyGradientDescentLoop()  # NOTE!\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ℹ️ Understanding the Default Trainer Loop\n",
    "\n",
    "The Lightning `Trainer` automates the standard optimization loop which every PyTorch user is familiar with:\n",
    "\n",
    "```python\n",
    "for i, batch in enumerate(dataloader):\n",
    "    x, y = batch\n",
    "    y_hat = model(x)\n",
    "    loss = loss_function(y_hat, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "```\n",
    "\n",
    "The core research logic is simply shifted to the `LightningModule`:\n",
    "\n",
    "```python\n",
    "for i, batch in enumerate(dataloader):\n",
    "    # x, y = batch                      moved to training_step\n",
    "    # y_hat = model(x)                  moved to training_step\n",
    "    # loss = loss_function(y_hat, y)    moved to training_step\n",
    "    loss = lightning_module.training_step(batch, i)\n",
    "\n",
    "    # Lightning handles automatically:\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "```\n",
    "\n",
    "Under the hood, the above loop is implemented using **the `Loop` API** like so:\n",
    "\n",
    "```python\n",
    "class DefaultLoop(Loop):\n",
    "    def advance(self, batch, i):  # NOTE.\n",
    "        loss = lightning_module.training_step(batch, i)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    def run(self, dataloader):  # NOTE.\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            self.advance(batch, i)\n",
    "```\n",
    "\n",
    "Defining a loop within a class interface instead of hard-coding a raw Python for/while loop has several benefits:\n",
    "* You can have full control over the data flow through loops.\n",
    "* You can add new loops and nest as many of them as you want.\n",
    "* If needed, the state of a loop can be [saved and resumed](https://pytorch-lightning.readthedocs.io/en/stable/extensions/loops_advanced.html#persisting-loop-state).\n",
    "* New hooks can be injected at any point.\n",
    "\n",
    "See gif:\n",
    "\n",
    "<img src=\"./assets/epoch-loop-steps.gif\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overriding the default Loops\n",
    "\n",
    "The fastest way to get started with loops, is to override functionality of an existing loop.\n",
    "\n",
    "Lightning has 4 (??) main loops which relies on:\n",
    "* [`FitLoop`](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.loops.FitLoop.html#pytorch_lightning.loops.FitLoop) for fitting (training and validating),\n",
    "* [`EvaluationLoop`](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.loops.dataloader.EvaluationLoop.html#pytorch_lightning.loops.dataloader.EvaluationLoop) for validating or testing,\n",
    "* [`PredictionLoop`](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.loops.dataloader.PredictionLoop.html#pytorch_lightning.loops.dataloader.PredictionLoop) for predicting.\n",
    "\n",
    "For simple changes that don’t require a custom loop, **you can modify each of these loops**.\n",
    "\n",
    "Each loop has **a series of methods that can be modified**. \n",
    "\n",
    "For example with the `FitLoop`:\n",
    "\n",
    "```python\n",
    "from pytorch_lightning.loops import FitLoop\n",
    "\n",
    "\n",
    "class MyLoop(FitLoop):\n",
    "    def advance(self):\n",
    "        \"\"\"Advance from one iteration to the next.\"\"\"\n",
    "\n",
    "    def on_advance_end(self):\n",
    "        \"\"\"Do something at the end of an iteration.\"\"\"\n",
    "\n",
    "    def on_run_end(self):\n",
    "        \"\"\"Do something when the loop ends.\"\"\"\n",
    "```\n",
    "\n",
    "> A full list with all built-in loops and subloops can be found here:\n",
    "> https://pytorch-lightning.readthedocs.io/en/stable/extensions/loops.html#loop-structure-extensions\n",
    "\n",
    "To add your own modifications to a loop, simply subclass an existing loop class and override what you need.\n",
    "\n",
    "Here is a simple example how to add a new hook:\n",
    "\n",
    "```python\n",
    "from pytorch_lightning.loops import FitLoop\n",
    "\n",
    "\n",
    "class CustomFitLoop(FitLoop):\n",
    "    def advance(self):\n",
    "        \"\"\"Put your custom logic here.\"\"\"\n",
    "```\n",
    "\n",
    "Now simply attach the correct loop in the trainer directly:\n",
    "```python\n",
    "trainer = Trainer(...)\n",
    "trainer.fit_loop = CustomFitLoop()  # NOTE.\n",
    "\n",
    "# fit() now uses the new FitLoop!\n",
    "trainer.fit(...)\n",
    "\n",
    "# the equivalent for validate()\n",
    "val_loop = CustomValLoop()\n",
    "trainer = Trainer()\n",
    "trainer.validate_loop = val_loop\n",
    "trainer.validate(...)\n",
    "```\n",
    "\n",
    "Finally, see gif:\n",
    "\n",
    "<img src=\"./assets/replace-fit-loop.gif\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a *New Loop From Scratch*\n",
    "\n",
    "You can also go wild and implement a full loop from scratch by sub-classing the Loop base class.\n",
    "\n",
    "You will need to override *a minimum of two things*:\n",
    "\n",
    "```python\n",
    "from pytorch_lightning.loop import Loop\n",
    "\n",
    "\n",
    "class MyFancyLoop(Loop):\n",
    "    @property\n",
    "    def done(self):\n",
    "        \"\"\"Provide a condition to stop the loop.\"\"\"\n",
    "\n",
    "    def advance(self):\n",
    "        \"\"\"\n",
    "        Access your dataloader/s in whatever way you want.\n",
    "        Do your fancy optimization things.\n",
    "        Call the LightningModule methods at your leisure.\n",
    "        \"\"\"\n",
    "```\n",
    "\n",
    "Finally, attach it into the `Trainer`:\n",
    "```python\n",
    "trainer = Trainer(...)\n",
    "trainer.fit_loop = MyFancyLoop()\n",
    "\n",
    "# fit() now uses your fancy loop!\n",
    "trainer.fit(...)\n",
    "```\n",
    "\n",
    "> ⚠️ But beware: Loop customization gives you more power and full control over the Trainer and with great power comes great responsibility.\n",
    "> We recommend that you familiarize yourself with *overriding the default loops* first before you start building a new loop from the ground up."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop API\n",
    "\n",
    "The `Loop` class is the base of all loops in the same way as the `LightningModule` is the base of all models.\n",
    "It defines a public interface that each loop implementation must follow, the key ones are:\n",
    "\n",
    "See `Loop` API here:\n",
    "\n",
    "https://pytorch-lightning.readthedocs.io/en/stable/extensions/loops.html#loop-api"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subloops\n",
    "\n",
    "**[❓] The explanation in section makes little sense**\n",
    "\n",
    "> When you want to customize **nested loops within loops**, use the `replace()` method:\n",
    "\n",
    "```python\n",
    "# This takes care of properly instantiating the new Loop and setting all references\n",
    "trainer.fit_loop.replace(epoch_loop=MyEpochLoop)\n",
    "# Trainer runs the fit loop with your new epoch loop!\n",
    "trainer.fit(model)\n",
    "```\n",
    "\n",
    "> Alternatively, for more fine-grained control, use the `connect()` method:\n",
    "\n",
    "```python\n",
    "# Optional: stitch back the trainer arguments\n",
    "epoch_loop = MyEpochLoop(trainer.fit_loop.epoch_loop.min_steps, trainer.fit_loop.epoch_loop.max_steps)\n",
    "# Optional: connect children loops as they might have existing state\n",
    "epoch_loop.connect(trainer.fit_loop.epoch_loop.batch_loop, trainer.fit_loop.epoch_loop.val_loop)\n",
    "# Instantiate and connect the loop.\n",
    "trainer.fit_loop.connect(epoch_loop=epoch_loop)\n",
    "trainer.fit(model)\n",
    "```\n",
    "\n",
    "More about the built-in loops and how they are composed is explained in the next section.\n",
    "\n",
    "![img](./assets/connect-epoch-loop.gif)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built-in Loops\n",
    "\n",
    "The training loop in Lightning is called **fit loop** and is actually a combination of several loops.\n",
    "\n",
    "Here is what the structure would look like in plain Python:\n",
    "\n",
    "```python\n",
    "# FitLoop\n",
    "for epoch in range(max_epochs):\n",
    "\n",
    "    # TrainingEpochLoop\n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # TrainingBatchLoop\n",
    "        for split_batch in tbptt_split(batch):\n",
    "\n",
    "            # OptimizerLoop\n",
    "            for optimizer_idx, opt in enumerate(optimizers):\n",
    "\n",
    "                loss = lightning_module.training_step(batch, batch_idx, optimizer_idx)\n",
    "                ...\n",
    "\n",
    "        # ValidationEpochLoop\n",
    "        for batch_idx, batch in enumerate(val_dataloader):\n",
    "            lightning_module.validation_step(batch, batch_idx, optimizer_idx)\n",
    "            ...\n",
    "```\n",
    "\n",
    "* Each of these `for`-loops represents a class implementing the Loop interface.\n",
    "\n",
    "**See also of built-in loops:**\n",
    "\n",
    "https://pytorch-lightning.readthedocs.io/en/stable/extensions/loops.html#built-in-loops\n",
    "\n",
    "* `FitLoop`\n",
    "* `TrainingEpochLoop`\n",
    "* `TrainingBatchLoop`\n",
    "* `OptimizerLoop`\n",
    "* `ManualOptimization`\n",
    "* `EvaluationLoop`\n",
    "* `PredictionLoop`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Available Loops in Lightning Flash\n",
    "\n",
    "> 🤔 Investigate further: [Lightning Flash](https://github.com/Lightning-AI/lightning-flash)\n",
    ">\n",
    "> Recipes for more complex ML scenarios implemented in Lightning.\n",
    "\n",
    "See Active Learning example in the tutorial:\n",
    "\n",
    "https://pytorch-lightning.readthedocs.io/en/stable/extensions/loops.html#available-loops-in-lightning-flash\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Examples\n",
    "\n",
    "https://pytorch-lightning.readthedocs.io/en/stable/extensions/loops.html#advanced-examples\n",
    "\n",
    "* [`K-fold Cross Validation`](https://github.com/Lightning-AI/lightning/blob/master/examples/pl_loops/kfold.py)\n",
    "* [`Yielding Training Step`](https://github.com/Lightning-AI/lightning/blob/master/examples/pl_loops/yielding_training_step.py)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn_py38_lightning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3a117739a79adb8d3a3cb28cb225d24b95a2a8e6607bba35806671d6c7dba5eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
