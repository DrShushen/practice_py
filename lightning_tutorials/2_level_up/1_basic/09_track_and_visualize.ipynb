{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following [this](https://pytorch-lightning.readthedocs.io/en/stable/visualize/logging_basic.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track and Visualize Experiments (basic)\n",
    "\n",
    "### Why do I need to track metrics?\n",
    "\n",
    "In model development, we track values of interest such as the `validation_loss` to visualize the learning process for\n",
    "our models.\n",
    "Model development is like driving a car without windows, charts and logs provide the windows to know where to drive the car.\n",
    "\n",
    "With Lightning, you can visualize virtually anything you can think of: numbers, text, images, audio.\n",
    "Your creativity and imagination are the only limiting factor.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track metrics\n",
    "\n",
    "Metric visualization is the most basic but powerful way of understanding how your model is doing throughout the model development process.\n",
    "\n",
    "To track a metric, simply use the `self.log` method available inside the `LightningModule`\n",
    "\n",
    "```python\n",
    "class LitModel(pl.LightningModule):\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        value = ...\n",
    "        self.log(\"some_value\", value)  # NOTE.\n",
    "```\n",
    "\n",
    "To log multiple metrics at once, use `self.log_dict`\n",
    "\n",
    "```python\n",
    "values = {\"loss\": loss, \"acc\": acc, \"metric_n\": metric_n}  # add more items if needed\n",
    "self.log_dict(values)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View in the commandline\n",
    "\n",
    "To view metrics in the commandline progress bar, set the `prog_bar` argument to `True`.\n",
    "\n",
    "```python\n",
    "self.log(..., prog_bar=True)\n",
    "\n",
    "# Progress bar will show your metric...\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View in the browser\n",
    "\n",
    "To view metrics in the browser you need to use an experiment manager with these capabilities.\n",
    "\n",
    "By Default, Lightning uses *Tensorboard* which is free and opensource.\n",
    "\n",
    "Tensorboard is already enabled by default.\n",
    "\n",
    "```python\n",
    "# every trainer already has tensorboard enabled by default\n",
    "trainer = Trainer()\n",
    "```\n",
    "\n",
    "To launch the tensorboard dashboard run the following command on the commandline.\n",
    "\n",
    "```sh\n",
    "tensorboard --logdir=lightning_logs/\n",
    "```\n",
    "\n",
    "If you’re using a notebook environment such as colab or kaggle or jupyter, launch Tensorboard with this command\n",
    "\n",
    "```python\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=lightning_logs/\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⚠️ Accumulate a metric\n",
    "\n",
    "When `self.log` is called inside the `training_step`, it generates a timeseries showing how the metric behaves over time.\n",
    "\n",
    "However, For the **validation and test sets** we are not generally interested in plotting the metric *values per batch* of data.\n",
    "Instead, we want to compute a **summary statistic** (such as average, min or max) **across the full split of data**.\n",
    "\n",
    "⚠️ When you call `self.log` inside the `validation_step` and `test_step`,\n",
    "Lightning automatically accumulates the metric and averages it once it’s gone through the whole split (epoch).\n",
    "```python\n",
    "def validation_step(self, batch, batch_idx):\n",
    "    value = batch_idx + 1\n",
    "    self.log(\"average_value\", value)\n",
    "```\n",
    "\n",
    "If you don’t want to average you can also choose from `{min,max,sum}` by passing the `reduce_fx` argument.\n",
    "\n",
    "```python\n",
    "# default function\n",
    "self.log(..., reduce_fx=\"mean\")\n",
    "```\n",
    "\n",
    "For other reductions, we recommend logging a [`torchmetrics.Metric`](https://torchmetrics.readthedocs.io/en/stable/pages/implement.html#torchmetrics.Metric) instance instead."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the saving directory\n",
    "\n",
    "By default, anything that is logged is saved to the current working directory.\n",
    "\n",
    "To use a different directory, set the `default_root_dir` argument in the `Trainer`.\n",
    "\n",
    "```python\n",
    "Trainer(default_root_dir=\"/your/custom/path\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
