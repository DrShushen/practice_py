{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following [this](https://pytorch-lightning.readthedocs.io/en/stable/common/checkpointing_basic.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a checkpoint?\n",
    "\n",
    "When a model is training, the performance changes as it continues to see more data. It is a best practice to save the state of a model throughout the training process. This gives you a version of the model, a checkpoint, at each key point during the development of the model. Once training has completed, use the checkpoint that corresponds to the best performance you found during the training process.\n",
    "\n",
    "Checkpoints also enable your training to resume from where it was in case the training process is interrupted.\n",
    "\n",
    "PyTorch Lightning checkpoints are fully usable in plain PyTorch.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents of a checkpoint\n",
    "\n",
    "A Lightning checkpoint contains a dump of the model’s entire internal state.\n",
    "\n",
    "> Unlike plain PyTorch, Lightning saves everything you need to restore a model even in the most complex distributed training environments.\n",
    "\n",
    "Inside a Lightning checkpoint you’ll find:\n",
    "* 16-bit scaling factor (if using 16-bit precision training)\n",
    "* Current epoch\n",
    "* Global step\n",
    "* LightningModule’s `state_dict`\n",
    "* State of all optimizers\n",
    "* State of all learning rate schedulers\n",
    "* State of all callbacks (for stateful callbacks)\n",
    "* State of datamodule (for stateful datamodules)\n",
    "* The hyperparameters used for that model if passed in as hparams (`Argparse.Namespace`)\n",
    "* The hyperparameters used for that datamodule if passed in as hparams (`Argparse.Namespace`)\n",
    "* State of Loops (if using Fault-Tolerant training)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save a checkpoint\n",
    "\n",
    "Lightning **automatically saves a checkpoint** for you in your current working directory,\n",
    "with the state of your last training epoch.\n",
    "\n",
    "This makes sure you can resume training in case it was interrupted.\n",
    "\n",
    "```python\n",
    "# simply by using the Trainer you get automatic checkpointing\n",
    "trainer = Trainer()\n",
    "\n",
    "# To change path: \n",
    "# saves checkpoints to 'some/path/' at every epoch end\n",
    "trainer = Trainer(default_root_dir=\"some/path/\")\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightningModule from checkpoint\n",
    "\n",
    "To load a `LightningModule` along with its weights and hyperparameters use the following method:\n",
    "\n",
    "```python\n",
    "model = MyLightningModule.load_from_checkpoint(\"/path/to/checkpoint.ckpt\")  # NOTE.\n",
    "\n",
    "# disable randomness, dropout, etc...\n",
    "model.eval()  # NOTE!\n",
    "\n",
    "# predict with the model\n",
    "y_hat = model(x)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save hyperparameters\n",
    "\n",
    "The `LightningModule` allows you to *automatically* save all the hyperparameters **passed to `init`**\n",
    "simply by calling `self.save_hyperparameters()`.\n",
    "\n",
    "```python\n",
    "class MyLightningModule(LightningModule):\n",
    "    def __init__(self, learning_rate, another_parameter, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()  # NOTE.\n",
    "```\n",
    "\n",
    "The hyperparameters are saved to the “hyper_parameters” key in the checkpoint.\n",
    "```python\n",
    "checkpoint = torch.load(checkpoint, map_location=lambda storage, loc: storage)\n",
    "print(checkpoint[\"hyper_parameters\"])\n",
    "# {\"learning_rate\": the_value, \"another_parameter\": the_other_value}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize with *other* parameters\n",
    "\n",
    "If you used the `self.save_hyperparameters()` method in the init of the `LightningModule`,\n",
    "you can initialize the model with **different hyperparameters**.\n",
    "\n",
    "```python\n",
    "# if you train and save the model like this it will use these values when loading\n",
    "# the weights. But you can overwrite this\n",
    "LitModel(in_dim=32, out_dim=10)\n",
    "\n",
    "# uses in_dim=32, out_dim=10\n",
    "model = LitModel.load_from_checkpoint(PATH)  # NOTE!\n",
    "\n",
    "# uses in_dim=128, out_dim=10\n",
    "model = LitModel.load_from_checkpoint(PATH, in_dim=128, out_dim=10)  # NOTE!\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Module from checkpoint\n",
    "\n",
    "> Lightning checkpoints are fully compatible with plain torch `nn.Modules`.#\n",
    "\n",
    "```python\n",
    "checkpoint = torch.load(CKPT_PATH)\n",
    "print(checkpoint.keys())\n",
    "```\n",
    "\n",
    "For example, let’s pretend we created a LightningModule like so:\n",
    "```python\n",
    "class Encoder(nn.Module):\n",
    "    ...\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    ...\n",
    "\n",
    "\n",
    "class Autoencoder(pl.LightningModule):\n",
    "    def __init__(self, encoder, decoder, *args, **kwargs):\n",
    "        ...\n",
    "\n",
    "\n",
    "autoencoder = Autoencoder(Encoder(), Decoder())\n",
    "```\n",
    "\n",
    "Once the autoencoder has trained, pull out the relevant weights for your torch nn.Module:\n",
    "```python\n",
    "checkpoint = torch.load(CKPT_PATH)\n",
    "encoder_weights = checkpoint[\"encoder\"]\n",
    "decoder_weights = checkpoint[\"decoder\"]\n",
    "```\n",
    "\n",
    "> As in, you can extract the \"standard\" torch checkpoint information from a lightning checkpoint."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disable checkpointing\n",
    "\n",
    "```python\n",
    "trainer = Trainer(enable_checkpointing=False)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resume training state\n",
    "\n",
    "If you don’t just want to load weights, **but instead restore the full training**, do the following:\n",
    "\n",
    "```python\n",
    "model = LitModel()\n",
    "trainer = Trainer()\n",
    "\n",
    "# automatically restores model, epoch, step, LR schedulers, apex, etc...\n",
    "trainer.fit(model, ckpt_path=\"some/path/to/my_checkpoint.ckpt\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
