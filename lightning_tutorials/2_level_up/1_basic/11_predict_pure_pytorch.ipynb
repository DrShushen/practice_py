{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following [this](https://pytorch-lightning.readthedocs.io/en/stable/deploy/production_intermediate.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use PyTorch as normal\n",
    "\n",
    "If you prefer to use PyTorch directly, feel free to use any Lightning checkpoint without Lightning.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    ...\n",
    "\n",
    "\n",
    "model = MyModel()\n",
    "\n",
    "# Pure PyTorch loading.\n",
    "checkpoint = torch.load(\"path/to/lightning/checkpoint.ckpt\")\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "\n",
    "# Pure PyTorch evaluation.\n",
    "model.eval()\n",
    "# ...\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract nn.Module from Lightning checkpoints\n",
    "\n",
    "You can also load the saved checkpoint and use it as a regular `torch.nn.Module`.\n",
    "\n",
    "You can extract all your `torch.nn.Module` and load the weights using the checkpoint saved using LightningModule after training.\n",
    "\n",
    "> For this, we recommend copying the exact implementation from your LightningModule init and forward method.\n",
    "\n",
    "```python\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    ...\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    ...\n",
    "\n",
    "\n",
    "class AutoEncoderProd(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "\n",
    "class AutoEncoderSystem(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.auto_encoder = AutoEncoderProd()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.auto_encoder.encoder(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.auto_encoder.encoder(x)\n",
    "        y_hat = self.auto_encoder.decoder(y_hat)\n",
    "        loss = ...\n",
    "        return loss\n",
    "\n",
    "\n",
    "# train it\n",
    "trainer = Trainer(devices=2, accelerator=\"gpu\", strategy=\"ddp\")\n",
    "model = AutoEncoderSystem()\n",
    "trainer.fit(model, train_dataloader, val_dataloader)\n",
    "trainer.save_checkpoint(\"best_model.ckpt\")\n",
    "\n",
    "\n",
    "# create the PyTorch model and load the checkpoint weights\n",
    "model = AutoEncoderProd()\n",
    "checkpoint = torch.load(\"best_model.ckpt\")\n",
    "hyper_parameters = checkpoint[\"hyper_parameters\"]\n",
    "\n",
    "# if you want to restore any hyperparameters, you can pass them too\n",
    "model = AutoEncoderProd(**hyper_parameters)\n",
    "\n",
    "model_weights = checkpoint[\"state_dict\"]\n",
    "\n",
    "# update keys by dropping `auto_encoder.`\n",
    "for key in list(model_weights):\n",
    "    model_weights[key.replace(\"auto_encoder.\", \"\")] = model_weights.pop(key)\n",
    "\n",
    "model.load_state_dict(model_weights)\n",
    "\n",
    "# Finally predict...\n",
    "model.eval()\n",
    "x = torch.randn(1, 64)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_hat = model(x)\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
