{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following [this](https://pytorch-lightning.readthedocs.io/en/stable/common/hyperparameters.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure hyperparameters from the CLI\n",
    "\n",
    "Lightning has utilities to interact seamlessly with the command line `ArgumentParser` and plays well with the\n",
    "hyperparameter optimization framework of your choice."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ArgumentParser\n",
    "\n",
    "Lightning is designed to augment a lot of the functionality of the built-in Python `ArgumentParser`.\n",
    "\n",
    "```python\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "parser = ArgumentParser()  # Using the built in ArgumentParser.\n",
    "parser.add_argument(\"--layer_1_dim\", type=int, default=128)\n",
    "args = parser.parse_args()\n",
    "```\n",
    "\n",
    "Allows:\n",
    "```sh\n",
    "python trainer.py --layer_1_dim 64\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Argparser Best Practices\n",
    "\n",
    "ℹ️ **Useful idea**\n",
    "\n",
    "It is best practice to layer your arguments in three sections.\n",
    "* Trainer args (`accelerator`, `devices`, `num_nodes`, etc…)\n",
    "* Model specific arguments (`layer_dim`, `num_layers`, `learning_rate`, etc…)\n",
    "* Program arguments (`data_path`, `cluster_email`, etc…)\n",
    "\n",
    "We can do this as follows.\n",
    "\n",
    "1.\n",
    "First, in your `LightningModule`, define the arguments specific to that module.\n",
    "Remember that data splits or data paths may also be specific to a module\n",
    "(i.e.: if your project has a model that trains on Imagenet and another on CIFAR-10).\n",
    "```python\n",
    "class LitModel(LightningModule):\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):  # NOTE This. parent_parser is passed in.\n",
    "        parser = parent_parser.add_argument_group(\"LitModel\")\n",
    "        parser.add_argument(\"--encoder_layers\", type=int, default=12)\n",
    "        parser.add_argument(\"--data_path\", type=str, default=\"/some/path\")\n",
    "        return parent_parser\n",
    "```\n",
    "\n",
    "2.\n",
    "Now in your main trainer file, add the Trainer args, the program args, and add the model args.\n",
    "```python\n",
    "# ----------------\n",
    "# trainer_main.py\n",
    "# ----------------\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "parser = ArgumentParser()\n",
    "\n",
    "# add PROGRAM level args\n",
    "parser.add_argument(\"--conda_env\", type=str, default=\"some_name\")\n",
    "parser.add_argument(\"--notification_email\", type=str, default=\"will@email.com\")\n",
    "\n",
    "# add model specific args\n",
    "parser = LitModel.add_model_specific_args(parser)  # NOTE.\n",
    "\n",
    "# add all the available trainer options to argparse\n",
    "# ie: now --accelerator --devices --num_nodes ... --fast_dev_run all work in the cli\n",
    "parser = Trainer.add_argparse_args(parser)  # NOTE.\n",
    "\n",
    "args = parser.parse_args()\n",
    "```\n",
    "\n",
    "Now you can call run your program like so:\n",
    "```python\n",
    "# init the trainer like this\n",
    "trainer = Trainer.from_argparse_args(args, early_stopping_callback=...)  # NOTE: Like so.\n",
    "\n",
    "# NOT like this\n",
    "trainer = Trainer(accelerator=hparams.accelerator, devices=hparams.devices, ...)  # NOTE: NOT like so.\n",
    "\n",
    "# init the model with Namespace directly\n",
    "model = LitModel(args)\n",
    "\n",
    "# or init the model with all the key-value pairs\n",
    "# NOTE this pattern.\n",
    "dict_args = vars(args)\n",
    "model = LitModel(**dict_args)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightningModule hyperparameters\n",
    "\n",
    "Often times we train many versions of a model. You might share that model or come back to it a few months later at which point it is very useful to know how that model was trained (i.e.: what learning rate, neural network, etc…).\n",
    "\n",
    "Lightning has a standardized way of saving the information for you in checkpoints and YAML files. The goal here is to improve readability and reproducibility.\n",
    "\n",
    "#### `save_hyperparameters()`\n",
    "\n",
    "Use `save_hyperparameters()` within your `LightningModule`’s `__init__` method.\n",
    "\n",
    "It will enable Lightning to store all the provided arguments under the `self.hparams` attribute.\n",
    "These hyperparameters will also be stored within the model checkpoint, which simplifies model re-instantiation after training.\n",
    "\n",
    "⚠️ Note the below carefully - relevant to things like delayed initialisation of parameters.\n",
    "```python\n",
    "class LitMNIST(LightningModule):\n",
    "    def __init__(self, layer_1_dim=128, learning_rate=1e-2):\n",
    "        super().__init__()\n",
    "        # call this to save (layer_1_dim=128, learning_rate=1e-4) to the checkpoint\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # equivalent\n",
    "        self.save_hyperparameters(\"layer_1_dim\", \"learning_rate\")\n",
    "\n",
    "        # Now possible to access layer_1_dim from hparams\n",
    "        self.hparams.layer_1_dim\n",
    "```\n",
    "\n",
    "In addition, loggers that support it will automatically log the contents of `self.hparams`.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Excluding hyperparameters\n",
    "\n",
    "By default, every parameter of the `__init__` method will be considered a hyperparameter to the `LightningModule`.\n",
    "\n",
    "However, sometimes some parameters need to be excluded from saving, for example when they are not serializable.\n",
    "\n",
    "Those parameters should be provided back when reloading the `LightningModule`.\n",
    "\n",
    "In this case, exclude them explicitly:\n",
    "\n",
    "```python\n",
    "class LitMNIST(LightningModule):\n",
    "    def __init__(self, loss_fx, generator_network, layer_1_dim=128):\n",
    "        super().__init__()\n",
    "        self.layer_1_dim = layer_1_dim\n",
    "        self.loss_fx = loss_fx\n",
    "\n",
    "        # call this to save only (layer_1_dim=128) to the checkpoint\n",
    "        self.save_hyperparameters(\"layer_1_dim\")  # NOTE.\n",
    "\n",
    "        # equivalent\n",
    "        self.save_hyperparameters(ignore=[\"loss_fx\", \"generator_network\"])  # NOTE.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `load_from_checkpoint()`\n",
    "\n",
    "`LightningModules` that have hyperparameters automatically saved with `save_hyperparameters()` can conveniently be\n",
    "loaded and instantiated directly from a checkpoint with `load_from_checkpoint()`.\n",
    "\n",
    "If parameters were excluded, they need to be provided at the time of loading.\n",
    "\n",
    "```python\n",
    "# to load specify *the other args*\n",
    "# the excluded parameters were `loss_fx` and `generator_network`\n",
    "model = LitMNIST.load_from_checkpoint(PATH, loss_fx=torch.nn.SomeOtherLoss, generator_network=MyGenerator())\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer args\n",
    "\n",
    "To recap, add ALL possible trainer flags to the argparser and init the `Trainer` this way:\n",
    "\n",
    "```python\n",
    "parser = ArgumentParser()\n",
    "parser = Trainer.add_argparse_args(parser)\n",
    "hparams = parser.parse_args()\n",
    "\n",
    "trainer = Trainer.from_argparse_args(hparams)\n",
    "\n",
    "# or if you need to pass in callbacks\n",
    "trainer = Trainer.from_argparse_args(hparams, enable_checkpointing=..., callbacks=[...])\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Lightning Modules\n",
    "\n",
    "We often have multiple Lightning Modules where each one has different arguments.\n",
    "\n",
    "Instead of polluting the `main.py` file, the `LightningModule` lets you define arguments for each one.\n",
    "\n",
    "```python\n",
    "class LitMNIST(LightningModule):\n",
    "    def __init__(self, layer_1_dim, **kwargs):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(28 * 28, layer_1_dim)\n",
    "\n",
    "    # NOTE:\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):\n",
    "        parser = parent_parser.add_argument_group(\"LitMNIST\")\n",
    "        parser.add_argument(\"--layer_1_dim\", type=int, default=128)\n",
    "        return parent_parser\n",
    "\n",
    "class GoodGAN(LightningModule):\n",
    "    def __init__(self, encoder_layers, **kwargs):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(layers=encoder_layers)\n",
    "\n",
    "    # NOTE:\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):\n",
    "        parser = parent_parser.add_argument_group(\"GoodGAN\")\n",
    "        parser.add_argument(\"--encoder_layers\", type=int, default=12)\n",
    "        return parent_parser\n",
    "```\n",
    "\n",
    "Now we can allow each model to inject the arguments it needs in the `main.py`:\n",
    "```python\n",
    "def main(args):\n",
    "    dict_args = vars(args)\n",
    "\n",
    "    # pick model\n",
    "    if args.model_name == \"gan\":\n",
    "        model = GoodGAN(**dict_args)\n",
    "    elif args.model_name == \"mnist\":\n",
    "        model = LitMNIST(**dict_args)\n",
    "\n",
    "    trainer = Trainer.from_argparse_args(args)\n",
    "    trainer.fit(model)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = ArgumentParser()\n",
    "    parser = Trainer.add_argparse_args(parser)\n",
    "\n",
    "    # figure out which model to use\n",
    "    parser.add_argument(\"--model_name\", type=str, default=\"gan\", help=\"gan or mnist\")\n",
    "\n",
    "    # THIS LINE IS KEY TO PULL THE MODEL NAME\n",
    "    temp_args, _ = parser.parse_known_args()\n",
    "\n",
    "    # let the model add what it wants\n",
    "    if temp_args.model_name == \"gan\":\n",
    "        parser = GoodGAN.add_model_specific_args(parser)\n",
    "    elif temp_args.model_name == \"mnist\":\n",
    "        parser = LitMNIST.add_model_specific_args(parser)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # train\n",
    "    main(args)\n",
    "```\n",
    "\n",
    "⚠️ BTW, it looks like in this example it's literally just *this model or that model*, they're not loading both...\n",
    "\n",
    "```sh\n",
    "$ python main.py --model_name gan --encoder_layers 24\n",
    "$ python main.py --model_name mnist --layer_1_dim 128\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn_py38_lightning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3a117739a79adb8d3a3cb28cb225d24b95a2a8e6607bba35806671d6c7dba5eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
