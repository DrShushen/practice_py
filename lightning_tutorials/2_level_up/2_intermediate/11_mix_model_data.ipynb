{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following [this](https://pytorch-lightning.readthedocs.io/en/stable/cli/lightning_cli_intermediate_2.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do I want to mix models and datasets\n",
    "\n",
    "Lightning projects usually begin with one model and one dataset.\n",
    "\n",
    "As the project grows in complexity and you introduce more models and more datasets, it becomes desirable to mix\n",
    "any model with any dataset directly from the commandline without changing your code.\n",
    "\n",
    "```sh\n",
    "# Mix and match anything\n",
    "$ python main.py fit --model=GAN --data=MNIST\n",
    "$ python main.py fit --model=Transformer --data=MNIST\n",
    "```\n",
    "\n",
    "This is what the Lightning CLI enables.\n",
    "\n",
    "Otherwise, this kind of configuration requires a significant amount of boilerplate that often looks like this:\n",
    "\n",
    "```python\n",
    "# Example boilerplate...\n",
    "\n",
    "# choose model\n",
    "if args.model == \"gan\":\n",
    "    model = GAN(args.feat_dim)\n",
    "elif args.model == \"transformer\":\n",
    "    model = Transformer(args.feat_dim)\n",
    "...\n",
    "\n",
    "# choose datamodule\n",
    "if args.data == \"MNIST\":\n",
    "    datamodule = MNIST()\n",
    "elif args.data == \"imagenet\":\n",
    "    datamodule = Imagenet()\n",
    "...\n",
    "\n",
    "# mix them!\n",
    "trainer.fit(model, datamodule)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple LightningModules\n",
    "\n",
    "To support multiple models, when instantiating LightningCLI **omit the `model_class` parameter**:\n",
    "\n",
    "```python\n",
    "# main.py\n",
    "\n",
    "from pytorch_lightning import demos\n",
    "from pytorch_lightning.utilities import cli as pl_cli\n",
    "\n",
    "\n",
    "class Model1(DemoModel):\n",
    "    def configure_optimizers(self):\n",
    "        print(\"⚡\", \"using Model1\", \"⚡\")\n",
    "        return super().configure_optimizers()\n",
    "\n",
    "\n",
    "class Model2(DemoModel):\n",
    "    def configure_optimizers(self):\n",
    "        print(\"⚡\", \"using Model2\", \"⚡\")\n",
    "        return super().configure_optimizers()\n",
    "\n",
    "\n",
    "cli = pl_cli.LightningCLI(datamodule_class=BoringDataModule)\n",
    "```\n",
    "\n",
    "Now you can choose between any model from the CLI:\n",
    "\n",
    "```bash\n",
    "# use Model1\n",
    "python main.py fit --model Model1\n",
    "\n",
    "# use Model2\n",
    "python main.py fit --model Model2\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple DataModules\n",
    "\n",
    "To support multiple data modules, when instantiating LightningCLI omit the datamodule_class parameter:\n",
    "\n",
    "```python\n",
    "# main.py\n",
    "import torch\n",
    "from pytorch_lightning.utilities import cli as pl_cli\n",
    "from pytorch_lightning import demos\n",
    "\n",
    "\n",
    "class FakeDataset1(BoringDataModule):\n",
    "    def train_dataloader(self):\n",
    "        print(\"⚡\", \"using FakeDataset1\", \"⚡\")\n",
    "        return torch.utils.data.DataLoader(self.random_train)\n",
    "\n",
    "\n",
    "class FakeDataset2(BoringDataModule):\n",
    "    def train_dataloader(self):\n",
    "        print(\"⚡\", \"using FakeDataset2\", \"⚡\")\n",
    "        return torch.utils.data.DataLoader(self.random_train)\n",
    "\n",
    "\n",
    "cli = pl_cli.LightningCLI(DemoModel)\n",
    "```\n",
    "\n",
    "Now you can choose between any dataset at runtime:\n",
    "\n",
    "```sh\n",
    "# use Model1\n",
    "python main.py fit --data FakeDataset1\n",
    "\n",
    "# use Model2\n",
    "python main.py fit --data FakeDataset2\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom optimizers\n",
    "\n",
    "Any subclass of `torch.optim.Optimizer` can be used as an optimizer:\n",
    "\n",
    "```python\n",
    "# main.py\n",
    "import torch\n",
    "from pytorch_lightning.utilities import cli as pl_cli\n",
    "from pytorch_lightning import demos\n",
    "\n",
    "\n",
    "class LitAdam(torch.optim.Adam):\n",
    "    def step(self, closure):\n",
    "        print(\"⚡\", \"using LitAdam\", \"⚡\")\n",
    "        super().step(closure)\n",
    "\n",
    "\n",
    "class FancyAdam(torch.optim.Adam):\n",
    "    def step(self, closure):\n",
    "        print(\"⚡\", \"using FancyAdam\", \"⚡\")\n",
    "        super().step(closure)\n",
    "\n",
    "\n",
    "cli = pl_cli.LightningCLI(DemoModel, BoringDataModule)  # NOTE: No need to pass in optimizers, just define them above.\n",
    "```\n",
    "\n",
    "Now you can choose between any optimizer at runtime:\n",
    "\n",
    "```sh\n",
    "# use LitAdam\n",
    "python main.py fit --optimizer LitAdam\n",
    "\n",
    "# use FancyAdam\n",
    "python main.py fit --optimizer FancyAdam\n",
    "```\n",
    "\n",
    "Bonus: If you need only 1 optimizer, the Lightning CLI already works out of the box with any Optimizer from torch.optim:\n",
    "\n",
    "```sh\n",
    "python main.py fit --optimizer AdamW\n",
    "```\n",
    "\n",
    "If the optimizer you want needs other arguments, add them via the CLI (no need to change your code)!\n",
    "\n",
    "```sh\n",
    "python main.py fit --optimizer SGD --optimizer.lr=0.01\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom LR schedulers\n",
    "\n",
    "Any subclass of `torch.optim.lr_scheduler._LRScheduler` can be used as learning rate scheduler:\n",
    "\n",
    "```python\n",
    "# main.py\n",
    "import torch\n",
    "from pytorch_lightning.utilities import cli as pl_cli\n",
    "from pytorch_lightning import demos\n",
    "\n",
    "\n",
    "class LitLRScheduler(torch.optim.lr_scheduler.CosineAnnealingLR):\n",
    "    def step(self):\n",
    "        print(\"⚡\", \"using LitLRScheduler\", \"⚡\")\n",
    "        super().step()\n",
    "\n",
    "\n",
    "cli = pl_cli.LightningCLI(DemoModel, BoringDataModule)\n",
    "```\n",
    "\n",
    "Now you can choose between any learning rate scheduler at runtime:\n",
    "\n",
    "```sh\n",
    "# LitLRScheduler\n",
    "python main.py fit --lr_scheduler LitLRScheduler\n",
    "```\n",
    "\n",
    "Bonus: If you need only 1 LRScheduler, the Lightning CLI already works out of the box with any LRScheduler from torch.optim:\n",
    "\n",
    "```sh\n",
    "python main.py fit --lr_scheduler CosineAnnealingLR\n",
    "python main.py fit --lr_scheduler LinearLR\n",
    "...\n",
    "```\n",
    "\n",
    "If the scheduler you want needs other arguments, add them via the CLI (no need to change your code)!\n",
    "\n",
    "```sh\n",
    "python main.py fit --lr_scheduler=ReduceLROnPlateau --lr_scheduler.monitor=epoch\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes from any package\n",
    "\n",
    "> In the previous sections the classes to select were defined in the same python file where the LightningCLI class is run.\n",
    "\n",
    "To select classes from *any package* by using only the class name, *import the respective package*:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from pytorch_lightning.utilities import cli as pl_cli\n",
    "import my_code.models  # noqa: F401\n",
    "import my_code.data_modules  # noqa: F401\n",
    "import my_code.optimizers  # noqa: F401\n",
    "\n",
    "cli = pl_cli.LightningCLI()\n",
    "```\n",
    "\n",
    "Now use any of the classes:\n",
    "\n",
    "```sh\n",
    "python main.py fit --model Model1 --data FakeDataset1 --optimizer LitAdam --lr_scheduler LitLRScheduler\n",
    "```\n",
    "\n",
    "The `# noqa: F401` comment avoids a linter warning that the import is unused.\n",
    "\n",
    "It is also possible to select subclasses that have not been imported by giving the full import path:\n",
    "\n",
    "```sh\n",
    "python main.py fit --model my_code.models.Model1\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
