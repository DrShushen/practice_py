{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following [this](https://pytorch-lightning.readthedocs.io/en/stable/data/datamodule.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightningDataModule\n",
    "\n",
    "A datamodule is a shareable, reusable class that encapsulates all the steps needed to process data.\n",
    "\n",
    "<img src=\"./assets/datamodule_overview.png\" width=1000/>\n",
    "\n",
    "<br/>\n",
    "\n",
    "<img src=\"./assets/datamodule_use.png\" width=200/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A datamodule encapsulates the five steps involved in data processing in PyTorch:\n",
    "\n",
    "* Download / tokenize / process.\n",
    "* Clean and (maybe) save to disk.\n",
    "* Load inside `Dataset`.\n",
    "* Apply transforms (rotate, tokenize, etc…).\n",
    "* Wrap inside a `DataLoader`.\n",
    "\n",
    "This class can then be shared and used anywhere:\n",
    "```python\n",
    "from pl_bolts.datamodules import CIFAR10DataModule, ImagenetDataModule\n",
    "\n",
    "model = LitClassifier()\n",
    "trainer = Trainer()\n",
    "\n",
    "imagenet = ImagenetDataModule()\n",
    "trainer.fit(model, datamodule=imagenet)\n",
    "\n",
    "cifar10 = CIFAR10DataModule()\n",
    "trainer.fit(model, datamodule=cifar10)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Why do I need a DataModule?\n",
    "\n",
    "In normal PyTorch code, the data cleaning/preparation is usually scattered across many files. This makes sharing and reusing the exact splits and transforms across projects impossible.\n",
    "\n",
    "Datamodules are for you if you ever asked the questions:\n",
    "* what splits did you use?\n",
    "* what transforms did you use?\n",
    "* what normalization did you use?\n",
    "* how did you prepare/tokenize the data?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a DataModule?\n",
    "\n",
    "[❗] A DataModule is simply a collection of:\n",
    "* a train_dataloader(s), \n",
    "* val_dataloader(s),\n",
    "* test_dataloader(s) and\n",
    "* predict_dataloader(s)\n",
    "* along with the matching transforms\n",
    "* and data processing/downloads steps required.\n",
    "\n",
    "Here’s a simple PyTorch example:\n",
    "\n",
    "```python\n",
    "# regular PyTorch\n",
    "test_data = MNIST(my_path, train=False, download=True)\n",
    "predict_data = MNIST(my_path, train=False, download=True)\n",
    "train_data = MNIST(my_path, train=True, download=True)\n",
    "train_data, val_data = random_split(train_data, [55000, 5000])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32)\n",
    "val_loader = DataLoader(val_data, batch_size=32)\n",
    "test_loader = DataLoader(test_data, batch_size=32)\n",
    "predict_loader = DataLoader(predict_data, batch_size=32)\n",
    "```\n",
    "\n",
    "The equivalent DataModule just organizes the same exact code, but makes it reusable across projects.\n",
    "\n",
    "```python\n",
    "class MNISTDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir: str = \"path/to/dir\", batch_size: int = 32):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        self.mnist_test = MNIST(self.data_dir, train=False)\n",
    "        self.mnist_predict = MNIST(self.data_dir, train=False)\n",
    "        mnist_full = MNIST(self.data_dir, train=True)\n",
    "        self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, batch_size=self.batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.mnist_test, batch_size=self.batch_size)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.mnist_predict, batch_size=self.batch_size)\n",
    "\n",
    "    def teardown(self, stage: str):\n",
    "        # Used to clean-up when the run is finished\n",
    "        ...\n",
    "```\n",
    "\n",
    "But now, as the complexity of your processing grows (transforms, multiple-GPU training),\n",
    "you can let Lightning handle those details for you while making this dataset reusable\n",
    "so you can share with colleagues or use in different projects.\n",
    "\n",
    "```python\n",
    "mnist = MNISTDataModule(my_path)\n",
    "model = LitClassifier()\n",
    "\n",
    "trainer = Trainer()\n",
    "trainer.fit(model, mnist)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s a more realistic, complex DataModule that shows how much more reusable the datamodule is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MNISTDataModule at 0x7f20241be880>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "# Note - you must have torchvision installed for this example\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class MNISTDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir: str = \"./\"):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # download\n",
    "        MNIST(self.data_dir, train=True, download=True)\n",
    "        MNIST(self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == \"fit\":\n",
    "            mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)\n",
    "            self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n",
    "\n",
    "        # Assign test dataset for use in dataloader(s)\n",
    "        if stage == \"test\":\n",
    "            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n",
    "\n",
    "        if stage == \"predict\":\n",
    "            self.mnist_predict = MNIST(self.data_dir, train=False, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, batch_size=32)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val, batch_size=32)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.mnist_test, batch_size=32)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.mnist_predict, batch_size=32)\n",
    "\n",
    "dm = MNISTDataModule()\n",
    "dm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightningDataModule API\n",
    "\n",
    "See full details of the API online:\n",
    "\n",
    "https://pytorch-lightning.readthedocs.io/en/stable/data/datamodule.html#lightningdatamodule-api\n",
    "\n",
    "To define a DataModule the following methods are used to create train/val/test/predict dataloaders:\n",
    "\n",
    "* prepare_data (how to download, tokenize, etc…)\n",
    "* setup (how to split, define dataset, etc…)\n",
    "* train_dataloader\n",
    "* val_dataloader\n",
    "* test_dataloader\n",
    "* predict_dataloader\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a DataModule\n",
    "\n",
    "The recommended way to use a DataModule is simply:\n",
    "\n",
    "```python\n",
    "dm = MNISTDataModule()\n",
    "model = Model()\n",
    "trainer.fit(model, datamodule=dm)\n",
    "trainer.test(datamodule=dm)\n",
    "trainer.validate(datamodule=dm)\n",
    "trainer.predict(datamodule=dm)\n",
    "```\n",
    "\n",
    "⚠️ If you need information from the dataset to build your model, then run `prepare_data` and `setup` manually \n",
    "(Lightning ensures the method runs on the correct devices).\n",
    "\n",
    "```python\n",
    "dm = MNISTDataModule()\n",
    "dm.prepare_data()\n",
    "dm.setup(stage=\"fit\")\n",
    "\n",
    "model = Model(num_classes=dm.num_classes, width=dm.width, vocab=dm.vocab)\n",
    "trainer.fit(model, dm)\n",
    "\n",
    "dm.setup(stage=\"test\")\n",
    "trainer.test(datamodule=dm)\n",
    "```\n",
    "\n",
    "You can access:\n",
    "* the current used datamodule of a trainer via `trainer.datamodule`\n",
    "* and the current used dataloaders via `trainer.train_dataloader`, `trainer.val_dataloaders` and `trainer.test_dataloaders`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataModules without Lightning\n",
    "\n",
    "You can of course use `DataModule`s in plain PyTorch code as well.\n",
    "\n",
    "```python\n",
    "# download, etc...\n",
    "dm = MNISTDataModule()\n",
    "dm.prepare_data()\n",
    "\n",
    "# splits/transforms\n",
    "dm.setup(stage=\"fit\")\n",
    "\n",
    "# use data\n",
    "for batch in dm.train_dataloader():\n",
    "    ...\n",
    "\n",
    "for batch in dm.val_dataloader():\n",
    "    ...\n",
    "\n",
    "dm.teardown(stage=\"fit\")\n",
    "\n",
    "# lazy load test data\n",
    "dm.setup(stage=\"test\")\n",
    "for batch in dm.test_dataloader():\n",
    "    ...\n",
    "\n",
    "dm.teardown(stage=\"test\")\n",
    "```\n",
    "\n",
    "But overall, `DataModule`s encourage reproducibility by allowing all details of a dataset to be specified in a unified structure."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters in DataModules\n",
    "\n",
    "Like `LightningModule`s, `DataModule`s support hyperparameters with the same API.\n",
    "\n",
    "```python\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "class CustomDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # access the saved hyperparameters\n",
    "        opt = optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "```\n",
    "\n",
    "Refer to `save_hyperparameters` in lightning module for more details."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save DataModule state\n",
    "\n",
    "When a checkpoint is created, it asks every DataModule for their state.\n",
    "\n",
    "[❗] If your `DataModule` defines the `state_dict` and `load_state_dict` methods,\n",
    "the checkpoint will automatically track and restore your `DataModules`.\n",
    "\n",
    "```python\n",
    "class LitDataModule(pl.DataModule):\n",
    "    def state_dict(self):  # NOTE.\n",
    "        # track whatever you want here\n",
    "        state = {\"current_train_batch_index\": self.current_train_batch_index}\n",
    "        return state\n",
    "\n",
    "    def load_state_dict(self, state_dict):  # NOTE.\n",
    "        # restore the state based on what you tracked in (def state_dict)\n",
    "        self.current_train_batch_index = state_dict[\"current_train_batch_index\"]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn_py38_lightning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3a117739a79adb8d3a3cb28cb225d24b95a2a8e6607bba35806671d6c7dba5eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
