{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following [this](https://pytorch-lightning.readthedocs.io/en/stable/accelerators/accelerator_prepare.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardware agnostic training (preparation)\n",
    "To train on CPU/GPU/TPU without changing your code, we need to build a few good habits."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete `.cuda()` or `.to()` calls\n",
    "\n",
    "Delete any calls to `.cuda()` or `.to(device)`.\n",
    "\n",
    "```python\n",
    "# before lightning\n",
    "def forward(self, x):\n",
    "    x = x.cuda(0)\n",
    "    layer_1.cuda(0)\n",
    "    x_hat = layer_1(x)\n",
    "\n",
    "\n",
    "# after lightning\n",
    "def forward(self, x):\n",
    "    x_hat = layer_1(x)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init tensors using `Tensor.to` and `register_buffer`\n",
    "\n",
    "When you need to create a new tensor, use `Tensor.to`.\n",
    "This will make your code scale to any arbitrary number of GPUs or TPUs with Lightning.\n",
    "\n",
    "```python\n",
    "# before lightning\n",
    "def forward(self, x):\n",
    "    z = torch.Tensor(2, 3)\n",
    "    z = z.cuda(0)  # NOTE: No.\n",
    "\n",
    "\n",
    "# with lightning\n",
    "def forward(self, x):\n",
    "    z = torch.Tensor(2, 3)\n",
    "    z = z.to(x)  # NOTE: Yes! Match it to the input device.\n",
    "```\n",
    "\n",
    "The `LightningModule` knows what device it is on. You can access the reference via `self.device`.\n",
    "\n",
    "Sometimes it is necessary to *store tensors as module attributes*. \n",
    "\n",
    "However, if they are not parameters they will *remain on the CPU even if the module gets moved to a new device*.\n",
    "\n",
    "To prevent that and remain device agnostic, register the tensor as a buffer in your modules’ `__init__` method with `register_buffer()`.\n",
    "\n",
    "```python\n",
    "class LitModel(LightningModule):\n",
    "    def __init__(self):\n",
    "        ...\n",
    "        self.register_buffer(\"sigma\", torch.eye(3))\n",
    "        # you can now access self.sigma anywhere in your module\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove samplers\n",
    "\n",
    "`DistributedSampler` is automatically handled by Lightning.\n",
    "\n",
    "See [`replace_sampler_ddp`](https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#replace-sampler-ddp) for more information.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synchronize validation and test logging\n",
    "\n",
    "When running in distributed mode, we have to ensure that the validation and test step logging calls are synchronized across processes.\n",
    "\n",
    "This is done by adding `sync_dist=True` to all `self.log` calls in the validation and test step.\n",
    "\n",
    "This ensures that each GPU worker has the same behaviour when tracking model checkpoints, \n",
    "which is important for later downstream tasks such as testing the best checkpoint across all workers.\n",
    "\n",
    "The `sync_dist` option can also be used in logging calls during the step methods, *but be aware that this can lead to significant communication overhead and slow down your training*.\n",
    "\n",
    "Note if you use any built in metrics or custom metrics that use `TorchMetrics`, these do not need to be updated and are automatically handled for you.\n",
    "\n",
    "```python\n",
    "def validation_step(self, batch, batch_idx):\n",
    "    x, y = batch\n",
    "    logits = self(x)\n",
    "    loss = self.loss(logits, y)\n",
    "    # Add sync_dist=True to sync logging across all GPU workers (may have performance impact)\n",
    "    self.log(\n",
    "        \"validation_loss\",\n",
    "        loss,\n",
    "        on_step=True,\n",
    "        on_epoch=True,\n",
    "        sync_dist=True,  # NOTE.\n",
    "    )\n",
    "\n",
    "\n",
    "def test_step(self, batch, batch_idx):\n",
    "    x, y = batch\n",
    "    logits = self(x)\n",
    "    loss = self.loss(logits, y)\n",
    "    # Add sync_dist=True to sync logging across all GPU workers (may have performance impact)\n",
    "    self.log(\n",
    "        \"test_loss\",\n",
    "        loss,\n",
    "        on_step=True,\n",
    "        on_epoch=True,\n",
    "        sync_dist=True,  # NOTE.\n",
    "    )\n",
    "```\n",
    "\n",
    "**⚠️ Complex**\n",
    "\n",
    "It is possible to perform some computation manually and log the reduced result on rank 0 as follows:\n",
    "\n",
    "```python\n",
    "def test_step(self, batch, batch_idx):\n",
    "    x, y = batch\n",
    "    tensors = self(x)\n",
    "    return tensors\n",
    "\n",
    "\n",
    "def test_epoch_end(self, outputs):\n",
    "    mean = torch.mean(self.all_gather(outputs))\n",
    "\n",
    "    # When logging only on rank 0, don't forget to add\n",
    "    # ``rank_zero_only=True`` to avoid deadlocks on synchronization.\n",
    "    if self.trainer.is_global_zero:\n",
    "        self.log(\"my_reduced_metric\", mean, rank_zero_only=True)\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make models pickleable\n",
    "\n",
    "It’s very likely your code is already pickleable, in that case no change in necessary. \n",
    "\n",
    "However, if you run a distributed model and get the following error:\n",
    "\n",
    "```python\n",
    "self._launch(process_obj)\n",
    "File \"/net/software/local/python/3.6.5/lib/python3.6/multiprocessing/popen_spawn_posix.py\", line 47,\n",
    "in _launch reduction.dump(process_obj, fp)\n",
    "File \"/net/software/local/python/3.6.5/lib/python3.6/multiprocessing/reduction.py\", line 60, in dump\n",
    "ForkingPickler(file, protocol).dump(obj)\n",
    "_pickle.PicklingError: Can't pickle <function <lambda> at 0x2b599e088ae8>:\n",
    "attribute lookup <lambda> on __main__ failed\n",
    "```\n",
    "\n",
    "This means something in your model definition, transforms, optimizer, dataloader or callbacks cannot be pickled, and the following code will fail:\n",
    "\n",
    "```python\n",
    "import pickle\n",
    "\n",
    "pickle.dump(some_object)\n",
    "```\n",
    "\n",
    "This is a limitation of using multiple processes for distributed training within PyTorch. To fix this issue, find your piece of code that cannot be pickled. The end of the stacktrace is usually helpful. ie: in the stacktrace example here, there seems to be a lambda function somewhere in the code which cannot be pickled.\n",
    "\n",
    "```python\n",
    "self._launch(process_obj)\n",
    "File \"/net/software/local/python/3.6.5/lib/python3.6/multiprocessing/popen_spawn_posix.py\", line 47,\n",
    "in _launch reduction.dump(process_obj, fp)\n",
    "File \"/net/software/local/python/3.6.5/lib/python3.6/multiprocessing/reduction.py\", line 60, in dump\n",
    "ForkingPickler(file, protocol).dump(obj)\n",
    "_pickle.PicklingError: Can't pickle [THIS IS THE THING TO FIND AND DELETE]:\n",
    "attribute lookup <lambda> on __main__ failed\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
