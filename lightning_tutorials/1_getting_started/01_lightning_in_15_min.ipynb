{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following [this](https://pytorch-lightning.readthedocs.io/en/stable/starter/introduction.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a LightningModule\n",
    "\n",
    "import os\n",
    "from torch import optim, nn, utils, Tensor\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "import pytorch_lightning as pl  # NOTE.\n",
    "\n",
    "# define any number of nn.Modules (or use your current ones)\n",
    "encoder = nn.Sequential(nn.Linear(28 * 28, 64), nn.ReLU(), nn.Linear(64, 3))\n",
    "decoder = nn.Sequential(nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, 28 * 28))\n",
    "\n",
    "# define the LightningModule\n",
    "class LitAutoEncoder(pl.LightningModule):  # NOTE.\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def training_step(self, batch, batch_idx):  # NOTE.\n",
    "        # training_step defines the train loop.\n",
    "        # it is independent of forward\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        loss = nn.functional.mse_loss(x_hat, x)\n",
    "        # Logging to TensorBoard by default\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):  # NOTE.\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "# init the autoencoder\n",
    "autoencoder = LitAutoEncoder(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /mnt/data-linux/Dropbox/Programming/wsl_repos/practice_py/lightning_tutorials/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f44ac560a2b04a77931327fadfbea135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /mnt/data-linux/Dropbox/Programming/wsl_repos/practice_py/lightning_tutorials/MNIST/raw/train-images-idx3-ubyte.gz to /mnt/data-linux/Dropbox/Programming/wsl_repos/practice_py/lightning_tutorials/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /mnt/data-linux/Dropbox/Programming/wsl_repos/practice_py/lightning_tutorials/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25cf4c6a788842a2a6a6275de02d99d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /mnt/data-linux/Dropbox/Programming/wsl_repos/practice_py/lightning_tutorials/MNIST/raw/train-labels-idx1-ubyte.gz to /mnt/data-linux/Dropbox/Programming/wsl_repos/practice_py/lightning_tutorials/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /mnt/data-linux/Dropbox/Programming/wsl_repos/practice_py/lightning_tutorials/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3f16ee5b1324e5da103ad1502426fd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /mnt/data-linux/Dropbox/Programming/wsl_repos/practice_py/lightning_tutorials/MNIST/raw/t10k-images-idx3-ubyte.gz to /mnt/data-linux/Dropbox/Programming/wsl_repos/practice_py/lightning_tutorials/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /mnt/data-linux/Dropbox/Programming/wsl_repos/practice_py/lightning_tutorials/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ac98d391a9f400889ed89f07affc881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /mnt/data-linux/Dropbox/Programming/wsl_repos/practice_py/lightning_tutorials/MNIST/raw/t10k-labels-idx1-ubyte.gz to /mnt/data-linux/Dropbox/Programming/wsl_repos/practice_py/lightning_tutorials/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a dataset\n",
    "# Lightning supports ANY iterable (DataLoader, numpy, etc…) for the train/val/test/predict splits.\n",
    "\n",
    "dataset = MNIST(os.getcwd(), download=True, transform=ToTensor())\n",
    "train_loader = utils.data.DataLoader(dataset)  # pyright: ignore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "The Lightning `Trainer` “mixes” any `LightningModule` with any dataset and abstracts away all the engineering complexity needed for scale.\n",
    "\n",
    "The Lightning Trainer automates [40+ tricks](https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#trainer-flags) including:\n",
    "* Epoch and batch iteration\n",
    "* optimizer.step(), loss.backward(), optimizer.zero_grad() calls\n",
    "* Calling of model.eval(), enabling/disabling grads during evaluation\n",
    "* Checkpoint Saving and Loading\n",
    "* Tensorboard (see loggers options)\n",
    "* Multi-GPU support\n",
    "* TPU\n",
    "* 16-bit precision AMP support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/mnt/data-linux/miniconda3/envs/learn_py38_lightning/lib/python3.8/site-packages/pytorch_lightning/trainer/setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "Missing logger folder: /mnt/data-linux/Dropbox/Programming/wsl_repos/practice_py/lightning_tutorials/lightning_logs\n",
      "\n",
      "  | Name    | Type       | Params\n",
      "---------------------------------------\n",
      "0 | encoder | Sequential | 50.4 K\n",
      "1 | decoder | Sequential | 51.2 K\n",
      "---------------------------------------\n",
      "101 K     Trainable params\n",
      "0         Non-trainable params\n",
      "101 K     Total params\n",
      "0.407     Total estimated model params size (MB)\n",
      "/mnt/data-linux/miniconda3/envs/learn_py38_lightning/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6362069ea3cb425680095bb74ed59666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "# train the model (hint: here are some helpful Trainer arguments for rapid idea iteration)\n",
    "trainer = pl.Trainer(limit_train_batches=100, max_epochs=1)\n",
    "trainer.fit(model=autoencoder, train_dataloaders=train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡ \n",
      "Predictions (4 image embeddings):\n",
      " tensor([[-5.3405e+29,  8.6866e+29,  2.0069e+29],\n",
      "        [-4.5485e+34,  8.2463e+36,  2.9913e+36],\n",
      "        [        nan,         nan,         nan],\n",
      "        [        nan,         nan,         nan]], grad_fn=<AddmmBackward0>) \n",
      " ⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡\n"
     ]
    }
   ],
   "source": [
    "# Use the model\n",
    "\n",
    "# Once you’ve trained the model you can export to onnx, torchscript and put it into production or\n",
    "# simply load the weights and run predictions.\n",
    "\n",
    "# load checkpoint\n",
    "checkpoint = \"./lightning_logs/version_0/checkpoints/epoch=0-step=100.ckpt\"  # NOTE.\n",
    "autoencoder = LitAutoEncoder.load_from_checkpoint(checkpoint, encoder=encoder, decoder=decoder)\n",
    "\n",
    "# choose your trained nn.Module\n",
    "encoder = autoencoder.encoder\n",
    "encoder.eval()\n",
    "\n",
    "# embed 4 fake images!\n",
    "fake_image_batch = Tensor(4, 28 * 28)\n",
    "embeddings = encoder(fake_image_batch)\n",
    "print(\"⚡\" * 20, \"\\nPredictions (4 image embeddings):\\n\", embeddings, \"\\n\", \"⚡\" * 20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize training\n",
    "\n",
    "Lightning comes with a *lot* of batteries included. A helpful one is Tensorboard for visualizing experiments.\n",
    "\n",
    "Run this on your commandline and open your browser to http://localhost:6006/\n",
    "\n",
    "```sh\n",
    "tensorboard --logdir .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type       | Params\n",
      "---------------------------------------\n",
      "0 | encoder | Sequential | 50.4 K\n",
      "1 | decoder | Sequential | 51.2 K\n",
      "---------------------------------------\n",
      "101 K     Trainable params\n",
      "0         Non-trainable params\n",
      "101 K     Total params\n",
      "0.407     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f80aa46778a445c9c8a86c3a7b9a081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(overfit_batches=1)` was configured so 1 batch will be used.\n",
      "\n",
      "  | Name    | Type       | Params\n",
      "---------------------------------------\n",
      "0 | encoder | Sequential | 50.4 K\n",
      "1 | decoder | Sequential | 51.2 K\n",
      "---------------------------------------\n",
      "101 K     Trainable params\n",
      "0         Non-trainable params\n",
      "101 K     Total params\n",
      "0.407     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7221a0427228419883c6b0952e9e7986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "# Supercharge training\n",
    "\n",
    "# Enable advanced training features using Trainer arguments.\n",
    "# These are state-of-the-art techniques that are automatically integrated into your training loop\n",
    "# without changes to your code.\n",
    "\n",
    "# train on 4 GPUs\n",
    "trainer = pl.Trainer(\n",
    "    devices=1,\n",
    "    accelerator=\"gpu\",\n",
    "    # --\n",
    "    max_epochs=3, # For illustration speed.\n",
    "    limit_train_batches=100,  # For illustration speed.\n",
    ")\n",
    "trainer.fit(model=autoencoder, train_dataloaders=train_loader)\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# 20+ helpful flags for rapid idea iteration\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    min_epochs=5,\n",
    "    overfit_batches=1,\n",
    "    # --\n",
    "    limit_train_batches=100,  # For illustration speed.\n",
    "    log_every_n_steps=1,  # For illustration speed.\n",
    ")\n",
    "\n",
    "trainer.fit(model=autoencoder, train_dataloaders=train_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More examples:\n",
    "\n",
    "```python\n",
    "# train on 4 GPUs\n",
    "trainer = Trainer(\n",
    "    devices=4,\n",
    "    accelerator=\"gpu\",\n",
    " )\n",
    "\n",
    "# train 1TB+ parameter models with Deepspeed/fsdp\n",
    "trainer = Trainer(\n",
    "    devices=4,\n",
    "    accelerator=\"gpu\",\n",
    "    strategy=\"deepspeed_stage_2\",\n",
    "    precision=16\n",
    " )\n",
    "\n",
    "# 20+ helpful flags for rapid idea iteration\n",
    "trainer = Trainer(\n",
    "    max_epochs=10,\n",
    "    min_epochs=5,\n",
    "    overfit_batches=1\n",
    " )\n",
    "\n",
    "# access the latest state of the art techniques\n",
    "trainer = Trainer(callbacks=[StochasticWeightAveraging(...)])\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximize flexibility\n",
    "\n",
    "Lightning’s core guiding principle is to always provide maximal flexibility without ever hiding any of the PyTorch.\n",
    "\n",
    "Lightning offers 5 added degrees of flexibility depending on your project’s complexity.\n",
    "\n",
    "#### Customize training loop\n",
    "Inject custom code anywhere in the Training loop using any of the 20+ methods (Hooks) available in the LightningModule.\n",
    "\n",
    "<img src=\"./assets/custom_loop.png\" alt=\"drawing\" style=\"width:700px;\"/>\n",
    "\n",
    "```python\n",
    "class LitAutoEncoder(pl.LightningModule):\n",
    "    def backward(self, loss, optimizer, optimizer_idx):\n",
    "        loss.backward()\n",
    "```\n",
    "* Hooks: https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html#lightning-hooks\n",
    "\n",
    "#### https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html#lightning-hooks\n",
    "If you have multiple lines of code with similar functionalities, you can use callbacks to easily group them together and toggle all of those lines on or off at the same time.\n",
    "![img]{ width: 200px; }(./assets/ext_loop.png)\n",
    "```python\n",
    "trainer = Trainer(callbacks=[AWSCheckpoints()])\n",
    "```\n",
    "\n",
    "#### Use a raw PyTorch loop\n",
    "For certain types of work at the bleeding-edge of research, Lightning offers experts full control of their training loops in various ways.\n",
    "\n",
    "ℹ️ See tutorials:\n",
    "* Manual optimization: https://pytorch-lightning.readthedocs.io/en/stable/model/build_model_advanced.html#manual-optimization\n",
    "* Lightning lite: https://pytorch-lightning.readthedocs.io/en/stable/model/build_model_expert.html\n",
    "* Loops: https://pytorch-lightning.readthedocs.io/en/stable/extensions/loops.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn_py38_lightning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3a117739a79adb8d3a3cb28cb225d24b95a2a8e6607bba35806671d6c7dba5eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
