{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c10c194-c206-4c2f-be3f-a9080e401c4f",
   "metadata": {},
   "source": [
    "# Imputation of missing values\n",
    "\n",
    "From this guide: https://scikit-learn.org/stable/modules/impute.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19de294-f3d1-4f11-9fa4-df518d304ab9",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "For various reasons, many real world datasets contain missing values, often encoded as blanks, NaNs or other placeholders. Such datasets however are incompatible with scikit-learn estimators which assume that all values in an array are numerical, and that all have and hold *meaning*. \n",
    "\n",
    "A basic strategy to use incomplete datasets is *to discard entire rows and/or columns* containing missing values. However, this comes at the price of losing data which may be valuable (even though incomplete). \n",
    "\n",
    "A better strategy is to **impute** the missing values, i.e., to infer them from the known part of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1a54d3-b222-4d75-859e-92eb67f9e28b",
   "metadata": {},
   "source": [
    "## [Univariate vs. Multivariate Imputation](https://scikit-learn.org/stable/modules/impute.html#univariate-vs-multivariate-imputation)\n",
    "\n",
    "One type of imputation algorithm is **univariate**: \n",
    "* imputes values in the i-th feature dimension using only non-missing values in that feature dimension (e.g. `impute.SimpleImputer`). \n",
    "\n",
    "By contrast, **multivariate** imputation algorithms:\n",
    "* use the *entire set of available feature dimensions* to estimate the missing values (e.g. `impute.IterativeImputer`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99611a8-87e7-42de-a070-4b86c1f52a7d",
   "metadata": {},
   "source": [
    "## [Univariate feature imputation](https://scikit-learn.org/stable/modules/impute.html#univariate-feature-imputation)\n",
    "\n",
    "The `SimpleImputer` class provides basic strategies for imputing missing values. \n",
    "\n",
    "Missing values can be imputed with: \n",
    "* a provided constant value, or \n",
    "* using the statistics (`mean`, `median` or `most frequent`) \n",
    "\n",
    "of each column in which the missing values are located. \n",
    "\n",
    "This class also allows for *different missing values encodings*.\n",
    "\n",
    "The following snippet demonstrates how to replace missing values, encoded as `np.nan`, using the *mean value* of the columns (axis 0) that contain the missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01fd5104-1d84-4972-a930-bc6c502d5fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[nan  2.]\n",
      " [ 6. nan]\n",
      " [ 7.  6.]]\n",
      "[[4.         2.        ]\n",
      " [6.         3.66666667]\n",
      " [7.         6.        ]]\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp = SimpleImputer(\n",
    "    missing_values=np.nan, \n",
    "    strategy='mean'\n",
    ")\n",
    "\n",
    "imp.fit([[1, 2], [np.nan, 3], [7, 6]])\n",
    "\n",
    "X = np.array(\n",
    "    [\n",
    "        [np.nan, 2], \n",
    "        [6, np.nan], \n",
    "        [7, 6]\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(X)\n",
    "print(imp.transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c505ba77-eacc-4a83-99a5-541f0a6c2956",
   "metadata": {},
   "source": [
    "The `SimpleImputer` class also supports **sparse matrices**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b79cf8a-af98-486c-8cd9-e304d6df43b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      "   (0, 0)\t1\n",
      "  (2, 0)\t8\n",
      "  (0, 1)\t2\n",
      "  (1, 1)\t-1\n",
      "  (2, 1)\t4\n",
      "X_test before:\n",
      "   (0, 0)\t-1\n",
      "  (1, 0)\t6\n",
      "  (2, 0)\t7\n",
      "  (0, 1)\t2\n",
      "  (1, 1)\t-1\n",
      "  (2, 1)\t6\n",
      "X_test after:\n",
      "   (0, 0)\t3.0\n",
      "  (1, 0)\t6.0\n",
      "  (2, 0)\t7.0\n",
      "  (0, 1)\t2.0\n",
      "  (1, 1)\t3.0\n",
      "  (2, 1)\t6.0\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "# CSC sparse matrix:\n",
    "X = sp.csc_matrix(\n",
    "    [\n",
    "        [1, 2], \n",
    "        [0, -1], \n",
    "        [8, 4]\n",
    "    ]\n",
    ")\n",
    "print(\"X:\\n\", X)\n",
    "\n",
    "imp = SimpleImputer(\n",
    "    missing_values=-1, \n",
    "    strategy='mean'\n",
    ")\n",
    "\n",
    "imp.fit(X)\n",
    "\n",
    "X_test = sp.csc_matrix(\n",
    "    [\n",
    "        [-1, 2], \n",
    "        [6, -1], \n",
    "        [7, 6]\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"X_test before:\\n\", X_test)\n",
    "X_test_after = imp.transform(X_test)\n",
    "print(\"X_test after:\\n\", X_test_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6095f073-610c-45f6-b7df-0f640e5a255e",
   "metadata": {},
   "source": [
    "Note that this format is not meant to be used to implicitly store missing values in the matrix because it would densify it at transform time. Missing values encoded by 0 must be used with dense input.\n",
    "\n",
    "The `SimpleImputer` class also supports categorical data represented as string values or [pandas categoricals](https://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html) when using the `'most_frequent'` or `'constant'` strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4337274-66a5-403a-8bb2-ddc4ca024edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1\n",
       "0    a    x\n",
       "1  NaN    y\n",
       "2    a  NaN\n",
       "3    b    y"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(\n",
    "    [\n",
    "        [\"a\", \"x\"],\n",
    "        [np.nan, \"y\"],\n",
    "        [\"a\", np.nan],\n",
    "        [\"b\", \"y\"]\n",
    "    ], \n",
    "    dtype=\"category\"  # <-- NOTE.\n",
    ")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bfd0b22-1a36-4d9a-b912-cbd240782039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['a', 'x'],\n",
       "       ['a', 'y'],\n",
       "       ['a', 'y'],\n",
       "       ['b', 'y']], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "imp.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89735962-6dd3-4b61-b673-dc8db981e3e6",
   "metadata": {},
   "source": [
    "## [Multivariate feature imputation](https://scikit-learn.org/stable/modules/impute.html#multivariate-feature-imputation)\n",
    "\n",
    "A more sophisticated approach is to use the `IterativeImputer` class, which models each feature with missing values *as a function of other features*, and uses that estimate for imputation. \n",
    "\n",
    "It does so in an *iterated round-robin fashion*: \n",
    "* at each step, a feature column is designated as output `y` and the other feature columns are treated as inputs `X`. \n",
    "* A *regressor is fit* on `(X, y)` for known `y`. \n",
    "* Then, the regressor is used to predict the missing values of y. \n",
    "\n",
    "This is done for each feature in an iterative fashion, and then is repeated for `max_iter` imputation rounds. *The results of the final imputation round are returned.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c542c203-7766-4c42-a649-eb9594f2ac56",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**⏱️⚙️ Note (Experimental as of version `0.24.2`)**\n",
    "\n",
    "This estimator is still **experimental** for now: default parameters or details of behaviour might change without any deprecation cycle. Resolving the following issues would help stabilize `IterativeImputer`: convergence criteria ([#14338](https://github.com/scikit-learn/scikit-learn/issues/14338)), default estimators ([#13286](https://github.com/scikit-learn/scikit-learn/issues/13286)), and use of random state ([#15611](https://github.com/scikit-learn/scikit-learn/issues/15611)). To use it, you need to explicitly import `enable_iterative_imputer`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4c5675c-2b82-4901-b26e-8293e85ab9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer  # NOTE!\n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57bc6777-8952-4c41-9dc3-113dfa952f28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IterativeImputer(random_state=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp = IterativeImputer(\n",
    "    max_iter=10, \n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "imp.fit(\n",
    "    [\n",
    "        [1, 2], \n",
    "        [3, 6], \n",
    "        [4, 8], \n",
    "        [np.nan, 3], \n",
    "        [7, np.nan]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9e2a76b-35f7-4be2-bb1f-d1ed4363f907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  2.]\n",
      " [ 6. 12.]\n",
      " [ 3.  6.]]\n"
     ]
    }
   ],
   "source": [
    "X_test = [[np.nan, 2], [6, np.nan], [np.nan, 6]]\n",
    "# the model learns that the second feature is double the first\n",
    "print(np.round(imp.transform(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97abf4d-3f1b-498f-a282-f10405a83562",
   "metadata": {},
   "source": [
    "Both `SimpleImputer` and `IterativeImputer` can be used in a `Pipeline` as a way to build a *composite estimator* that supports imputation. \n",
    "\n",
    "**See example:** 📚 [Imputing missing values before building an estimator](https://scikit-learn.org/stable/auto_examples/impute/plot_iterative_imputer_variants_comparison.html#sphx-glr-auto-examples-impute-plot-iterative-imputer-variants-comparison-py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4133ad1-41aa-421f-b529-65a8faa82364",
   "metadata": {},
   "source": [
    "### Flexibility of IterativeImputer\n",
    "\n",
    "There are many well-established imputation packages in the `R` data science ecosystem: `Amelia`, `mi`, `mice`, `missForest`, etc. \n",
    "\n",
    "`missForest` is popular, and turns out to be *a particular instance of different sequential imputation algorithms* that can all be implemented with `IterativeImputer` by passing in different regressors to be used for predicting missing feature values. In the case of `missForest`, this regressor is a `Random Forest`. \n",
    "\n",
    "**See example:** 📚🎓 [Imputing missing values with variants of IterativeImputer](https://scikit-learn.org/stable/auto_examples/impute/plot_iterative_imputer_variants_comparison.html#sphx-glr-auto-examples-impute-plot-iterative-imputer-variants-comparison-py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385e1606-b005-402e-a9bf-a3a0b04758e1",
   "metadata": {},
   "source": [
    "### Multiple vs. Single Imputation\n",
    "\n",
    "ℹ️ In the **statistics community**, it is common practice to *perform multiple imputations*, generating, for example, `m` separate imputations for a single feature matrix. \n",
    "\n",
    "Each of these `m` imputations is then *put through the subsequent analysis pipeline* (e.g. feature engineering, clustering, regression, classification). \n",
    "\n",
    "The `m` final analysis results (e.g. held-out validation errors) allow the data scientist to obtain understanding of how analytic results may differ as a consequence of the inherent uncertainty caused by the missing values. \n",
    "\n",
    "**The above practice is called multiple imputation.**\n",
    "\n",
    "💡🎓 Our implementation of `IterativeImputer` was inspired by the `R MICE package` (*Multivariate Imputation by Chained Equations*) `Stef van Buuren, Karin Groothuis-Oudshoorn (2011). \"mice: Multivariate Imputation by Chained Equations in R\". Journal of Statistical Software 45: 1-67.`, **but differs from it by returning a single imputation instead of multiple imputations**. However, `IterativeImputer` can also be used for *multiple imputations* by applying it repeatedly to the same dataset with different random seeds when `sample_posterior=True`. See `Roderick J A Little and Donald B Rubin (1986). “Statistical Analysis with Missing Data”. John Wiley & Sons, Inc., New York, NY, USA.`, chapter 4 for more discussion on multiple vs. single imputations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865eb8c2-5331-4ec8-9338-718885912e69",
   "metadata": {},
   "source": [
    "## [Nearest neighbors imputation](https://scikit-learn.org/stable/modules/impute.html#nearest-neighbors-imputation)\n",
    "\n",
    "The `KNNImputer` class provides imputation for filling in missing values using the k-Nearest Neighbors approach. \n",
    "* By default, a euclidean distance metric that supports missing values, `nan_euclidean_distances`, is used to find the nearest neighbors.\n",
    "* Each missing feature is imputed using values from `n_neighbors` nearest neighbors that have a value for the feature.\n",
    "* The feature of the neighbors are averaged uniformly or weighted by distance to each neighbor.\n",
    "* If a sample has more than one feature missing, then the neighbors for that sample can be different depending on the particular feature being imputed.\n",
    "* When the number of available neighbors is less than `n_neighbors` and there are no defined distances to the training set, the training set average for that feature is used during imputation.\n",
    "* If there is at least one neighbor with a defined distance, the weighted or unweighted average of the remaining neighbors will be used during imputation.\n",
    "* If a feature is always missing in training, it is removed during transform (❗).\n",
    "\n",
    "The following snippet demonstrates how to replace missing values, encoded as `np.nan`, using the mean feature value of the two nearest neighbors of samples with missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1939c363-5bca-4998-a3b8-136b072fd17e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1. , 2. , 4. ],\n",
       "       [3. , 4. , 3. ],\n",
       "       [5.5, 6. , 5. ],\n",
       "       [8. , 8. , 7. ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "nan = np.nan\n",
    "X = [\n",
    "    [1,   2,   nan], \n",
    "    [3,   4,   3], \n",
    "    [nan, 6,   5], \n",
    "    [8,   8,   7]\n",
    "]\n",
    "\n",
    "imputer = KNNImputer(\n",
    "    n_neighbors=2, \n",
    "    weights=\"uniform\"\n",
    ")\n",
    "\n",
    "imputer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ca6099-8e7d-4af0-9539-c6307a70cdbe",
   "metadata": {},
   "source": [
    "## [Marking imputed values](https://scikit-learn.org/stable/modules/impute.html#marking-imputed-values)\n",
    "\n",
    "The `MissingIndicator` transformer is useful to transform a dataset into corresponding **binary matrix indicating the presence of missing values** in the dataset. \n",
    "\n",
    "This transformation is useful in conjunction with imputation. When using imputation, preserving the information about which values had been missing can be informative. \n",
    "\n",
    "> Note that both the `SimpleImputer` and `IterativeImputer` have the boolean parameter `add_indicator` (`False` by default) which when set to `True` provides a convenient way of *stacking the output of the `MissingIndicator` transformer with the output of the imputer*.\n",
    "\n",
    "`NaN` is usually used as the placeholder for missing values. **However, it enforces the data type to be `float`**. The parameter `missing_values` allows to specify other placeholder such as integer. In the following example, we will use `-1` as missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4413c923-1bb7-48cb-9f5c-cd502a766f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True, False],\n",
       "       [False,  True,  True],\n",
       "       [False,  True, False]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import MissingIndicator\n",
    "X = np.array(\n",
    "    [\n",
    "        [-1, -1, 1, 3],\n",
    "        [4, -1, 0, -1],\n",
    "        [8, -1, 1, 0]\n",
    "    ]\n",
    ")\n",
    "\n",
    "indicator = MissingIndicator(missing_values=-1)  # <-- See this: integer `-1` interpreted as missing indicator.\n",
    "\n",
    "mask_missing_values_only = indicator.fit_transform(X)\n",
    "mask_missing_values_only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1c4911-3a66-4397-a151-d7d3303b0f20",
   "metadata": {},
   "source": [
    "The `features` parameter is used to choose the features for which the mask is constructed. By default, it is `'missing-only'` which returns the imputer mask of the *features containing missing values at fit time*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "902f9927-5bef-4a3d-b5ff-222c2a1f8a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indicator.features_\n",
    "# Note feature of index 2 is missing here, it didn't have any missing indicators at fit time (here, -1s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb4afa5-2204-451f-8257-eedfca291f58",
   "metadata": {},
   "source": [
    "The `features` parameter can be set to `'all'` to return all features whether or not they contain missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24fd71a9-2468-483f-a7c9-7ddc92023e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True, False, False],\n",
       "       [False,  True, False,  True],\n",
       "       [False,  True, False, False]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indicator = MissingIndicator(\n",
    "    missing_values=-1, \n",
    "    features=\"all\"  # <-- This.\n",
    ")\n",
    "\n",
    "mask_all = indicator.fit_transform(X)\n",
    "mask_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "088c4108-c7b1-4974-ad57-7bc5fb50d81b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indicator.features_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f5f4f9-4876-4d49-a0b0-be1b8ef2c592",
   "metadata": {},
   "source": [
    "⚠️ When using the `MissingIndicator` in a `Pipeline`, be sure to use the \n",
    "* `FeatureUnion` or \n",
    "* `ColumnTransformer` \n",
    "\n",
    "to add the indicator features to the regular features.\n",
    "\n",
    "First we obtain the `iris` dataset, and add some missing values to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64260845-647e-45e3-9e4e-68eb181d326b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.impute import SimpleImputer, MissingIndicator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import FeatureUnion, make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13104fb2-1859-4d06-9060-f835cc4e1edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05dd0f27-f07e-48db-9692-b53146d76f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some values to NaN.\n",
    "mask = np.random.randint(0, 2, size=X.shape).astype(bool)\n",
    "X[mask] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec7396bc-9b5a-4a88-ac0a-c90b7517922a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, _ = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=100,\n",
    "    random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22269297-5122-4ed8-9e6d-ffce14c3c3de",
   "metadata": {},
   "source": [
    "Now we create a `FeatureUnion`. \n",
    "\n",
    "All features will be imputed using `SimpleImputer`, in order to enable classifiers to work with this data. \n",
    "\n",
    "Additionally, it adds the indicator variables from `MissingIndicator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bce79fa0-f653-4635-be2a-8b32ef82aab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureUnion(transformer_list=[('features', SimpleImputer()),\n",
       "                               ('indicators', MissingIndicator())])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(100, 8)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer = FeatureUnion(\n",
    "    transformer_list=[\n",
    "        ('features', SimpleImputer(strategy='mean')),\n",
    "        ('indicators', MissingIndicator())\n",
    "    ]\n",
    ")\n",
    "display(transformer)\n",
    "\n",
    "transformer = transformer.fit(X_train, y_train)\n",
    "results = transformer.transform(X_test)\n",
    "results.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dd22df-4758-4184-add1-ef0e8600504e",
   "metadata": {},
   "source": [
    "Of course, we cannot use the transformer to make any predictions. We should wrap this in a `Pipeline` with a classifier (e.g., a `DecisionTreeClassifier`) to be able to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c8408c9c-815a-47a4-be9c-a98ded97bd64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = make_pipeline(transformer, DecisionTreeClassifier())  # Using convenience function `make_pipeline()`\n",
    "clf = clf.fit(X_train, y_train)\n",
    "results = clf.predict(X_test)\n",
    "results.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
