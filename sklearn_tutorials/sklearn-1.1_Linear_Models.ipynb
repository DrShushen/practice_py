{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ba49710-6569-4ae5-857b-c2411cf118c9",
   "metadata": {},
   "source": [
    "# Linear Models\n",
    "\n",
    "From this guide: https://scikit-learn.org/stable/modules/linear_model.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a831a6-8cbe-4114-a826-c23ca5540e92",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "* Refer to vector $\\mathbf{w}$ as `coef_` and $w_0$ (aka $b$) as `intercept_`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7fb01d-a29c-4b1b-aec6-5cfb2bcf3e1d",
   "metadata": {},
   "source": [
    "### [Ordinary Least Squares](https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaae7b92-1b92-42d3-b36f-ceda9fb3d42a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4897b3f4-43d0-4d66-a99e-0013e7a803e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.5])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4af99ae5-8e4a-455b-bd5a-678768686adf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.220446049250313e-16"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f672eca6-e053-4bf0-a89e-f2082819e0e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.5])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.predict([[1.5, 1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec79bc2-04ae-41c2-854b-6d0f7fa46aad",
   "metadata": {},
   "source": [
    "#### ⚠️ Warning: **multicollinearity**\n",
    "\n",
    "The coefficient estimates for Ordinary Least Squares rely on the independence of the features. When features are correlated and the columns of the design matrix\n",
    "have an **approximate linear dependence**, the design matrix becomes close to **singula**r and as a result, the least-squares estimate becomes highly sensitive to random errors in the observed target, producing a large variance. This situation of **multicollinearity** can arise, for example, when data are collected without an experimental design."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba3a6be-74be-49de-8610-a191d3e7c1af",
   "metadata": {},
   "source": [
    "### [Ridge regression and classification](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression-and-classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3429b826-b5d6-4f76-afca-1852a4d59abe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=0.5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.Ridge(alpha=.5)\n",
    "reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cf8effa-448e-4f4a-bd61-acd89f180be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.34545455, 0.34545455])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7e2e39c-faa3-4e52-a3bc-8b0fe6e3f020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1363636363636364"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "666c9c6f-0c2c-4b52-a776-bde258420e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.17272727])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.predict([[1.5, 1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac65bc58-2b9d-410a-a441-1bed6cf3391d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c26b882-96a8-42bb-aa1e-5de12e36eadb",
   "metadata": {},
   "source": [
    "### A look at `RidgeClassifier`\n",
    "\n",
    "> The Ridge regressor has a classifier variant: `RidgeClassifier`. This classifier first converts binary targets to `{-1, 1}` and then treats the problem as a regression task, optimizing the same objective as above. The predicted class corresponds to the sign of the regressor’s prediction. For multiclass classification, the problem is treated as multi-output regression, and the predicted class corresponds to the output with the highest value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0777d80-e0f0-45ae-ad87-7017df8168ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "928b3526-54ec-4118-a330-adcaa71f5e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426, 30)\n",
      "[1.317e+01 1.822e+01 8.428e+01 5.373e+02 7.466e-02 5.994e-02 4.859e-02\n",
      " 2.870e-02 1.454e-01 5.549e-02 2.023e-01 6.850e-01 1.236e+00 1.689e+01\n",
      " 5.969e-03 1.493e-02 1.564e-02 8.463e-03 1.093e-02 1.672e-03 1.490e+01\n",
      " 2.389e+01 9.510e+01 6.876e+02 1.282e-01 1.965e-01 1.876e-01 1.045e-01\n",
      " 2.235e-01 6.925e-02]\n",
      "(143,)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=101)\n",
    "print(X_train.shape)\n",
    "print(X_train[0])\n",
    "print(y_test.shape)\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e18f74a-95ac-448c-828a-592e8e8091c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RidgeClassifier().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbbbce2e-f423-424a-8378-9dd0f4607240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 1.0,\n",
       " 'class_weight': None,\n",
       " 'copy_X': True,\n",
       " 'fit_intercept': True,\n",
       " 'max_iter': None,\n",
       " 'normalize': False,\n",
       " 'random_state': None,\n",
       " 'solver': 'auto',\n",
       " 'tol': 0.001}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8eb82e32-8349-4ba7-9cf4-2ba11fd58965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 30)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 3.17715622e-01,  3.91183399e-03, -5.98678406e-03,\n",
       "        -1.94898947e-03, -2.35923818e-01,  8.31182162e-02,\n",
       "        -3.36432631e-01, -4.89128320e-01, -4.01751257e-01,\n",
       "        -2.47037117e-03, -5.80264936e-01, -2.67387091e-02,\n",
       "         1.83323341e-02, -3.33142059e-04, -6.91611380e-02,\n",
       "         2.09853873e-01,  5.38900668e-01, -3.25060544e-02,\n",
       "        -5.33803282e-02,  2.00747251e-02, -5.75684261e-01,\n",
       "        -2.46569761e-02,  1.53541482e-02,  2.79396044e-03,\n",
       "        -6.04025660e-01, -2.35153428e-01, -6.30611100e-01,\n",
       "        -8.45232302e-01, -7.16762302e-01, -1.39276037e-01]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.37355073]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4.37355073])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(clf.coef_.shape)\n",
    "display(clf.coef_)\n",
    "print(clf.intercept_)\n",
    "display(clf.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4c1c4cc-ec4f-4ef4-87bd-a3bd5a3daabf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff2e18b-819d-4494-a8cc-56daff8908c1",
   "metadata": {},
   "source": [
    "* `predict()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46a850be-9b1e-4dc9-ab35-94a9bfa62484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X_test[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1abf21-f871-4998-a390-908fde1327b1",
   "metadata": {},
   "source": [
    "* `score()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06242542-085f-442e-9d18-0f162146637b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test[0:5], y_test[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3214eac-c9d1-49cf-bf25-da116af87b0b",
   "metadata": {},
   "source": [
    "* `decision_function()`:\n",
    "\n",
    "    In this case:\n",
    "    > Predict confidence scores for samples. The confidence score for a sample is proportional to the signed distance of that sample to the hyperplane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a1f1cba-92cc-4983-907b-58f80e3828cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.78994738,  0.5125506 ,  0.85589808, -0.45582771,  1.19121259])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.decision_function(X_test[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b91fc0-c9f4-4f98-9515-bfb876b2ffde",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12747a3-87f3-4192-bf1b-153fbd3bd69c",
   "metadata": {},
   "source": [
    "### [Setting the regularization parameter: leave-one-out Cross-Validation](https://scikit-learn.org/stable/modules/linear_model.html#setting-the-regularization-parameter-leave-one-out-cross-validation)\n",
    "\n",
    "`RidgeCV`: The object works in the same way as `GridSearchCV` except that it defaults to **Leave-One-Out** Cross-Validation. Specifying the value of the `cv` attribute will trigger the use of cross-validation with `GridSearchCV`, for example `cv=10` for 10-fold cross-validation, rather than Leave-One-Out Cross-Validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c64521ef-0935-4b92-a5f9-4d2b034b24b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "reg = linear_model.RidgeCV(alphas=np.logspace(-6, 6, 13))\n",
    "reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])\n",
    "reg.alpha_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bf550d-ef5b-4915-bd7a-838220df48f3",
   "metadata": {},
   "source": [
    "### [Lasso](https://scikit-learn.org/stable/modules/linear_model.html#lasso)\n",
    "\n",
    "* For an interesting example, see: [Compressive sensing: tomography reconstruction with L1 prior (Lasso)](https://scikit-learn.org/stable/auto_examples/applications/plot_tomography_l1_reconstruction.html#sphx-glr-auto-examples-applications-plot-tomography-l1-reconstruction-py)\n",
    "* The `alpha` parameter controls the degree of sparsity of the estimated coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "edc5d202-a24b-4ff2-98dc-e3a8d2dc02be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.Lasso(alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0fa91db2-e01f-4925-969a-92c8178b99b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [[0, 0], [1, 1]]\n",
      "y: [0, 1]\n"
     ]
    }
   ],
   "source": [
    "X = [[0, 0], [1, 1]]\n",
    "y = [0, 1]\n",
    "print(\"X:\", X)\n",
    "print(\"y:\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d29767b-3f00-4fc2-9c71-102b1b72b576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a82d191b-7a05-43df-b108-0c9ac98f9657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.predict([[1, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57e7589-f762-45ab-986e-8971437ab207",
   "metadata": {},
   "source": [
    "### [Lasso: Hyperparameter Search Using cross-validation](https://scikit-learn.org/stable/modules/linear_model.html#using-cross-validation)\n",
    "\n",
    "scikit-learn exposes objects that set the Lasso `alpha` parameter by **cross-validation**: \n",
    "* `LassoCV` and \n",
    "* `LassoLarsCV`. \n",
    "\n",
    "`LassoLarsCV` is based on the Least Angle Regression algorithm explained below.\n",
    "\n",
    "For high-dimensional datasets with many collinear features, `LassoCV` is most often preferable. However, `LassoLarsCV` has the advantage of exploring more relevant values of `alpha` parameter, and if the number of samples is very small compared to the number of features, it is often faster than LassoCV.\n",
    "\n",
    "Alternatively, the estimator \n",
    "* `LassoLarsIC` \n",
    "\n",
    "proposes to use the Akaike information criterion (AIC) and the Bayes Information criterion (BIC).\n",
    "\n",
    "See this example for more details: [Lasso model selection: Cross-Validation / AIC / BIC](https://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_model_selection.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e388a5e4-fd6d-4e78-b948-84f546010c60",
   "metadata": {},
   "source": [
    "### [Multi-task Lasso](https://scikit-learn.org/stable/modules/linear_model.html#multi-task-lasso)\n",
    "\n",
    "The `MultiTaskLasso` is a linear model that estimates sparse coefficients for multiple regression problems jointly: `y` is a **2D array**, of shape `(n_samples, n_tasks)`. The constraint is that the selected features are the same for all the regression problems, also called tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e06e29-a868-4f0f-bb77-e9def281b5ce",
   "metadata": {},
   "source": [
    "### [Elastic-Net](https://scikit-learn.org/stable/modules/linear_model.html#elastic-net)\n",
    "\n",
    "`ElasticNet` is a linear regression model trained with both $l_1$ and $l_2$-norm regularization of the coefficients.\n",
    "\n",
    "This combination allows for learning a sparse model where few of the weights are non-zero like `Lasso`, while still maintaining the regularization properties of Ridge. We control the convex combination of $l_1$ and $l_2$ using the `l1_ratio` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba59aaa-3959-4189-afd4-b845fb1fe4c8",
   "metadata": {},
   "source": [
    "### [Mutli-task Elastic-Net](https://scikit-learn.org/stable/modules/linear_model.html#multi-task-elastic-net)\n",
    "\n",
    "The `MultiTaskElasticNet` is an elastic-net model that estimates sparse coefficients for multiple regression problems jointly: `Y` is a **2D array** of shape `(n_samples, n_tasks)`. The constraint is that the selected features are the same for all the regression problems, also called tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c68db8f-b02c-43f6-89ce-236a6b5c2ea3",
   "metadata": {},
   "source": [
    "### [Least Angle Regression (LARS)](https://scikit-learn.org/stable/modules/linear_model.html#least-angle-regression)\n",
    "\n",
    "Least-angle regression (LARS) is a regression algorithm for high-dimensional data. LARS is similar to forward stepwise regression. At each step, it finds the feature most correlated with the target. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3639d0ef-ef62-47ae-bf64-f37dfbdd68b3",
   "metadata": {},
   "source": [
    "### [LARS Lasso](https://scikit-learn.org/stable/modules/linear_model.html#lars-lasso)\n",
    "\n",
    "`LassoLars` is a lasso model implemented using the LARS algorithm, and unlike the implementation based on coordinate descent, this yields the exact solution, which is piecewise linear as a function of the norm of its coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0867521e-b8ee-4b7b-9eef-4c65b5b5b19e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.71715729, 0.        ])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.LassoLars(alpha=.1)\n",
    "reg.fit([[0, 0], [1, 1]], [0, 1])\n",
    "reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025f3c95-90a2-4f62-85c6-de6d604e7a62",
   "metadata": {},
   "source": [
    "### [Orthogonal Matching Pursuit (OMP)](https://scikit-learn.org/stable/modules/linear_model.html#orthogonal-matching-pursuit-omp)\n",
    "\n",
    "`OrthogonalMatchingPursuit` and `orthogonal_mp` implements the OMP algorithm for approximating the fit of a linear model with constraints imposed on the number of non-zero coefficients $l_0$ (i.e. the pseudo-norm)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e41878-6c11-4490-95ab-76c5afc75e88",
   "metadata": {},
   "source": [
    "### [Bayesian Regression](https://scikit-learn.org/stable/modules/linear_model.html#bayesian-regression)\n",
    "\n",
    "> Bayesian regression techniques can be used to include regularization parameters in the estimation procedure: the regularization parameter is not set in a hard sense but tuned to the data at hand.\n",
    "\n",
    "> 📚 This is well explained in: *C. Bishop: Pattern Recognition and Machine learning*\n",
    "\n",
    "* [Bayesian Ridge Regression](https://scikit-learn.org/stable/modules/linear_model.html#bayesian-ridge-regression): `BayesianRidge`\n",
    "* [Automatic Relevance Determination - ARD](https://scikit-learn.org/stable/modules/linear_model.html#automatic-relevance-determination-ard): `ARDRegression`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc0aa2b9-dd34-4bf1-a333-8a242b259f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [[0.0, 0.0], [1.0, 1.0], [2.0, 2.0], [3.0, 3.0]]\n",
      "y: [0.0, 1.0, 2.0, 3.0]\n",
      "w: [0.49999993 0.49999993]\n",
      "predicion for [1., 0.]:\n",
      "[0.50000013]\n"
     ]
    }
   ],
   "source": [
    "# BayesianRidge example:\n",
    "from sklearn import linear_model\n",
    "\n",
    "X = [[0., 0.], [1., 1.], [2., 2.], [3., 3.]]\n",
    "Y = [0., 1., 2., 3.]\n",
    "print(\"X:\", X)\n",
    "print(\"y:\", Y)\n",
    "\n",
    "reg = linear_model.BayesianRidge()\n",
    "\n",
    "reg.fit(X, Y)\n",
    "print(\"w:\", reg.coef_)\n",
    "\n",
    "print(\"predicion for [1., 0.]:\")\n",
    "print(reg.predict([[1, 0.]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2f0087-10e2-47ec-8129-af6eef6b0628",
   "metadata": {},
   "source": [
    "### [Logistic regression](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)\n",
    "\n",
    "Logistic regression, despite its name, is a linear model for classification rather than regression. Logistic regression is also known in the literature as logit regression, maximum-entropy classification (MaxEnt) or the log-linear classifier. In this model, the probabilities describing the possible outcomes of a single trial are modeled using a logistic function.\n",
    "\n",
    "Logistic regression is implemented in `LogisticRegression`. This implementation can fit **binary**, **One-vs-Rest**, or **multinomial** logistic regression with optional $l_1$, $l_2$, or Elastic-Net regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e534e9-0d2f-4134-9453-eae20f7a0ea3",
   "metadata": {},
   "source": [
    "### [Generalized Linear Regression](https://scikit-learn.org/stable/modules/linear_model.html#generalized-linear-regression)\n",
    "\n",
    "* See therory at the above link.\n",
    "* Implementation provided is `TweedieRegressor`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb1ac93-97a1-4477-95a9-e04a02bcc96a",
   "metadata": {},
   "source": [
    "### [Stochastic Gradient Descent - SGD](https://scikit-learn.org/stable/modules/linear_model.html#stochastic-gradient-descent-sgd)\n",
    "\n",
    "Stochastic gradient descent is a simple yet very efficient approach to fit linear models. It is particularly useful when the number of samples (and the number of features) is very large. The `partial_fit` method allows online/out-of-core learning.\n",
    "\n",
    "Provided are:\n",
    "* SGDClassifier \n",
    "* SGDRegressor\n",
    "\n",
    "These fit linear models for classification and regression using different (convex) loss functions and different penalties. E.g., with `loss=\"log\"`, `SGDClassifier` fits a logistic regression model, while with `loss=\"hinge\"` it fits a linear support vector machine (SVM)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749c22cd-eb8b-4685-8a66-f3927d494fc1",
   "metadata": {},
   "source": [
    "### [Perceptron](https://scikit-learn.org/stable/modules/linear_model.html#perceptron)\n",
    "\n",
    "* `Perceptron` is a classification algorithm which shares the same underlying implementation with `SGDClassifier`. In fact, `Perceptron()` is equivalent to `SGDClassifier(loss=\"perceptron\", eta0=1, learning_rate=\"constant\", penalty=None)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e00a8c-a53a-4405-805e-4ddabd77e1f1",
   "metadata": {},
   "source": [
    "### [Passive Aggressive Algorithms](https://scikit-learn.org/stable/modules/linear_model.html#passive-aggressive-algorithms)\n",
    "\n",
    "The passive-aggressive algorithms are a family of algorithms for large-scale learning. They are similar to the Perceptron in that they do not require a learning rate. However, contrary to the Perceptron, they include a regularization parameter `C`.\n",
    "\n",
    "For classification, `PassiveAggressiveClassifier` can be used with `loss='hinge'` (PA-I) or `loss='squared_hinge'` (PA-II). For regression, `PassiveAggressiveRegressor` can be used with `loss='epsilon_insensitive'` (PA-I) or `loss='squared_epsilon_insensitive'` (PA-II)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0b996f-096f-4d62-8f14-de7f9743929f",
   "metadata": {},
   "source": [
    "### [Robustness regression: outliers and modeling errors](https://scikit-learn.org/stable/modules/linear_model.html#robustness-regression-outliers-and-modeling-errors)\n",
    "\n",
    "See theory at the above link.\n",
    "\n",
    "Provided are:\n",
    "* `RANSACRegressor`\n",
    "* `TheilSenRegressor`\n",
    "* `HuberRegressor`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267159ec-5c2e-4644-899e-c731e25408b2",
   "metadata": {},
   "source": [
    "### [Polynomial regression: extending linear models with basis function](https://scikit-learn.org/stable/modules/linear_model.html#polynomial-regression-extending-linear-models-with-basis-functions)\n",
    "\n",
    "One common pattern within machine learning is to use linear models trained on nonlinear functions of the data. This approach maintains the generally fast performance of linear methods, while allowing them to fit a much wider range of data.\n",
    "\n",
    "For example, a simple linear regression can be extended by constructing **polynomial features** from the coefficients. \n",
    "\n",
    "Polynomial regression is in the same class of linear models we considered above (i.e. the model is linear in $w$) and can be solved by the same techniques. By considering linear fits within a higher-dimensional space built with these basis functions, the model has the flexibility to fit a much broader range of data.\n",
    "\n",
    "* The provided **transformer** is `PolynomialFeatures`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7bb573d6-3dda-4cea-8b25-3d9fe62f1d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [2 3]\n",
      " [4 5]]\n",
      "[[ 1.  0.  1.  0.  0.  1.]\n",
      " [ 1.  2.  3.  4.  6.  9.]\n",
      " [ 1.  4.  5. 16. 20. 25.]]\n"
     ]
    }
   ],
   "source": [
    "# Example of using PolynomialFeatures:\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import numpy as np\n",
    "\n",
    "X = np.arange(6).reshape(3, 2)\n",
    "print(X)\n",
    "\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "print(X_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786059bc-5a54-473d-8c72-a66763b9fe07",
   "metadata": {},
   "source": [
    "The features of `X` have been transformed from $[x_1, x_2]$ to $[1, x_1, x_2, x^2_1, x_1 x_2, x^2_2]$, and can now be used within any linear model.\n",
    "\n",
    "This sort of preprocessing can be streamlined with the `Pipeline` tools. A single object representing a simple polynomial regression can be created and used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dc92e076-d349-413f-9a5f-e036d306ea4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        ('poly', PolynomialFeatures(degree=3)),  # <-- Transform here.\n",
    "        ('linear', LinearRegression(fit_intercept=False))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e7998ec8-9cab-4d2d-beff-8259ec7bef79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [0 1 2 3 4]\n",
      "y: [  3   1  -5 -21 -53]\n"
     ]
    }
   ],
   "source": [
    "# fit to an order-3 polynomial data\n",
    "x = np.arange(5)\n",
    "y = 3 - 2 * x + x ** 2 - x ** 3\n",
    "print(\"x:\", x)\n",
    "print(\"y:\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "865ca870-ba0f-47f3-bd5a-d39282ad52fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3., -2.,  1., -1.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = pipe.fit(x[:, np.newaxis], y)\n",
    "pipe.named_steps['linear'].coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64ad53d-a1d0-4a70-a105-53afaa34232c",
   "metadata": {},
   "source": [
    "The linear model trained on polynomial features is able to exactly recover the input polynomial coefficients (by nature of this example).\n",
    "\n",
    "In some cases it’s not necessary to include higher powers of any single feature, but only the so-called *interaction features* that multiply together at most $d$ distinct features. These can be gotten from `PolynomialFeatures` with the setting `interaction_only=True`.\n",
    "\n",
    "For example, when dealing with boolean features, $x^n_i=x_i$ for all $n$ and is therefore useless; but $x_i x_j$ represents the conjunction of two booleans. This way, we can solve the XOR problem with a linear classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "94f35c4e-fbec-4408-9c33-c5b028d62df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      " [[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]]\n",
      "y = X[:, 0] ^ X[:, 1]: [0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import numpy as np\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = X[:, 0] ^ X[:, 1]\n",
    "print(\"X:\\n\", X)\n",
    "print(\"y = X[:, 0] ^ X[:, 1]:\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3dead736-302a-4cba-9c66-0c0d2079c7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X after `PolynomialFeatures` applied (interaction_only=True): [[1 0 0 0]\n",
      " [1 0 1 0]\n",
      " [1 1 0 0]\n",
      " [1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "X = PolynomialFeatures(interaction_only=True).fit_transform(X).astype(int)\n",
    "print(\"X after `PolynomialFeatures` applied (interaction_only=True):\", X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8b4b7e60-3bce-4f02-8fab-50aa28b38aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now fit a Perceptron (which by itself can't solve XOR)\n",
    "clf = Perceptron(\n",
    "    fit_intercept=False, \n",
    "    max_iter=10, \n",
    "    tol=None,\n",
    "    shuffle=False\n",
    ").fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "62d61aba-f7f9-44ce-951f-d000bfb3e2f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2a16ddf2-ad91-4546-87ae-8a5e26e3ca18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e54a512-21fe-4e0f-a34b-0d4883d90576",
   "metadata": {},
   "source": [
    "And the classifier “predictions” are perfect."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
