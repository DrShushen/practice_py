{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5d14aa6-2f3c-418c-9122-c0190e6dbfc1",
   "metadata": {},
   "source": [
    "# Cross-validation: evaluating estimator performance\n",
    "\n",
    "From this guide: https://scikit-learn.org/stable/modules/cross_validation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf24b25-d7c4-4a61-96d9-9c76e421967d",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Learning the parameters of a prediction function and testing it on the *same data* is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called **overfitting**. To avoid it, it is common practice when performing a (supervised) machine learning experiment to *hold out* part of the available data as a test set `X_test, y_test`.  \n",
    "\n",
    "Here is a flowchart of typical cross validation workflow in model training. The best parameters can be determined by e.g. grid search techniques.\n",
    "\n",
    "<img src=\"https://scikit-learn.org/stable/_images/grid_search_workflow.png\" width=400 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e7e0327-fc00-44c2-9f26-0a674f622561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4) (150,)\n",
      "[5.1 3.5 1.4 0.2]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Using train_test_split() to split data:\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "print(X[0])\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c280029-9817-421e-b954-9a6bad346f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (90, 4) y_train.shape: (90,)\n",
      "X_test.shape: (60, 4) y_test.shape: (60,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=0.4, \n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "print(\"X_train.shape:\", X_train.shape, \"y_train.shape:\", y_train.shape)\n",
    "print(\"X_test.shape:\", X_test.shape, \"y_test.shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce6852b2-87ff-44b7-ac23-7ac81e5e3d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060dfece-15b0-483a-8eb9-baf48dc1ae46",
   "metadata": {},
   "source": [
    "When evaluating different settings (‚Äúhyperparameters‚Äù) for estimators, such as the `C` setting that must be manually set for an SVM, there is still a risk of **overfitting on the *test set*** because the parameters can be tweaked until the estimator performs optimally. This way, knowledge about the test set can ‚Äúleak‚Äù into the model and evaluation metrics no longer report on generalization performance. \n",
    "\n",
    "To solve this problem, yet another part of the dataset can be held out as a so-called ‚Äúvalidation set‚Äù: training proceeds on the training set, after which evaluation is done on the validation set, and when the experiment seems to be successful, final evaluation can be done on the test set.\n",
    "\n",
    "However, by partitioning the available data into three sets, we drastically reduce the number of samples which can be used for learning the model, and the results can depend on a particular random choice for the pair of (train, validation) sets.\n",
    "\n",
    "A solution to this problem is a procedure called **cross-validation** (CV for short). \n",
    "\n",
    "> A test set should still be held out for final evaluation, but the validation set is no longer needed when doing CV. \n",
    "\n",
    "In the basic approach, called *k*-fold CV, the training set is split into *k* smaller sets (other approaches are described below, but generally follow the same principles). The following procedure is followed for each of the *k* ‚Äúfolds‚Äù:\n",
    "\n",
    "* A model is trained using $k-1$ of the folds as training data;\n",
    "* the resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy).\n",
    "\n",
    "The performance measure reported by *k*-fold cross-validation **is then the average** of the values computed in the loop. This approach can be computationally expensive, but does not waste too much data (as is the case when fixing an arbitrary validation set), which is a major advantage in problems such as inverse inference where the number of samples is very small.\n",
    "\n",
    "<img src=\"https://scikit-learn.org/stable/_images/grid_search_cross_validation.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aac617b-85f0-4b49-b1b1-e2a0a65ddd11",
   "metadata": {},
   "source": [
    "## [Computing cross-validated metrics](https://scikit-learn.org/stable/modules/cross_validation.html#computing-cross-validated-metrics)\n",
    "\n",
    "The simplest way to use cross-validation is to call the `cross_val_score` helper function on the estimator and the dataset.\n",
    "\n",
    "The following example demonstrates how to estimate the accuracy of a linear kernel support vector machine on the *iris dataset* by splitting the data, fitting a model and computing the score **5 consecutive times (with different splits each time)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5975007f-2fcc-41d3-94a6-fbc44bdbb201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.96666667 1.         0.96666667 0.96666667 1.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score  # <-- This helper function.\n",
    "\n",
    "clf = svm.SVC(kernel='linear', C=1, random_state=42)\n",
    "\n",
    "scores = cross_val_score(clf, X, y, cv=5, verbose=True)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600531cc-4e0d-42c0-bfe7-df60da6fb5b6",
   "metadata": {},
   "source": [
    "The mean score and the standard deviation are hence given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00687e1a-9a34-4fe1-b677-96f73bb836e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98 accuracy with a standard deviation of 0.02\n"
     ]
    }
   ],
   "source": [
    "print(f\"{scores.mean():0.2f} accuracy with a standard deviation of {scores.std():0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d40a50c-61ae-4f21-80ef-7313acd8ec3f",
   "metadata": {},
   "source": [
    "* ‚ö†Ô∏è By default, the score computed at each CV iteration is the **score method of the estimator**. \n",
    "\n",
    "It is possible to change this by using the scoring parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a082a0e9-b2de-4921-9740-8be1f03f209b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.96658312 1.         0.96658312 0.96658312 1.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "scores = cross_val_score(\n",
    "    clf, \n",
    "    X, \n",
    "    y, \n",
    "    cv=5, \n",
    "    scoring='f1_macro'  # <-- This.\n",
    ")\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ed9307-fe13-4df1-bffd-45ac2c8b40a2",
   "metadata": {},
   "source": [
    "* üîç See [The scoring parameter: defining model evaluation rules](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter) for details.\n",
    "\n",
    "* üí° When the `cv` argument is an integer, `cross_val_score` uses the `KFold` or `StratifiedKFold` strategies by default, the latter being used if the estimator derives from `ClassifierMixin`.\n",
    "\n",
    "It is also possible to use other **cross validation strategies** by passing a `cross validation iterator` instead, for instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c34965a3-b16f-4840-9a12-711a1a685ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.97777778 0.97777778 1.         0.95555556 1.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "n_samples = X.shape[0]\n",
    "\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\n",
    "\n",
    "scores = cross_val_score(\n",
    "    clf, \n",
    "    X, \n",
    "    y, \n",
    "    cv=cv  # ‚ö†Ô∏è We are passing a `ShuffleSplit` cross validation iterator here (which is a CV strat.), not integer anymore!\n",
    ")\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3649ca-8401-47ce-9e9a-a431f8ece2ce",
   "metadata": {},
   "source": [
    "Another option is to use an `iterable` yielding `(train, test)` splits as **arrays of indices**, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1617bd8a-54db-4f6d-bab1-3efd73244843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_cv_2folds(X):\n",
    "    n = X.shape[0]\n",
    "    i = 1\n",
    "    while i <= 2:\n",
    "        idx = np.arange(n * (i - 1) / 2, n * i / 2, dtype=int)\n",
    "        yield idx, idx  # Yield two arrays of indices.\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78ee5ab1-988b-4352-a7c7-1749f349df76",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_cv = custom_cv_2folds(X)  # Initialise the iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "980cf6b9-9db5-4ab9-b5fa-62c80b24cb11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.97333333])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(clf, X, y, cv=custom_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f5e4df-2314-43b6-9add-3fed0b10b7d8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6780740a-3d6a-4c0f-9461-c8e11df84cbf",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è Warning on preprocessing / data transformation with held out data\n",
    "\n",
    "Just as it is important to test a predictor on data **held-out** from training, preprocessing (such as standardization, feature selection, etc.) and similar data **transformations** similarly should be *learnt from a training set (only)* and applied to held-out data for prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddcf1344-e242-42cd-b146-f9de0b6ee62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (150, 4)\n",
      "X_train.shape: (90, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "print(\"X.shape:\", X.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=0.4, \n",
    "    random_state=0\n",
    ")\n",
    "print(\"X_train.shape:\", X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4e657c6-9ed2-4762-996c-842bcb4fc973",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(X_train)  # Fit scaler on TRAINING set.\n",
    "X_train_transformed = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52dffdd4-7e42-4233-a145-2fe62af42d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(C=1).fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3f501fe-b20a-4671-81f3-e02fc4045d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_transformed = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7edf7637-7be9-4734-b88c-dbc7e8ec0e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test_transformed, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e50cef-32ad-453c-af1e-04af939c20d7",
   "metadata": {},
   "source": [
    "A `Pipeline` makes it **easier** to compose estimators, *providing this behavior under cross-validation*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e086121-4d96-4499-ad93-de47c37d61da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.pipeline.Pipeline'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.97777778, 0.93333333, 0.95555556, 0.93333333, 0.97777778])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "clf = make_pipeline(\n",
    "    preprocessing.StandardScaler(), \n",
    "    svm.SVC(C=1)\n",
    ")\n",
    "print(type(clf))\n",
    "cross_val_score(clf, X, y, cv=cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ebefd6-6780-49d1-a1ab-c7b36da57bc9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cdd84c-5b35-4007-a747-75566aff1d93",
   "metadata": {},
   "source": [
    "## [The `cross_validate` function and multiple metric evaluation](https://scikit-learn.org/stable/modules/cross_validation.html#the-cross-validate-function-and-multiple-metric-evaluation)\n",
    "\n",
    "The `cross_validate` function differs from `cross_val_score` in two ways:\n",
    "1. It allows specifying **multiple metrics** for evaluation.\n",
    "2. It *returns a `dict`* containing **fit-times**, **score-times** *(and optionally training scores as well as fitted estimators)* in addition to the test score.\n",
    "\n",
    "Returns:\n",
    "* For **single metric** evaluation, where the scoring parameter is a *string*, *callable* or `None`, the keys will be - `['test_score', 'fit_time', 'score_time']`\n",
    "* And for **multiple metric** evaluation, the return value is a `dict` with the following keys - `['test_<scorer1_name>', 'test_<scorer2_name>', 'test_<scorer...>', 'fit_time', 'score_time']`\n",
    "\n",
    "`return_train_score` is set to `False` by default to save computation time. To evaluate the scores on the training set as well you need to set it to `True`. You may also retain the estimator fitted on each training set by setting `return_estimator=True`.\n",
    "\n",
    "* ü§î I believe it's talking about actual time (wall clock time) taken for fitting and scoring here, [see this](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate)...\n",
    "\n",
    "The multiple metrics can be specified either as a *list, tuple or set of predefined scorer names*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "93012b67-b9b7-47cf-9d61-ded849b9c6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate  # <-- This function.\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "scoring = ['precision_macro', 'recall_macro', 'f1_micro']  # Choose *three* metrics here.\n",
    "\n",
    "clf = svm.SVC(kernel='linear', C=1, random_state=0)  # Define estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "73c61b4b-d96b-491a-84e8-9938eeadb5eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fit_time',\n",
       " 'score_time',\n",
       " 'test_f1_micro',\n",
       " 'test_precision_macro',\n",
       " 'test_recall_macro']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_validate(clf, X, y, scoring=scoring)\n",
    "\n",
    "# Show all the keys of returned `dict`:\n",
    "sorted(scores.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f5f18cf-d6de-49c5-8e66-e5850cda1b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time:\n",
      "> type: <class 'numpy.ndarray'>\n",
      "[0.00093913 0.0009346  0.00081086 0.00087953 0.00082636]\n",
      "================================================================================\n",
      "score_time:\n",
      "> type: <class 'numpy.ndarray'>\n",
      "[0.00221133 0.0021584  0.00215769 0.00214863 0.00218296]\n",
      "================================================================================\n",
      "\n",
      "\n",
      "[Actual Scores]:\n",
      "\n",
      "test_precision_macro:\n",
      "> type: <class 'numpy.ndarray'>\n",
      "[0.96969697 1.         0.96969697 0.96969697 1.        ]\n",
      "================================================================================\n",
      "test_recall_macro:\n",
      "> type: <class 'numpy.ndarray'>\n",
      "[0.96666667 1.         0.96666667 0.96666667 1.        ]\n",
      "================================================================================\n",
      "test_f1_micro:\n",
      "> type: <class 'numpy.ndarray'>\n",
      "[0.96666667 1.         0.96666667 0.96666667 1.        ]\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "for idx, (k, v) in enumerate(scores.items()):\n",
    "    print(f\"{k}:\")\n",
    "    print(f\"> type: {type(v)}\")\n",
    "    print(v)\n",
    "    print(\"=\" * 80)\n",
    "    if idx == 1:\n",
    "        print(\"\\n\\n[Actual Scores]:\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0dea96d4-0cfc-45ec-ac40-bc59e59b32fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fit_time', 'score_time', 'test_prec_macro', 'test_rec_macro', 'train_prec_macro', 'train_rec_macro']\n"
     ]
    }
   ],
   "source": [
    "# Similar example, but:\n",
    "# using a dict mapping scorer name to a predefined or custom scoring function.\n",
    "\n",
    "from sklearn.metrics import make_scorer  # Helper.\n",
    "\n",
    "scoring = {\n",
    "    'prec_macro': 'precision_macro',  # Existing scorer named by a string.\n",
    "    'rec_macro': make_scorer(recall_score, average='macro')  # Custom scorer, here defined via a helper function `make_scorer`.\n",
    "}\n",
    "\n",
    "scores = cross_validate(\n",
    "    clf, \n",
    "    X, \n",
    "    y, \n",
    "    scoring=scoring,\n",
    "    cv=5, \n",
    "    return_train_score=True  # This time, also return train score...\n",
    ")\n",
    "print(sorted(scores.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fc086afb-1ef9-49e0-a351-60c2cfc8ea82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time:\n",
      "> type: <class 'numpy.ndarray'>\n",
      "[0.00091481 0.0006578  0.00063038 0.0006721  0.00066805]\n",
      "================================================================================\n",
      "score_time:\n",
      "> type: <class 'numpy.ndarray'>\n",
      "[0.00119829 0.00108337 0.00120997 0.00126553 0.00119758]\n",
      "================================================================================\n",
      "\n",
      "\n",
      "[Actual Scores]:\n",
      "\n",
      "test_prec_macro:\n",
      "> type: <class 'numpy.ndarray'>\n",
      "[0.96969697 1.         0.96969697 0.96969697 1.        ]\n",
      "================================================================================\n",
      "train_prec_macro:\n",
      "> type: <class 'numpy.ndarray'>\n",
      "[0.97674419 0.97674419 0.99186992 0.98412698 0.98333333]\n",
      "================================================================================\n",
      "test_rec_macro:\n",
      "> type: <class 'numpy.ndarray'>\n",
      "[0.96666667 1.         0.96666667 0.96666667 1.        ]\n",
      "================================================================================\n",
      "train_rec_macro:\n",
      "> type: <class 'numpy.ndarray'>\n",
      "[0.975      0.975      0.99166667 0.98333333 0.98333333]\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "for idx, (k, v) in enumerate(scores.items()):\n",
    "    print(f\"{k}:\")\n",
    "    print(f\"> type: {type(v)}\")\n",
    "    print(v)\n",
    "    print(\"=\" * 80)\n",
    "    if idx == 1:\n",
    "        print(\"\\n\\n[Actual Scores]:\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b299d78c-0013-477e-b85b-120bf8a1db49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['estimator', 'fit_time', 'score_time', 'test_score']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of `cross_validate` using only a single metric:\n",
    "scores = cross_validate(clf, X, y, scoring='precision_macro', cv=5, return_estimator=True)\n",
    "sorted(scores.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5336e2f-9ca6-4549-bdfa-8ad82f13f5b4",
   "metadata": {},
   "source": [
    "### [Obtaining predictions by cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html#obtaining-predictions-by-cross-validation)\n",
    "\n",
    "The function `cross_val_predict` has a similar interface to `cross_val_score`, but returns:\n",
    "\n",
    "* **For each element in the input, the prediction that was obtained for that element when it was in the test set**. \\[‚ÄºÔ∏è\\]\n",
    "\n",
    "Only cross-validation strategies that assign all elements to a test set exactly once can be used (otherwise, an exception is raised).\n",
    "\n",
    "\n",
    "‚ö†Ô∏è **Warning** Note on inappropriate usage of `cross_val_predict`:\n",
    "The result of `cross_val_predict` may be different from those obtained using `cross_val_score` as **the elements are grouped in different ways**. The function `cross_val_score` takes an *average over cross-validation folds*, whereas `cross_val_predict` simply *returns the labels (or probabilities) from several distinct models undistinguished*. Thus, `cross_val_predict` is ***not an appropriate measure of generalisation error***.\n",
    "\n",
    "The function cross_val_predict is appropriate for:\n",
    "* Visualization of predictions obtained from different models.\n",
    "* Model blending: When predictions of one supervised estimator are used to train another estimator in ensemble methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2a0e69-7b96-4a9b-b6a9-ffb051c9a14d",
   "metadata": {},
   "source": [
    "## [Cross validation iterators](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-iterators)\n",
    "\n",
    "The following sections list utilities to generate indices that can be used to generate dataset splits according to different cross validation strategies.\n",
    "\n",
    "* [Cross-validation iterators for i.i.d. data](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-iterators-for-i-i-d-data):\n",
    "    1. `KFold`\n",
    "    2. `RepeatedKFold`\n",
    "    3. `LeaveOneOut`\n",
    "    4. `LeavePOut`\n",
    "    5. `ShuffleSplit`\n",
    "* [Cross-validation iterators with stratification based on class labels](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-iterators-with-stratification-based-on-class-labels):\n",
    "    * Useful when large imbalance in the distribution of the target classes.\n",
    "    6. `StratifiedKFold` (Note also`RepeatedStratifiedKFold`)\n",
    "    7. `StratifiedShuffleSplit`\n",
    "* [Cross-validation iterators for grouped data](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-iterators-for-grouped-data):\n",
    "    * The i.i.d. assumption is broken if the underlying generative process yield groups of dependent samples.\n",
    "    * An example would be when there is medical data collected from multiple patients, with multiple samples taken from each patient. And such data is likely to be dependent on the individual group. In our example, the patient id for each sample will be its group identifier.\n",
    "    * ‚ÄºÔ∏è **In this case we would like to know if a model trained on a particular set of groups generalizes well to the *unseen* groups.** To measure this, we need to ensure that *all the samples in the validation fold come from groups that are not represented at all in the paired training fold*.\n",
    "    * The following cross-validation splitters can be used to do that. The grouping identifier for the samples is specified via the `groups` parameter.\n",
    "    8. `GroupKFold`\n",
    "    9. `LeaveOneGroupOut`\n",
    "    10. `LeavePGroupsOut`\n",
    "    11. `GroupShuffleSplit`\n",
    "* [Predefined Fold-Splits / Validation-Sets](https://scikit-learn.org/stable/modules/cross_validation.html#predefined-fold-splits-validation-sets)\n",
    "    12. `PredefinedSplit`\n",
    "* [Cross validation of time series data](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-of-time-series-data):\n",
    "    13. `TimeSeriesSplit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1a56a6a9-c7c1-437d-a2ec-68b84a0f566f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th split:\n",
      "train idx: [2 3] test idx: [0 1]\n",
      "train set: ['c', 'd'] test set: ['a', 'b']\n",
      "\n",
      "1th split:\n",
      "train idx: [0 1] test idx: [2 3]\n",
      "train set: ['a', 'b'] test set: ['c', 'd']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. KFold \n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X = [\"a\", \"b\", \"c\", \"d\"]\n",
    "kf = KFold(n_splits=2)\n",
    "for kth, (train, test) in enumerate(kf.split(X)):\n",
    "    print(f\"{kth}th split:\")\n",
    "    print(f\"train idx: {train} test idx: {test}\")\n",
    "    print(f\"train set: {[X[i_] for i_ in train]} test set: {[X[i_] for i_ in test]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "66a794bc-6568-4b28-a89d-324d685142bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      " [[1 2]\n",
      " [3 4]\n",
      " [1 2]\n",
      " [3 4]] \n",
      "\n",
      "0th split:\n",
      "train idx: [2 3] test idx: [0 1]\n",
      "train set: [array([1, 2]), array([3, 4])] test set: [array([1, 2]), array([3, 4])]\n",
      "\n",
      "1th split:\n",
      "train idx: [0 1] test idx: [2 3]\n",
      "train set: [array([1, 2]), array([3, 4])] test set: [array([1, 2]), array([3, 4])]\n",
      "\n",
      "2th split:\n",
      "train idx: [0 2] test idx: [1 3]\n",
      "train set: [array([1, 2]), array([1, 2])] test set: [array([3, 4]), array([3, 4])]\n",
      "\n",
      "3th split:\n",
      "train idx: [1 3] test idx: [0 2]\n",
      "train set: [array([3, 4]), array([3, 4])] test set: [array([1, 2]), array([1, 2])]\n",
      "\n",
      "> Note K=2 k-fold ran TWICE ==> Total of 4 runs.\n"
     ]
    }
   ],
   "source": [
    "# 2. RepeatedKFold\n",
    "# Repeats K-Fold n times. It can be used when one requires to run KFold n times, producing different splits in each repetition.\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "print(\"X:\\n\", X, \"\\n\")\n",
    "random_state = 12883823\n",
    "rkf = RepeatedKFold(n_splits=2, n_repeats=2, random_state=random_state)\n",
    "\n",
    "for kth, (train, test) in enumerate(rkf.split(X)):\n",
    "    print(f\"{kth}th split:\")\n",
    "    print(f\"train idx: {train} test idx: {test}\")\n",
    "    print(f\"train set: {[X[i_] for i_ in train]} test set: {[X[i_] for i_ in test]}\")\n",
    "    print()\n",
    "\n",
    "print(\"> Note K=2 k-fold ran TWICE ==> Total of 4 runs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494723c7-da8d-4eb1-8034-35c0759e15a5",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "Leave One Out (LOO):\n",
    "* `LeaveOneOut` (or LOO) is a simple cross-validation. Each learning set is created by taking all the samples except one, the test set being **the (single) sample left out**. Thus, for $n$ samples, we have $n$ different training sets and $n$ different tests set. \n",
    "* This cross-validation procedure does not waste much data as only one sample is removed from the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8d8965ff-d51b-4eed-96bd-97606cdd1b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [1, 2, 3, 4] \n",
      "\n",
      "0th split:\n",
      "train idx: [1 2 3] test idx: [0]\n",
      "train set: [2, 3, 4] test set: [1]\n",
      "\n",
      "1th split:\n",
      "train idx: [0 2 3] test idx: [1]\n",
      "train set: [1, 3, 4] test set: [2]\n",
      "\n",
      "2th split:\n",
      "train idx: [0 1 3] test idx: [2]\n",
      "train set: [1, 2, 4] test set: [3]\n",
      "\n",
      "3th split:\n",
      "train idx: [0 1 2] test idx: [3]\n",
      "train set: [1, 2, 3] test set: [4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. LeaveOneOut\n",
    "\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "X = [1, 2, 3, 4]\n",
    "print(\"X:\", X, \"\\n\")\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "for kth, (train, test) in enumerate(loo.split(X)):\n",
    "    print(f\"{kth}th split:\")\n",
    "    print(f\"train idx: {train} test idx: {test}\")\n",
    "    print(f\"train set: {[X[i_] for i_ in train]} test set: {[X[i_] for i_ in test]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af32c4b3-40ed-4f72-877c-72ebe8f8b48c",
   "metadata": {},
   "source": [
    "Leave P Out (LPO):\n",
    "\n",
    "* `LeavePOut` is very similar to `LeaveOneOut` as it creates all the possible training/test sets by removing $p$ samples from the complete set. \n",
    "* For $n$ samples, this produces ${n \\choose p}$ train-test pairs. \n",
    "* **Unlike `LeaveOneOut` and `KFold`, the test sets will *overlap* for $p>1$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bf9cb2e2-2fe0-4e5d-8f48-02ae6e2f66b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [100 200 300 400] \n",
      "\n",
      "0th split:\n",
      "train idx: [2 3] test idx: [0 1]\n",
      "train set: [300, 400] test set: [100, 200]\n",
      "\n",
      "1th split:\n",
      "train idx: [1 3] test idx: [0 2]\n",
      "train set: [200, 400] test set: [100, 300]\n",
      "\n",
      "2th split:\n",
      "train idx: [1 2] test idx: [0 3]\n",
      "train set: [200, 300] test set: [100, 400]\n",
      "\n",
      "3th split:\n",
      "train idx: [0 3] test idx: [1 2]\n",
      "train set: [100, 400] test set: [200, 300]\n",
      "\n",
      "4th split:\n",
      "train idx: [0 2] test idx: [1 3]\n",
      "train set: [100, 300] test set: [200, 400]\n",
      "\n",
      "5th split:\n",
      "train idx: [0 1] test idx: [2 3]\n",
      "train set: [100, 200] test set: [300, 400]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. LeavePOut\n",
    "# Leave-2-Out on a dataset with 4 samples: \n",
    "\n",
    "from sklearn.model_selection import LeavePOut\n",
    "\n",
    "X = (1 + np.arange(4)) * 100\n",
    "print(\"X:\", X, \"\\n\")\n",
    "\n",
    "lpo = LeavePOut(p=2)\n",
    "\n",
    "for kth, (train, test) in enumerate(lpo.split(X)):\n",
    "    print(f\"{kth}th split:\")\n",
    "    print(f\"train idx: {train} test idx: {test}\")\n",
    "    print(f\"train set: {[X[i_] for i_ in train]} test set: {[X[i_] for i_ in test]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627e6bd8-6a5d-4498-8727-2931bdfe1e7a",
   "metadata": {},
   "source": [
    "Random permutations cross-validation a.k.a. Shuffle & Split:\n",
    "\n",
    "* The `ShuffleSplit` iterator will generate a user defined number of independent train / test dataset splits. \n",
    "* Samples are first shuffled and then split into a pair of train and test sets.\n",
    "* It is possible to control the randomness for reproducibility of the results by explicitly seeding the `random_state` pseudo random number generator.\n",
    "\n",
    "\n",
    "* `ShuffleSplit` is thus a good alternative to `KFold` cross validation that allows a finer control on the number of iterations and the proportion of samples on each side of the train / test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "81347897-d168-4627-9c50-9955d82adbff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [0 1 2 3 4 5 6 7 8 9] \n",
      "\n",
      "0th split:\n",
      "train idx: [9 1 6 7 3 0 5] test idx: [2 8 4]\n",
      "train set: [9, 1, 6, 7, 3, 0, 5] test set: [2, 8, 4]\n",
      "\n",
      "1th split:\n",
      "train idx: [2 9 8 0 6 7 4] test idx: [3 5 1]\n",
      "train set: [2, 9, 8, 0, 6, 7, 4] test set: [3, 5, 1]\n",
      "\n",
      "2th split:\n",
      "train idx: [4 5 1 0 6 9 7] test idx: [2 3 8]\n",
      "train set: [4, 5, 1, 0, 6, 9, 7] test set: [2, 3, 8]\n",
      "\n",
      "3th split:\n",
      "train idx: [2 7 5 8 0 3 4] test idx: [6 1 9]\n",
      "train set: [2, 7, 5, 8, 0, 3, 4] test set: [6, 1, 9]\n",
      "\n",
      "4th split:\n",
      "train idx: [4 1 0 6 8 9 3] test idx: [5 2 7]\n",
      "train set: [4, 1, 0, 6, 8, 9, 3] test set: [5, 2, 7]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. ShuffleSplit\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "X = np.arange(10)\n",
    "print(\"X:\", X, \"\\n\")\n",
    "\n",
    "ss = ShuffleSplit(n_splits=5, test_size=0.25, random_state=0)\n",
    "\n",
    "for kth, (train, test) in enumerate(ss.split(X)):\n",
    "    print(f\"{kth}th split:\")\n",
    "    print(f\"train idx: {train} test idx: {test}\")\n",
    "    print(f\"train set: {[X[i_] for i_ in train]} test set: {[X[i_] for i_ in test]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47239957-92e8-41dd-aceb-8364833d4e77",
   "metadata": {},
   "source": [
    "Stratified k-fold:\n",
    "\n",
    "* `StratifiedKFold` is a variation of k-fold which returns stratified folds: each set contains *approximately the same percentage of samples of each target class* as the complete set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "599a26df-1fde-4ebb-8d70-00eabec8f53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X [(50, 1)]:\n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      " ...\n",
      "y [(50,)]:\n",
      " [0 0 1 1 1 1 1] ...\n",
      "\n",
      "---3 folds---\n",
      "Showing [<0 count> <1 count>] among y for each split:\n",
      "\n",
      "StratifiedKFold:\n",
      "train -  [30  3]   |   test -  [15  2]\n",
      "train -  [30  3]   |   test -  [15  2]\n",
      "train -  [30  4]   |   test -  [15  1]\n",
      "\n",
      "KFold (NOT stratified, for comparison):\n",
      "train -  [28  5]   |   test -  [17]\n",
      "train -  [28  5]   |   test -  [17]\n",
      "train -  [34]   |   test -  [11  5]\n"
     ]
    }
   ],
   "source": [
    "# 6. StratifiedKFold\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import numpy as np\n",
    "\n",
    "X, y = np.ones((50, 1)), np.hstack(([0] * 45, [1] * 5))\n",
    "print(f\"X [{X.shape}]:\\n\", X[-7:], \"\\n ...\")\n",
    "print(f\"y [{y.shape}]:\\n\", y[-7:], \"...\")\n",
    "\n",
    "print(\"\\n---3 folds---\")\n",
    "print(\"Showing [<0 count> <1 count>] among y for each split:\")\n",
    "\n",
    "print(\"\\nStratifiedKFold:\")\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "for train, test in skf.split(X, y):\n",
    "    print('train -  {}   |   test -  {}'.format(\n",
    "        np.bincount(y[train]), np.bincount(y[test])))\n",
    "\n",
    "print(\"\\nKFold (NOT stratified, for comparison):\")\n",
    "kf = KFold(n_splits=3)\n",
    "for train, test in kf.split(X, y):\n",
    "    print('train -  {}   |   test -  {}'.format(\n",
    "        np.bincount(y[train]), np.bincount(y[test])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa387d6-135b-4caf-986e-b744742b5220",
   "metadata": {},
   "source": [
    "<br/>\n",
    "Stratified Shuffle Split:\n",
    "\n",
    "* `StratifiedShuffleSplit` is a variation of ShuffleSplit, which returns stratified splits, i.e which creates splits by preserving the same percentage for each target class as in the complete set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25387b1c-1235-4db8-bfca-23ee8c82ee47",
   "metadata": {},
   "source": [
    "Group k-fold:\n",
    "\n",
    "* `GroupKFold` is a variation of k-fold which **ensures that the same group is *not* represented in both testing and training sets**. \n",
    "* For example if the data is obtained from different subjects with several samples per-subject and if the model is flexible enough to learn from highly person specific features it could fail to generalize to new subjects. \n",
    "* `GroupKFold` makes it possible to detect this kind of overfitting situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ff598bf9-b931-469a-9cc7-3296d973b6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:      [0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 8.8, 9, 10]\n",
      "y:      ['a', 'b', 'b', 'b', 'c', 'c', 'c', 'd', 'd', 'd']\n",
      "groups: [1, 1, 1, 2, 2, 2, 3, 3, 3, 3]\n",
      "\n",
      "0th split:\n",
      "train idx: [0 1 2 3 4 5] test idx: [6 7 8 9]\n",
      "train set: [0.1, 0.2, 2.2, 2.4, 2.3, 4.55] test set: [5.8, 8.8, 9, 10]\n",
      "train grp: [1, 1, 1, 2, 2, 2] test set: [3, 3, 3, 3]\n",
      "\n",
      "1th split:\n",
      "train idx: [0 1 2 6 7 8 9] test idx: [3 4 5]\n",
      "train set: [0.1, 0.2, 2.2, 5.8, 8.8, 9, 10] test set: [2.4, 2.3, 4.55]\n",
      "train grp: [1, 1, 1, 3, 3, 3, 3] test set: [2, 2, 2]\n",
      "\n",
      "2th split:\n",
      "train idx: [3 4 5 6 7 8 9] test idx: [0 1 2]\n",
      "train set: [2.4, 2.3, 4.55, 5.8, 8.8, 9, 10] test set: [0.1, 0.2, 2.2]\n",
      "train grp: [2, 2, 2, 3, 3, 3, 3] test set: [1, 1, 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8. GroupKFold\n",
    "# Imagine you have three subjects, each with an associated number from 1 to 3:\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "X = [0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 8.8, 9, 10]\n",
    "y = [\"a\", \"b\", \"b\", \"b\", \"c\", \"c\", \"c\", \"d\", \"d\", \"d\"]\n",
    "groups = [1, 1, 1, 2, 2, 2, 3, 3, 3, 3]\n",
    "\n",
    "print(\"X:     \", X)\n",
    "print(\"y:     \", y)\n",
    "print(\"groups:\", groups)\n",
    "print()\n",
    "\n",
    "gkf = GroupKFold(n_splits=3)\n",
    "\n",
    "for kth, (train, test) in enumerate(\n",
    "    gkf.split(\n",
    "        X, \n",
    "        y, \n",
    "        groups=groups  # Note the specification of `groups` parameter in `.split()` call!\n",
    "    )\n",
    "):\n",
    "    print(f\"{kth}th split:\")\n",
    "    print(f\"train idx: {train} test idx: {test}\")\n",
    "    print(f\"train set: {[X[i_] for i_ in train]} test set: {[X[i_] for i_ in test]}\")\n",
    "    print(f\"train grp: {[groups[i_] for i_ in train]} test set: {[groups[i_] for i_ in test]}\")\n",
    "    print()\n",
    "\n",
    "# Each subject is in a different testing fold, and the same subject is never in both testing and training. \n",
    "# Notice that the folds do not have exactly the same size due to the imbalance in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad08c2fe-c6ac-43a1-8ebf-cdae29eefa99",
   "metadata": {},
   "source": [
    "Leave One Group Out:\n",
    "\n",
    "* (Like before) `LeaveOneGroupOut` is a cross-validation scheme which holds out the samples according to a third-party provided array of integer groups. \n",
    "* (Like before) This group information can be used to encode arbitrary domain specific pre-defined cross-validation folds.\n",
    "* Each training set is thus constituted by all the samples *except the ones related to a specific group*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8f94d658-9124-4638-b576-ce9363b5834a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:      [1, 5, 10, 50, 60, 70, 80]\n",
      "y:      [0, 1, 1, 2, 2, 2, 2]\n",
      "groups: [1, 1, 2, 2, 3, 3, 3]\n",
      "\n",
      "0th split:\n",
      "train idx: [2 3 4 5 6] test idx: [0 1]\n",
      "train set: [10, 50, 60, 70, 80] test set: [1, 5]\n",
      "train grp: [2, 2, 3, 3, 3] test set: [1, 1]\n",
      "\n",
      "1th split:\n",
      "train idx: [0 1 4 5 6] test idx: [2 3]\n",
      "train set: [1, 5, 60, 70, 80] test set: [10, 50]\n",
      "train grp: [1, 1, 3, 3, 3] test set: [2, 2]\n",
      "\n",
      "2th split:\n",
      "train idx: [0 1 2 3] test idx: [4 5 6]\n",
      "train set: [1, 5, 10, 50] test set: [60, 70, 80]\n",
      "train grp: [1, 1, 2, 2] test set: [3, 3, 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "X = [1, 5, 10, 50, 60, 70, 80]\n",
    "y = [0, 1, 1, 2, 2, 2, 2]\n",
    "groups = [1, 1, 2, 2, 3, 3, 3]\n",
    "print(\"X:     \", X)\n",
    "print(\"y:     \", y)\n",
    "print(\"groups:\", groups)\n",
    "print()\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "for kth, (train, test) in enumerate(logo.split(X, y, groups=groups)):\n",
    "    print(f\"{kth}th split:\")\n",
    "    print(f\"train idx: {train} test idx: {test}\")\n",
    "    print(f\"train set: {[X[i_] for i_ in train]} test set: {[X[i_] for i_ in test]}\")\n",
    "    print(f\"train grp: {[groups[i_] for i_ in train]} test set: {[groups[i_] for i_ in test]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b353cca-546e-4558-8292-712c3d2f7c9b",
   "metadata": {},
   "source": [
    "Leave P Groups Out:\n",
    "* `LeavePGroupsOut` is similar as `LeaveOneGroupOut`, but removes samples related to $P$ groups for each training/test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ee7129b2-0650-4b85-b326-56441b3fa3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:      [0 1 2 3 4 5]\n",
      "y:      [1, 1, 1, 2, 2, 2]\n",
      "groups: [1, 1, 2, 2, 3, 3]\n",
      "\n",
      "... leave 2 groups out ...\n",
      "\n",
      "0th split:\n",
      "train idx: [4 5] test idx: [0 1 2 3]\n",
      "train set: [4, 5] test set: [0, 1, 2, 3]\n",
      "train grp: [3, 3] test set: [1, 1, 2, 2]\n",
      "\n",
      "1th split:\n",
      "train idx: [2 3] test idx: [0 1 4 5]\n",
      "train set: [2, 3] test set: [0, 1, 4, 5]\n",
      "train grp: [2, 2] test set: [1, 1, 3, 3]\n",
      "\n",
      "2th split:\n",
      "train idx: [0 1] test idx: [2 3 4 5]\n",
      "train set: [0, 1] test set: [2, 3, 4, 5]\n",
      "train grp: [1, 1] test set: [2, 2, 3, 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeavePGroupsOut\n",
    "\n",
    "X = np.arange(6)\n",
    "y = [1, 1, 1, 2, 2, 2]\n",
    "groups = [1, 1, 2, 2, 3, 3]\n",
    "print(\"X:     \", X)\n",
    "print(\"y:     \", y)\n",
    "print(\"groups:\", groups)\n",
    "print()\n",
    "\n",
    "lpgo = LeavePGroupsOut(n_groups=2)\n",
    "\n",
    "print(\"... leave 2 groups out ...\\n\")\n",
    "\n",
    "for kth, (train, test) in enumerate(lpgo.split(X, y, groups=groups)):\n",
    "    print(f\"{kth}th split:\")\n",
    "    print(f\"train idx: {train} test idx: {test}\")\n",
    "    print(f\"train set: {[X[i_] for i_ in train]} test set: {[X[i_] for i_ in test]}\")\n",
    "    print(f\"train grp: {[groups[i_] for i_ in train]} test set: {[groups[i_] for i_ in test]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9461f0d1-33b5-4595-a4f3-4ad6f5e9be9a",
   "metadata": {},
   "source": [
    "Group Shuffle Split:\n",
    " \n",
    "* The `GroupShuffleSplit` iterator behaves as a combination of `ShuffleSplit` and `LeavePGroupsOut`, \n",
    "* ...and generates a sequence of randomized partitions in which a *subset of groups* are held out for *each split*.\n",
    "\n",
    "\n",
    "* This class is useful when the behavior of `LeavePGroupsOut` is desired, but the number of groups is large enough that generating all possible partitions with groups withheld would be prohibitively expensive. In such a scenario, `GroupShuffleSplit` provides a random sample (with replacement) of the train / test splits generated by `LeavePGroupsOut`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9da0feae-24d9-48d8-8042-8bd3264a31f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:      [0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 0.001]\n",
      "y:      ['a', 'b', 'b', 'b', 'c', 'c', 'c', 'a']\n",
      "groups: [1, 1, 2, 2, 3, 3, 4, 4]\n",
      "\n",
      "0th split:\n",
      "train idx: [0 1 2 3] test idx: [4 5 6 7]\n",
      "train set: [0.1, 0.2, 2.2, 2.4] test set: [2.3, 4.55, 5.8, 0.001]\n",
      "train grp: [1, 1, 2, 2] test set: [3, 3, 4, 4]\n",
      "\n",
      "1th split:\n",
      "train idx: [2 3 6 7] test idx: [0 1 4 5]\n",
      "train set: [2.2, 2.4, 5.8, 0.001] test set: [0.1, 0.2, 2.3, 4.55]\n",
      "train grp: [2, 2, 4, 4] test set: [1, 1, 3, 3]\n",
      "\n",
      "2th split:\n",
      "train idx: [2 3 4 5] test idx: [0 1 6 7]\n",
      "train set: [2.2, 2.4, 2.3, 4.55] test set: [0.1, 0.2, 5.8, 0.001]\n",
      "train grp: [2, 2, 3, 3] test set: [1, 1, 4, 4]\n",
      "\n",
      "3th split:\n",
      "train idx: [4 5 6 7] test idx: [0 1 2 3]\n",
      "train set: [2.3, 4.55, 5.8, 0.001] test set: [0.1, 0.2, 2.2, 2.4]\n",
      "train grp: [3, 3, 4, 4] test set: [1, 1, 2, 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "X = [0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 0.001]\n",
    "y = [\"a\", \"b\", \"b\", \"b\", \"c\", \"c\", \"c\", \"a\"]\n",
    "groups = [1, 1, 2, 2, 3, 3, 4, 4]\n",
    "print(\"X:     \", X)\n",
    "print(\"y:     \", y)\n",
    "print(\"groups:\", groups)\n",
    "print()\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=4, test_size=0.5, random_state=0)\n",
    "\n",
    "for kth, (train, test) in enumerate(gss.split(X, y, groups=groups)):\n",
    "    print(f\"{kth}th split:\")\n",
    "    print(f\"train idx: {train} test idx: {test}\")\n",
    "    print(f\"train set: {[X[i_] for i_ in train]} test set: {[X[i_] for i_ in test]}\")\n",
    "    print(f\"train grp: {[groups[i_] for i_ in train]} test set: {[groups[i_] for i_ in test]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef589bb-7658-495e-81f3-f87ce5913c9c",
   "metadata": {},
   "source": [
    "<br/>\n",
    "Predefined Fold-Splits / Validation-Sets:\n",
    "\n",
    "* For some datasets, a pre-defined split of the data into training- and validation fold or into several cross-validation folds already exists. \n",
    "* Using [`PredefinedSplit`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.PredefinedSplit.html#sklearn.model_selection.PredefinedSplit) it is possible to use these folds e.g. when searching for hyperparameters.\n",
    "\n",
    "\n",
    "* For example, when using a validation set, set the `test_fold` to 0 for all samples that are part of the validation set, and to -1 for all other samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e821cb5-455b-4f71-8db8-8f644b12fa31",
   "metadata": {},
   "source": [
    "#### [Using cross-validation iterators to split train and test](https://scikit-learn.org/stable/modules/cross_validation.html#using-cross-validation-iterators-to-split-train-and-test):\n",
    "\n",
    "* The above group cross-validation functions may also be useful for splitting a dataset into training and testing subsets. \n",
    "* Note that the convenience function `train_test_split` is *a wrapper around `ShuffleSplit`* and thus only allows for stratified splitting (using the class labels) and *cannot account for groups*.\n",
    "* To perform the train and test split, use the indices for the train and test subsets yielded by the generator output by the `split()` method of the cross-validation splitter. \n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b72bf15a-f310-490e-9d73-05c3f1811e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:      [1.00e-01 2.00e-01 2.20e+00 2.40e+00 2.30e+00 4.55e+00 5.80e+00 1.00e-03]\n",
      "y:      ['a' 'b' 'b' 'b' 'c' 'c' 'c' 'a']\n",
      "groups: [1 1 2 2 3 3 4 4]\n",
      "\n",
      "Train and test shapes:\n",
      "(6,) (2,)\n",
      "Train and test unique groups:\n",
      "[1 2 4] [3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "X = np.array([0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 0.001])\n",
    "y = np.array([\"a\", \"b\", \"b\", \"b\", \"c\", \"c\", \"c\", \"a\"])\n",
    "groups = np.array([1, 1, 2, 2, 3, 3, 4, 4])\n",
    "print(\"X:     \", X)\n",
    "print(\"y:     \", y)\n",
    "print(\"groups:\", groups)\n",
    "print()\n",
    "\n",
    "# Get indices from a .split() call of GroupShuffleSplit, use next() as it's a generator. \n",
    "train_indx, test_indx = next(\n",
    "    GroupShuffleSplit(random_state=7).split(X, y, groups)\n",
    ")\n",
    "\n",
    "# Apply indices to data:\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    X[train_indx], X[test_indx], y[train_indx], y[test_indx]\n",
    "print(\"Train and test shapes:\")\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "print(\"Train and test unique groups:\")\n",
    "print(np.unique(groups[train_indx]), np.unique(groups[test_indx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33547b1-f206-4494-81e3-dd4850bf7fa6",
   "metadata": {},
   "source": [
    "#### Cross validation of time series data\n",
    "\n",
    "1. Time series data is characterised by the **correlation between observations that are near in time (autocorrelation)**. \n",
    "2. However, classical cross-validation techniques such as `KFold` and `ShuffleSplit` assume the samples are *independent and identically distributed*, and would result in unreasonable correlation between training and testing instances (yielding poor estimates of generalisation error) on time series data. \n",
    "3. Therefore, it is very important to evaluate our model **for time series data on the ‚Äúfuture‚Äù observations least like those that are used to train the model**. \n",
    "4. To achieve this, one solution is provided by `TimeSeriesSplit`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563ee475-bd59-451c-b803-383e451ec3c7",
   "metadata": {},
   "source": [
    "Time Series Split:\n",
    "\n",
    "* `TimeSeriesSplit` is a variation of k-fold which **returns first $k$ folds as train set and the $(k+1)$th fold as test set**. \n",
    "* Note that unlike standard cross-validation methods, **successive training sets are *supersets* of those that come before them**. \n",
    "* Also, it adds all surplus data to the first training partition, which is always used to train the model.\n",
    "* This class can be used to cross-validate time series data samples that are observed at fixed time intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9cf6bb5f-e120-4e2f-8da3-c8d06fe77096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      " [[ 1  2]\n",
      " [ 3  4]\n",
      " [ 5  6]\n",
      " [ 7  8]\n",
      " [ 9 10]\n",
      " [11 12]]\n",
      "y:\n",
      " [1 2 3 4 5 6]\n",
      "\n",
      "TimeSeriesSplit(gap=0, max_train_size=None, n_splits=3, test_size=None)\n",
      "0th split:\n",
      "train idx: [0 1 2] test idx: [3]\n",
      "train set: [array([1, 2]), array([3, 4]), array([5, 6])] test set: [array([7, 8])]\n",
      "\n",
      "1th split:\n",
      "train idx: [0 1 2 3] test idx: [4]\n",
      "train set: [array([1, 2]), array([3, 4]), array([5, 6]), array([7, 8])] test set: [array([ 9, 10])]\n",
      "\n",
      "2th split:\n",
      "train idx: [0 1 2 3 4] test idx: [5]\n",
      "train set: [array([1, 2]), array([3, 4]), array([5, 6]), array([7, 8]), array([ 9, 10])] test set: [array([11, 12])]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 13. TimeSeriesSplit\n",
    "# Example of 3-split time series cross-validation on a dataset with 6 samples:\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12]])\n",
    "y = np.array([1, 2, 3, 4, 5, 6])\n",
    "print(\"X:\\n\", X)\n",
    "print(\"y:\\n\", y)\n",
    "print()\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "print(tscv)\n",
    "\n",
    "for kth, (train, test) in enumerate(tscv.split(X)):\n",
    "    print(f\"{kth}th split:\")\n",
    "    print(f\"train idx: {train} test idx: {test}\")\n",
    "    print(f\"train set: {[X[i_] for i_ in train]} test set: {[X[i_] for i_ in test]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474ab24c-c345-48ca-8fb1-78bb94c3ee26",
   "metadata": {},
   "source": [
    "### [A note on shuffling](https://scikit-learn.org/stable/modules/cross_validation.html#a-note-on-shuffling)\n",
    "\n",
    "* If the data ordering is not arbitrary (e.g. samples with the same class label are contiguous), shuffling it first may be essential to get a meaningful cross- validation result.\n",
    "* However, the opposite may be true if the samples are *not* independently and identically distributed. \n",
    "    * For example, if samples correspond to news articles, and are ordered by their time of publication, then shuffling the data will likely lead to a model that is overfit and an inflated validation score: it will be tested on samples that are artificially similar (close in time) to training samples.\n",
    "\n",
    "\n",
    "* Some cross validation iterators, such as `KFold`, have an *inbuilt option to shuffle the data indices before splitting them*. \n",
    "\n",
    "**` ` üí° Note that: ` `**\n",
    "* This consumes less memory than shuffling the data directly.\n",
    "* *By default no shuffling occurs*, including for the (stratified) K fold cross- validation performed by specifying `cv=some_integer` to `cross_val_score`, grid search, etc. (Keep in mind that train_test_split still returns a random split.)\n",
    "* The `random_state` parameter defaults to `None`, meaning that the shuffling will be different every time `KFold(..., shuffle=True)` is iterated. \n",
    "    * However, `GridSearchCV` will use the same shuffling for each set of parameters validated by a single call to its fit method.\n",
    "    * To get identical results for each split, set `random_state` to an integer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b813c941-445b-4db9-a404-8e7457b09af9",
   "metadata": {},
   "source": [
    "### [Cross validation and model selection](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-and-model-selection)\n",
    "\n",
    "Cross validation iterators can also be used to directly perform model selection using **Grid Search** for the optimal hyperparameters of the model. \n",
    "\n",
    "This is the topic of the next section of the User Guide: [Tuning the hyper-parameters of an estimator](https://scikit-learn.org/stable/modules/grid_search.html#grid-search)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae080d5-bc57-402e-8b21-7f2fef84718c",
   "metadata": {},
   "source": [
    "### [Permutation test score](https://scikit-learn.org/stable/modules/cross_validation.html#permutation-test-score)\n",
    "\n",
    "[`permutation_test_score`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.permutation_test_score.html#sklearn.model_selection.permutation_test_score) offers another way to evaluate the performance of classifiers.\n",
    "\n",
    "It provides a permutation-based p-value, which represents how likely an observed performance of the classifier would be obtained by chance. \n",
    "\n",
    "The null hypothesis in this test is that the classifier fails to leverage any statistical dependency between the features and the labels to make correct predictions on left out data. \n",
    "\n",
    "* `permutation_test_score` generates a null distribution by calculating `n_permutations` different permutations of the data. \n",
    "* In each permutation the labels are randomly shuffled, thereby removing any dependency between the features and the labels. \n",
    "* The p-value output is the fraction of permutations for which the average cross-validation score obtained by the model is better than the cross-validation score obtained by the model using the original data. \n",
    "\n",
    "**For reliable results `n_permutations` should typically be larger than 100 and `cv` between 3-10 folds.**\n",
    "\n",
    "For more details, see title link."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
